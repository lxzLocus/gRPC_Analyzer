{
  "changed_files": [
    {
      "path": "controller/server/src/main/java/com/emc/pravega/controller/eventProcessor/impl/EventProcessorCell.java",
      "content": "/**\n *\n *  Copyright (c) 2017 Dell Inc., or its subsidiaries.\n *\n */\npackage com.emc.pravega.controller.eventProcessor.impl;\n\nimport com.emc.pravega.common.LoggerHelpers;\nimport com.emc.pravega.controller.eventProcessor.CheckpointConfig;\nimport com.emc.pravega.controller.store.checkpoint.CheckpointStore;\nimport com.emc.pravega.controller.store.checkpoint.CheckpointStoreException;\nimport com.emc.pravega.controller.eventProcessor.ExceptionHandler;\nimport com.emc.pravega.controller.eventProcessor.EventProcessorInitException;\nimport com.emc.pravega.controller.eventProcessor.EventProcessorReinitException;\nimport com.emc.pravega.controller.eventProcessor.EventProcessorConfig;\nimport com.emc.pravega.controller.eventProcessor.ControllerEvent;\nimport com.emc.pravega.stream.EventRead;\nimport com.emc.pravega.stream.EventStreamReader;\nimport com.emc.pravega.stream.Position;\nimport com.google.common.annotations.VisibleForTesting;\nimport com.google.common.util.concurrent.AbstractExecutionThreadService;\nimport com.google.common.util.concurrent.Service;\nimport lombok.AccessLevel;\nimport lombok.Getter;\nimport lombok.extern.slf4j.Slf4j;\n\nimport javax.annotation.concurrent.NotThreadSafe;\n\n/**\n * This is an internal class that embeds the following.\n * 1. Event processor instance.\n * 2. Checkpoint state encapsulating checkpoint persistence logic.\n * 3. A reader reference that is part of the reader group associated\n *    with the EventProcessor group to which this event processor belongs.\n * 4. A delegate, which provides a single thread of execution for invoking\n *    the event processor methods like process, beforeStart, afterStop, etc.\n *\n * This object manages life cycle of an event processor by invoking it's beforeStart, process, afterStop methods.\n *\n * @param <T> Event type parameter.\n */\n@Slf4j\nclass EventProcessorCell<T extends ControllerEvent> {\n\n    private final EventStreamReader<T> reader;\n    private final String objectId;\n\n    @VisibleForTesting\n    @Getter(value = AccessLevel.PACKAGE)\n    private EventProcessor<T> actor;\n\n    /**\n     * Event processor cell encapsulates a delegate which extends AbstractExecutionThreadService.\n     * This delegate provides a single thread of execution for the event processor.\n     * This prevents sub-classes of EventProcessor from controlling EventProcessor's lifecycle.\n     */\n    private final Service delegate;\n\n    private class Delegate extends AbstractExecutionThreadService {\n\n        private final long defaultTimeout = 2000L;\n        private final EventProcessorConfig<T> eventProcessorConfig;\n        private EventRead<T> event;\n        private final CheckpointState state;\n\n        Delegate(final String process, final String readerId, final EventProcessorConfig<T> eventProcessorConfig,\n                 final CheckpointStore checkpointStore) {\n            this.eventProcessorConfig = eventProcessorConfig;\n            this.state = new CheckpointState(checkpointStore, process,\n                    eventProcessorConfig.getConfig().getReaderGroupName(), readerId,\n                    eventProcessorConfig.getConfig().getCheckpointConfig().getCheckpointPeriod());\n        }\n\n        @Override\n        protected final void startUp() {\n            log.debug(\"Event processor STARTUP {}, state={}\", objectId, state());\n            try {\n                actor.beforeStart();\n            } catch (Exception e) {\n                log.warn(String.format(\"Failed while executing preStart for event processor %s\", objectId), e);\n                handleException(new EventProcessorInitException(e));\n            }\n        }\n\n        @Override\n        protected final void run() throws Exception {\n            log.debug(\"Event processor RUN {}, state={}\", objectId, state());\n\n            while (isRunning()) {\n                try {\n                    event = reader.readNextEvent(defaultTimeout);\n                    if (event != null && event.getEvent() != null) {\n                        // invoke the user specified event processing method\n                        actor.process(event.getEvent());\n\n                        // possibly persist event position\n                        state.store(event.getPosition());\n                    }\n                } catch (Exception e) {\n                    log.warn(String.format(\"Failed in run method of event processor %s\", objectId), e);\n                    handleException(e);\n                }\n            }\n\n        }\n\n        @Override\n        protected final void shutDown() throws Exception {\n            log.debug(\"Event processor SHUTDOWN {}, state={}\", objectId, state());\n            try {\n                actor.afterStop();\n            } catch (Exception e) {\n                // Error encountered while cleanup is just logged.\n                // AbstractExecutionThreadService shall transition the service to failed state.\n                log.warn(String.format(\"Failed while executing afterStop for event processor %s\", objectId), e);\n                throw e;\n            } finally {\n\n                // If exception is thrown in any of the following operations, it is just logged.\n                // Some other controller process is responsible for cleaning up reader and its position object\n\n                // First close the reader, which implicitly notifies reader position to the reader group\n                reader.close();\n\n                // Next, clean up the reader and its position from checkpoint store\n                state.stop();\n            }\n        }\n\n        private void restart(Throwable error, T event) {\n            log.debug(\"Event processor RESTART {}, state={}\", objectId, state());\n            try {\n\n                actor.beforeRestart(error, event);\n\n                // Now clean up the event processor state by re-creating it and then invoke startUp.\n                actor = createEventProcessor(eventProcessorConfig);\n\n                startUp();\n\n            } catch (Exception e) {\n                log.warn(String.format(\"Failed while executing preRestart for event processor %s\", objectId), e);\n                handleException(new EventProcessorReinitException(e));\n            }\n        }\n\n        private void handleException(Exception e) {\n            ExceptionHandler.Directive directive = eventProcessorConfig.getExceptionHandler().run(e);\n            switch (directive) {\n                case Restart:\n                    this.restart(e, event == null ? null : event.getEvent());\n                    break;\n\n                case Resume:\n                    // no action\n                    break;\n\n                case Stop:\n                    this.stopAsync();\n                    break;\n            }\n        }\n    }\n\n    @NotThreadSafe\n    private class CheckpointState {\n\n        private final CheckpointStore checkpointStore;\n        private final String process;\n        private final String readerGroupName;\n        private final String readerId;\n        private final CheckpointConfig.CheckpointPeriod checkpointPeriod;\n\n        private int count;\n        private int previousCheckpointIndex;\n        private long previousCheckpointTimestamp;\n\n        CheckpointState(final CheckpointStore checkpointStore,\n                        final String process,\n                        final String readerGroupName,\n                        final String readerId,\n                        final CheckpointConfig.CheckpointPeriod checkpointPeriod) {\n            this.checkpointStore = checkpointStore;\n            this.process = process;\n            this.readerGroupName = readerGroupName;\n            this.readerId = readerId;\n            this.checkpointPeriod = checkpointPeriod;\n\n            count = 0;\n            previousCheckpointIndex = 0;\n            previousCheckpointTimestamp = System.currentTimeMillis();\n        }\n\n        void store(Position position) {\n            count++;\n            final long timestamp = System.currentTimeMillis();\n\n            final int countInterval = count - previousCheckpointIndex;\n            final long timeInterval = timestamp - previousCheckpointTimestamp;\n\n            if (countInterval >= checkpointPeriod.getNumEvents() ||\n                    timeInterval >= 1000 * checkpointPeriod.getNumSeconds()) {\n\n                try {\n\n                    checkpointStore.setPosition(process, readerGroupName, readerId, position);\n                    // update the previous checkpoint stats if successful,\n                    // otherwise, we again attempt checkpointing after processing next event\n                    previousCheckpointIndex = count;\n                    previousCheckpointTimestamp = timestamp;\n\n                } catch (CheckpointStoreException cse) {\n                    // Log the exception, without updating previous checkpoint index or timestamp.\n                    // So that persisting checkpoint shall be attempted again after processing next message.\n                    log.warn(String.format(\"Failed persisting checkpoint for event processor %s\", objectId),\n                            cse.getCause());\n                }\n            }\n        }\n\n        void stop() throws CheckpointStoreException {\n            checkpointStore.removeReader(process, readerGroupName, readerId);\n        }\n    }\n\n    EventProcessorCell(final EventProcessorConfig<T> eventProcessorConfig,\n                       final EventStreamReader<T> reader,\n                       final String process,\n                       final String readerId,\n                       final int index,\n                       final CheckpointStore checkpointStore) {\n        this.objectId = String.format(\"EventProcessor[%s:%s]\", eventProcessorConfig.getConfig().getReaderGroupName(),\n                index);\n        this.reader = reader;\n        this.actor = createEventProcessor(eventProcessorConfig);\n        this.delegate = new Delegate(process, readerId, eventProcessorConfig, checkpointStore);\n    }\n\n    final void startAsync() {\n        long traceId = LoggerHelpers.traceEnterWithContext(log, this.objectId, \"startAsync\");\n        try {\n            delegate.startAsync();\n        } finally {\n            LoggerHelpers.traceLeave(log, this.objectId, \"startAsync\", traceId);\n        }\n    }\n\n    final void stopAsync() {\n        long traceId = LoggerHelpers.traceEnterWithContext(log, this.objectId, \"stopAsync\");\n        try {\n            delegate.stopAsync();\n        } finally {\n            LoggerHelpers.traceLeave(log, this.objectId, \"stopAsync\", traceId);\n        }\n    }\n\n    final void awaitRunning() {\n        delegate.awaitRunning();\n    }\n\n    final void awaitTerminated() {\n        delegate.awaitTerminated();\n    }\n\n    private EventProcessor<T> createEventProcessor(final EventProcessorConfig<T> eventProcessorConfig) {\n        return eventProcessorConfig.getSupplier().get();\n    }\n\n    @Override\n    public String toString() {\n        return String.format(\"%s[%s]\", objectId, this.delegate.state());\n    }\n}\n"
    },
    {
      "path": "controller/server/src/main/java/com/emc/pravega/controller/requesthandler/RequestHandlersInit.java",
      "content": "/**\n * Copyright (c) 2017 Dell Inc., or its subsidiaries.\n */\npackage com.emc.pravega.controller.requesthandler;\n\nimport com.emc.pravega.ClientFactory;\nimport com.emc.pravega.ReaderGroupManager;\nimport com.emc.pravega.common.ExceptionHelpers;\nimport com.emc.pravega.common.concurrent.FutureHelpers;\nimport com.emc.pravega.common.util.Retry;\nimport com.emc.pravega.controller.requests.ScaleRequest;\nimport com.emc.pravega.controller.server.eventProcessor.LocalController;\nimport com.emc.pravega.controller.server.ControllerService;\nimport com.emc.pravega.controller.store.stream.DataNotFoundException;\nimport com.emc.pravega.controller.store.stream.StreamAlreadyExistsException;\nimport com.emc.pravega.controller.store.stream.StreamMetadataStore;\nimport com.emc.pravega.controller.stream.api.grpc.v1.Controller;\nimport com.emc.pravega.controller.task.Stream.StreamMetadataTasks;\nimport com.emc.pravega.controller.task.Stream.StreamTransactionMetadataTasks;\nimport com.emc.pravega.controller.util.Config;\nimport com.emc.pravega.stream.EventStreamReader;\nimport com.emc.pravega.stream.EventStreamWriter;\nimport com.emc.pravega.stream.EventWriterConfig;\nimport com.emc.pravega.stream.Position;\nimport com.emc.pravega.stream.ReaderConfig;\nimport com.emc.pravega.stream.ReaderGroup;\nimport com.emc.pravega.stream.ReaderGroupConfig;\nimport com.emc.pravega.stream.ScalingPolicy;\nimport com.emc.pravega.stream.Sequence;\nimport com.emc.pravega.stream.StreamConfiguration;\nimport com.emc.pravega.stream.impl.JavaSerializer;\nimport com.emc.pravega.stream.impl.PositionImpl;\nimport com.emc.pravega.stream.impl.ReaderGroupManagerImpl;\nimport com.google.common.base.Preconditions;\nimport lombok.extern.slf4j.Slf4j;\n\nimport java.util.Collections;\nimport java.util.concurrent.CompletableFuture;\nimport java.util.concurrent.CompletionException;\nimport java.util.concurrent.ScheduledExecutorService;\nimport java.util.concurrent.atomic.AtomicReference;\n\n@Slf4j\npublic class RequestHandlersInit {\n    private static final StreamConfiguration REQUEST_STREAM_CONFIG = StreamConfiguration.builder()\n                                                                                        .scope(Config.INTERNAL_SCOPE)\n                                                                                        .streamName(Config.SCALE_STREAM_NAME)\n                                                                                        .scalingPolicy(ScalingPolicy.fixed(1))\n                                                                                        .build();\n\n    private static final AtomicReference<ScaleRequestHandler> SCALE_HANDLER_REF = new AtomicReference<>();\n    private static final AtomicReference<EventStreamReader<ScaleRequest>> SCALE_READER_REF = new AtomicReference<>();\n    private static final AtomicReference<EventStreamWriter<ScaleRequest>> SCALE_WRITER_REF = new AtomicReference<>();\n    private static final AtomicReference<RequestReader<ScaleRequest, ScaleRequestHandler>> SCALE_REQUEST_READER_REF = new AtomicReference<>();\n    private static final AtomicReference<StreamMetadataStore> CHECKPOINT_STORE_REF = new AtomicReference<>();\n    private static final AtomicReference<JavaSerializer<Position>> SERIALIZER_REF = new AtomicReference<>();\n\n    public static CompletableFuture<Void> bootstrapRequestHandlers(ControllerService controller,\n                                                                   StreamMetadataStore checkpointStore,\n                                                                   ScheduledExecutorService executor) {\n        Preconditions.checkNotNull(controller);\n        Preconditions.checkNotNull(checkpointStore);\n        Preconditions.checkNotNull(executor);\n\n        final LocalController localController = new LocalController(controller);\n        ClientFactory clientFactory = ClientFactory.withScope(Config.INTERNAL_SCOPE, localController);\n\n        ReaderGroupManager readerGroupManager = new ReaderGroupManagerImpl(Config.INTERNAL_SCOPE, localController, clientFactory);\n\n        CHECKPOINT_STORE_REF.set(checkpointStore);\n        SERIALIZER_REF.set(new JavaSerializer<>());\n\n        return createScope(controller, executor)\n                .thenCompose(x -> createStreams(controller, executor))\n                .thenCompose(y -> startScaleReader(clientFactory, readerGroupManager, controller.getStreamMetadataTasks(),\n                        controller.getStreamStore(), controller.getStreamTransactionMetadataTasks(),\n                        executor))\n                .thenCompose(z -> SCALE_REQUEST_READER_REF.get().run());\n    }\n\n    private static CompletableFuture<Void> createScope(ControllerService controller, ScheduledExecutorService executor) {\n        CompletableFuture<Void> result = new CompletableFuture<>();\n        Retry.indefinitelyWithExpBackoff(10, 10, 10000,\n                e -> log.error(\"Exception while creating request stream {}\", e))\n                .runAsync(() -> controller.createScope(Config.INTERNAL_SCOPE)\n                        .whenComplete((res, ex) -> {\n                            if (ex != null) {\n                                // fail and exit\n                                throw new CompletionException(ex);\n                            }\n                            if (res != null && res.equals(Controller.CreateScopeStatus.Status.FAILURE)) {\n                                throw new RuntimeException(\"Failed to create scope while starting controller\");\n                            }\n                            result.complete(null);\n                        }), executor);\n        return result;\n    }\n\n    private static CompletableFuture<Void> createStreams(ControllerService controller, ScheduledExecutorService executor) {\n        CompletableFuture<Void> result = new CompletableFuture<>();\n        Retry.indefinitelyWithExpBackoff(10, 10, 10000,\n                e -> log.error(\"Exception while creating request stream {}\", e))\n                .runAsync(() -> controller.createStream(REQUEST_STREAM_CONFIG, System.currentTimeMillis())\n                        .whenComplete((res, ex) -> {\n                            if (ex != null && !(ex instanceof StreamAlreadyExistsException)) {\n                                // fail and exit\n                                throw new CompletionException(ex);\n                            }\n                            if (res != null && res.equals(Controller.CreateStreamStatus.Status.FAILURE)) {\n                                throw new RuntimeException(\"Failed to create stream while starting controller\");\n                            }\n                            result.complete(null);\n                        }), executor);\n        return result;\n    }\n\n    private static CompletableFuture<Void> startScaleReader(ClientFactory clientFactory, ReaderGroupManager readerGroupManager, StreamMetadataTasks streamMetadataTasks, StreamMetadataStore streamStore, StreamTransactionMetadataTasks streamTransactionMetadataTasks, ScheduledExecutorService executor) {\n        CompletableFuture<Void> result = new CompletableFuture<>();\n        Retry.indefinitelyWithExpBackoff(10, 10, 10000,\n                e -> log.error(\"Exception while starting reader {}\", e))\n                .runAsync(() -> {\n                    ReaderGroupConfig groupConfig = ReaderGroupConfig.builder().startingPosition(Sequence.MIN_VALUE).build();\n\n                    ReaderGroup readerGroup = readerGroupManager.createReaderGroup(Config.SCALE_READER_GROUP, groupConfig, Collections.singleton(Config.SCALE_STREAM_NAME));\n\n                    if (SCALE_HANDLER_REF.get() == null) {\n                        SCALE_HANDLER_REF.compareAndSet(null, new ScaleRequestHandler(streamMetadataTasks, streamStore, streamTransactionMetadataTasks, executor));\n                    }\n\n                    if (SCALE_READER_REF.get() == null) {\n                        // If a reader with the same reader id already exists, It means no one reported reader offline.\n                        // Report reader offline and create new reader with the same id.\n                        if (readerGroup.getOnlineReaders().contains(Config.SCALE_READER_ID)) {\n                            // read previous checkpoint if any\n                            FutureHelpers.getAndHandleExceptions(CHECKPOINT_STORE_REF.get()\n                                    .readCheckpoint(Config.SCALE_READER_GROUP, Config.SCALE_READER_ID)\n                                    .handle((res, ex) -> {\n                                        if (ex != null && !(ExceptionHelpers.getRealException(ex) instanceof DataNotFoundException)) {\n                                            throw new CompletionException(ex);\n                                        }\n\n                                        Position position = res == null ? new PositionImpl(Collections.emptyMap()) :\n                                                SERIALIZER_REF.get().deserialize(res);\n                                        readerGroup.readerOffline(Config.SCALE_READER_ID, position);\n\n                                        return null;\n                                    }), CompletionException::new);\n                        }\n\n                        SCALE_READER_REF.compareAndSet(null, clientFactory.createReader(Config.SCALE_READER_ID,\n                                Config.SCALE_READER_GROUP,\n                                new JavaSerializer<>(),\n                                ReaderConfig.builder().build()));\n                    }\n\n                    if (SCALE_WRITER_REF.get() == null) {\n                        SCALE_WRITER_REF.compareAndSet(null, clientFactory.createEventWriter(Config.SCALE_STREAM_NAME,\n                                new JavaSerializer<>(),\n                                EventWriterConfig.builder().build()));\n                    }\n\n                    if (SCALE_REQUEST_READER_REF.get() == null) {\n                        SCALE_REQUEST_READER_REF.compareAndSet(null, new RequestReader<>(\n                                Config.SCALE_READER_ID,\n                                Config.SCALE_READER_GROUP,\n                                SCALE_WRITER_REF.get(),\n                                SCALE_READER_REF.get(),\n                                SCALE_HANDLER_REF.get(),\n                                SERIALIZER_REF.get(),\n                                CHECKPOINT_STORE_REF.get(),\n                                executor));\n                    }\n\n                    log.debug(\"bootstrapping request handlers done\");\n                    result.complete(null);\n                    return result;\n                }, executor);\n        return result;\n    }\n}\n"
    },
    {
      "path": "controller/server/src/main/java/com/emc/pravega/controller/server/ControllerServiceStarter.java",
      "content": "/**\n * Copyright (c) 2017 Dell Inc., or its subsidiaries.\n */\npackage com.emc.pravega.controller.server;\n\nimport com.emc.pravega.common.LoggerHelpers;\nimport com.emc.pravega.controller.fault.SegmentContainerMonitor;\nimport com.emc.pravega.controller.fault.UniformContainerBalancer;\nimport com.emc.pravega.controller.requesthandler.RequestHandlersInit;\nimport com.emc.pravega.controller.server.eventProcessor.ControllerEventProcessors;\nimport com.emc.pravega.controller.server.eventProcessor.LocalController;\nimport com.emc.pravega.controller.server.rest.RESTServer;\nimport com.emc.pravega.controller.server.rpc.grpc.GRPCServer;\nimport com.emc.pravega.controller.store.checkpoint.CheckpointStore;\nimport com.emc.pravega.controller.store.checkpoint.CheckpointStoreFactory;\nimport com.emc.pravega.controller.store.client.StoreClient;\nimport com.emc.pravega.controller.store.client.StoreClientFactory;\nimport com.emc.pravega.controller.store.host.HostControllerStore;\nimport com.emc.pravega.controller.store.host.HostStoreFactory;\nimport com.emc.pravega.controller.store.stream.StreamMetadataStore;\nimport com.emc.pravega.controller.store.stream.StreamStoreFactory;\nimport com.emc.pravega.controller.store.task.TaskMetadataStore;\nimport com.emc.pravega.controller.store.task.TaskStoreFactory;\nimport com.emc.pravega.controller.task.Stream.StreamMetadataTasks;\nimport com.emc.pravega.controller.task.Stream.StreamTransactionMetadataTasks;\nimport com.emc.pravega.controller.task.TaskSweeper;\nimport com.emc.pravega.controller.timeout.TimeoutService;\nimport com.emc.pravega.controller.timeout.TimerWheelTimeoutService;\nimport com.emc.pravega.stream.impl.netty.ConnectionFactory;\nimport com.emc.pravega.stream.impl.netty.ConnectionFactoryImpl;\nimport com.google.common.annotations.VisibleForTesting;\nimport com.google.common.util.concurrent.AbstractIdleService;\nimport com.google.common.util.concurrent.ThreadFactoryBuilder;\nimport lombok.extern.slf4j.Slf4j;\nimport org.apache.curator.framework.CuratorFramework;\n\nimport java.net.InetAddress;\nimport java.net.UnknownHostException;\nimport java.util.UUID;\nimport java.util.concurrent.CountDownLatch;\nimport java.util.concurrent.Executors;\nimport java.util.concurrent.ScheduledExecutorService;\nimport java.util.concurrent.TimeUnit;\n\n/**\n * Creates the controller service, given the service configuration.\n */\n@Slf4j\npublic final class ControllerServiceStarter extends AbstractIdleService {\n    private final ControllerServiceConfig serviceConfig;\n    private final String objectId;\n\n    private ScheduledExecutorService controllerServiceExecutor;\n    private ScheduledExecutorService taskExecutor;\n    private ScheduledExecutorService storeExecutor;\n    private ScheduledExecutorService requestExecutor;\n    private ScheduledExecutorService eventProcExecutor;\n\n    private StreamMetadataStore streamStore;\n    private TaskMetadataStore taskMetadataStore;\n    private HostControllerStore hostStore;\n    private CheckpointStore checkpointStore;\n\n    private StreamMetadataTasks streamMetadataTasks;\n    private StreamTransactionMetadataTasks streamTransactionMetadataTasks;\n    private SegmentContainerMonitor monitor;\n\n    private TimeoutService timeoutService;\n    private ControllerService controllerService;\n\n    private LocalController localController;\n    private ControllerEventProcessors controllerEventProcessors;\n\n    /**\n     * ControllerReadyLatch is released once localController, streamTransactionMetadataTasks and controllerService\n     * variables are initialized in the startUp method.\n     */\n    private final CountDownLatch controllerReadyLatch;\n\n    private GRPCServer grpcServer;\n    private RESTServer restServer;\n\n    public ControllerServiceStarter(ControllerServiceConfig serviceConfig) {\n        this.serviceConfig = serviceConfig;\n        this.objectId = \"ControllerServiceStarter\";\n        this.controllerReadyLatch = new CountDownLatch(1);\n    }\n\n    @Override\n    protected void startUp() {\n        long traceId = LoggerHelpers.traceEnterWithContext(log, this.objectId, \"startUp\");\n        log.info(\"Initiating controller service startUp\");\n        log.info(\"Event processors enabled = {}\", serviceConfig.getEventProcessorConfig().isPresent());\n        log.info(\"Request handlers enabled = {}\", serviceConfig.isRequestHandlersEnabled());\n        log.info(\"     gRPC server enabled = {}\", serviceConfig.getGRPCServerConfig().isPresent());\n        log.info(\"     REST server enabled = {}\", serviceConfig.getRestServerConfig().isPresent());\n\n        try {\n            //Initialize the executor service.\n            controllerServiceExecutor = Executors.newScheduledThreadPool(serviceConfig.getServiceThreadPoolSize(),\n                    new ThreadFactoryBuilder().setNameFormat(\"servicepool-%d\").build());\n\n            taskExecutor = Executors.newScheduledThreadPool(serviceConfig.getTaskThreadPoolSize(),\n                    new ThreadFactoryBuilder().setNameFormat(\"taskpool-%d\").build());\n\n            storeExecutor = Executors.newScheduledThreadPool(serviceConfig.getStoreThreadPoolSize(),\n                    new ThreadFactoryBuilder().setNameFormat(\"storepool-%d\").build());\n\n            requestExecutor = Executors.newScheduledThreadPool(serviceConfig.getRequestHandlerThreadPoolSize(),\n                    new ThreadFactoryBuilder().setNameFormat(\"requestpool-%d\").build());\n\n            eventProcExecutor = Executors.newScheduledThreadPool(serviceConfig.getEventProcThreadPoolSize(),\n                    new ThreadFactoryBuilder().setNameFormat(\"eventprocpool-%d\").build());\n\n            log.info(\"Creating store client\");\n            StoreClient storeClient = StoreClientFactory.createStoreClient(serviceConfig.getStoreClientConfig());\n\n            log.info(\"Creating the stream store\");\n            streamStore = StreamStoreFactory.createStore(storeClient, storeExecutor);\n\n            log.info(\"Creating the task store\");\n            taskMetadataStore = TaskStoreFactory.createStore(storeClient, taskExecutor);\n\n            log.info(\"Creating the host store\");\n            hostStore = HostStoreFactory.createStore(serviceConfig.getHostMonitorConfig(), storeClient);\n\n            log.info(\"Creating the checkpoint store\");\n            checkpointStore = CheckpointStoreFactory.create(storeClient);\n\n            String hostId;\n            try {\n                //On each controller process restart, it gets a fresh hostId,\n                //which is a combination of hostname and random GUID.\n                hostId = InetAddress.getLocalHost().getHostAddress() + \"-\" + UUID.randomUUID().toString();\n            } catch (UnknownHostException e) {\n                log.warn(\"Failed to get host address.\", e);\n                hostId = UUID.randomUUID().toString();\n            }\n\n            if (serviceConfig.getHostMonitorConfig().isHostMonitorEnabled()) {\n                //Start the Segment Container Monitor.\n                monitor = new SegmentContainerMonitor(hostStore, (CuratorFramework) storeClient.getClient(),\n                        new UniformContainerBalancer(),\n                        serviceConfig.getHostMonitorConfig().getHostMonitorMinRebalanceInterval());\n                log.info(\"Starting segment container monitor\");\n                monitor.startAsync();\n            }\n\n            ConnectionFactory connectionFactory = new ConnectionFactoryImpl(false);\n            SegmentHelper segmentHelper = new SegmentHelper();\n            streamMetadataTasks = new StreamMetadataTasks(streamStore, hostStore, taskMetadataStore,\n                    segmentHelper, taskExecutor, hostId, connectionFactory);\n            streamTransactionMetadataTasks = new StreamTransactionMetadataTasks(streamStore,\n                    hostStore, taskMetadataStore, segmentHelper, taskExecutor, hostId, connectionFactory);\n            timeoutService = new TimerWheelTimeoutService(streamTransactionMetadataTasks,\n                    serviceConfig.getTimeoutServiceConfig());\n            controllerService = new ControllerService(streamStore, hostStore, streamMetadataTasks,\n                    streamTransactionMetadataTasks, timeoutService, new SegmentHelper(), controllerServiceExecutor);\n\n            // Setup event processors.\n            setController(new LocalController(controllerService));\n\n            if (serviceConfig.getEventProcessorConfig().isPresent()) {\n                // Create ControllerEventProcessor object.\n                controllerEventProcessors = new ControllerEventProcessors(hostId,\n                        serviceConfig.getEventProcessorConfig().get(), localController, checkpointStore, streamStore,\n                        hostStore, segmentHelper, connectionFactory, eventProcExecutor);\n\n                // Bootstrap and start it asynchronously.\n                log.info(\"Starting event processors\");\n                ControllerEventProcessors.bootstrap(localController, serviceConfig.getEventProcessorConfig().get(),\n                        streamTransactionMetadataTasks, eventProcExecutor)\n                        .thenAcceptAsync(x -> controllerEventProcessors.startAsync(), eventProcExecutor);\n            }\n\n            // Start request handlers\n            if (serviceConfig.isRequestHandlersEnabled()) {\n                log.info(\"Starting request handlers\");\n                RequestHandlersInit.bootstrapRequestHandlers(controllerService, streamStore, requestExecutor);\n            }\n\n            // Start RPC server.\n            if (serviceConfig.getGRPCServerConfig().isPresent()) {\n                grpcServer = new GRPCServer(controllerService, serviceConfig.getGRPCServerConfig().get());\n                grpcServer.startAsync();\n                log.info(\"Awaiting start of rpc server\");\n                grpcServer.awaitRunning();\n            }\n\n            // Start REST server.\n            if (serviceConfig.getRestServerConfig().isPresent()) {\n                restServer = new RESTServer(controllerService, serviceConfig.getRestServerConfig().get());\n                restServer.startAsync();\n                log.info(\"Awaiting start of REST server\");\n                restServer.awaitRunning();\n            }\n\n            // Hook up TaskSweeper.sweepOrphanedTasks as a callback on detecting some controller node failure.\n            // todo: hook up TaskSweeper.sweepOrphanedTasks with Failover support feature\n            // Controller has a mechanism to track the currently active controller host instances. On detecting a failure of\n            // any controller instance, the failure detector stores the failed HostId in a failed hosts directory (FH), and\n            // invokes the taskSweeper.sweepOrphanedTasks for each failed host. When all resources under the failed hostId\n            // are processed and deleted, that failed HostId is removed from FH folder.\n            // Moreover, on controller process startup, it detects any hostIds not in the currently active set of\n            // controllers and starts sweeping tasks orphaned by those hostIds.\n            TaskSweeper taskSweeper = new TaskSweeper(taskMetadataStore, hostId, streamMetadataTasks,\n                    streamTransactionMetadataTasks);\n\n            // Finally wait for controller event processors to start\n            if (serviceConfig.getEventProcessorConfig().isPresent()) {\n                log.info(\"Awaiting start of controller event processors\");\n                controllerEventProcessors.awaitRunning();\n            }\n        } finally {\n            LoggerHelpers.traceLeave(log, this.objectId, \"startUp\", traceId);\n        }\n    }\n\n    @Override\n    protected void shutDown() throws Exception {\n        long traceId = LoggerHelpers.traceEnterWithContext(log, this.objectId, \"shutDown\");\n        log.info(\"Initiating controller service shutDown\");\n\n        try {\n            if (restServer != null) {\n                restServer.stopAsync();\n            }\n            if (grpcServer != null) {\n                grpcServer.stopAsync();\n            }\n            if (controllerEventProcessors != null) {\n                controllerEventProcessors.stopAsync();\n            }\n            if (monitor != null) {\n                log.info(\"Stopping the segment container monitor\");\n                monitor.stopAsync();\n            }\n            timeoutService.stopAsync();\n\n            // Next stop all executors\n            log.info(\"Stopping executors\");\n            eventProcExecutor.shutdown();\n            requestExecutor.shutdown();\n            storeExecutor.shutdown();\n            taskExecutor.shutdown();\n            controllerServiceExecutor.shutdown();\n\n            // Finally, await termination of all services\n            if (restServer != null) {\n                log.info(\"Awaiting termination of REST server\");\n                restServer.awaitTerminated();\n            }\n\n            if (grpcServer != null) {\n                log.info(\"Awaiting termination of rpc server\");\n                grpcServer.awaitTerminated();\n            }\n\n            if (controllerEventProcessors != null) {\n                log.info(\"Awaiting termination of controller event processors\");\n                controllerEventProcessors.awaitTerminated();\n            }\n\n            if (monitor != null) {\n                log.info(\"Awaiting termination of segment container monitor\");\n                monitor.awaitTerminated();\n            }\n\n            log.info(\"Awaiting termination of eventProc executor\");\n            eventProcExecutor.awaitTermination(5, TimeUnit.SECONDS);\n\n            log.info(\"Awaiting termination of requestHandler executor\");\n            requestExecutor.awaitTermination(5, TimeUnit.SECONDS);\n\n            log.info(\"Awaiting termination of store executor\");\n            storeExecutor.awaitTermination(5, TimeUnit.SECONDS);\n\n            log.info(\"Awaiting termination of task executor\");\n            taskExecutor.awaitTermination(5, TimeUnit.SECONDS);\n\n            log.info(\"Awaiting termination of controllerService executor\");\n            controllerServiceExecutor.awaitTermination(5, TimeUnit.SECONDS);\n        } finally {\n            LoggerHelpers.traceLeave(log, this.objectId, \"shutDown\", traceId);\n        }\n    }\n\n    @VisibleForTesting\n    public boolean awaitTasksModuleInitialization(long timeout, TimeUnit timeUnit) throws InterruptedException {\n        controllerReadyLatch.await();\n        return this.streamTransactionMetadataTasks.awaitInitialization(timeout, timeUnit);\n    }\n\n    @VisibleForTesting\n    public ControllerService getControllerService() throws InterruptedException {\n        controllerReadyLatch.await();\n        return this.controllerService;\n    }\n\n    @VisibleForTesting\n    public LocalController getController() throws InterruptedException {\n        controllerReadyLatch.await();\n        return this.localController;\n    }\n\n    private void setController(LocalController controller) {\n        this.localController = controller;\n        controllerReadyLatch.countDown();\n    }\n}\n"
    },
    {
      "path": "controller/server/src/main/java/com/emc/pravega/controller/task/Stream/StreamMetadataTasks.java",
      "content": "/**\n * Copyright (c) 2017 Dell Inc., or its subsidiaries.\n */\npackage com.emc.pravega.controller.task.Stream;\n\nimport com.emc.pravega.common.ExceptionHelpers;\nimport com.emc.pravega.common.concurrent.FutureHelpers;\nimport com.emc.pravega.controller.server.SegmentHelper;\nimport com.emc.pravega.controller.store.host.HostControllerStore;\nimport com.emc.pravega.controller.store.stream.DataNotFoundException;\nimport com.emc.pravega.controller.store.stream.OperationContext;\nimport com.emc.pravega.controller.store.stream.Segment;\nimport com.emc.pravega.controller.store.stream.StoreException;\nimport com.emc.pravega.controller.store.stream.StreamMetadataStore;\nimport com.emc.pravega.controller.store.stream.tables.State;\nimport com.emc.pravega.controller.store.task.Resource;\nimport com.emc.pravega.controller.store.task.TaskMetadataStore;\nimport com.emc.pravega.controller.stream.api.grpc.v1.Controller.CreateStreamStatus;\nimport com.emc.pravega.controller.stream.api.grpc.v1.Controller.DeleteStreamStatus;\nimport com.emc.pravega.controller.stream.api.grpc.v1.Controller.ScaleResponse;\nimport com.emc.pravega.controller.stream.api.grpc.v1.Controller.SegmentRange;\nimport com.emc.pravega.controller.stream.api.grpc.v1.Controller.UpdateStreamStatus;\nimport com.emc.pravega.controller.task.Task;\nimport com.emc.pravega.controller.task.TaskBase;\nimport com.emc.pravega.stream.ScalingPolicy;\nimport com.emc.pravega.stream.StreamConfiguration;\nimport com.emc.pravega.stream.impl.ModelHelper;\nimport com.emc.pravega.stream.impl.netty.ConnectionFactory;\nimport java.io.Serializable;\nimport java.util.AbstractMap;\nimport java.util.ArrayList;\nimport java.util.Collections;\nimport java.util.List;\nimport java.util.Set;\nimport java.util.concurrent.CompletableFuture;\nimport java.util.concurrent.CompletionException;\nimport java.util.concurrent.ScheduledExecutorService;\nimport java.util.stream.Collectors;\nimport java.util.stream.IntStream;\nimport lombok.extern.slf4j.Slf4j;\n\nimport static com.emc.pravega.controller.store.stream.StoreException.Type.NODE_EXISTS;\nimport static com.emc.pravega.controller.store.stream.StoreException.Type.NODE_NOT_FOUND;\nimport static com.emc.pravega.controller.task.Stream.TaskStepsRetryHelper.withRetries;\n\n/**\n * Collection of metadata update tasks on stream.\n * Task methods are annotated with @Task annotation.\n * <p>\n * Any update to the task method signature should be avoided, since it can cause problems during upgrade.\n * Instead, a new overloaded method may be created with the same task annotation name but a new version.\n */\n@Slf4j\npublic class StreamMetadataTasks extends TaskBase {\n\n    private final StreamMetadataStore streamMetadataStore;\n    private final HostControllerStore hostControllerStore;\n    private final ConnectionFactory connectionFactory;\n    private final SegmentHelper segmentHelper;\n\n    public StreamMetadataTasks(final StreamMetadataStore streamMetadataStore,\n            final HostControllerStore hostControllerStore, final TaskMetadataStore taskMetadataStore,\n            final SegmentHelper segmentHelper, final ScheduledExecutorService executor, final String hostId,\n            final ConnectionFactory connectionFactory) {\n        this(streamMetadataStore, hostControllerStore, taskMetadataStore, segmentHelper, executor, new Context(hostId),\n             connectionFactory);\n    }\n\n    private StreamMetadataTasks(final StreamMetadataStore streamMetadataStore,\n            final HostControllerStore hostControllerStore, final TaskMetadataStore taskMetadataStore,\n            final SegmentHelper segmentHelper, final ScheduledExecutorService executor, final Context context,\n            ConnectionFactory connectionFactory) {\n        super(taskMetadataStore, executor, context);\n        this.streamMetadataStore = streamMetadataStore;\n        this.hostControllerStore = hostControllerStore;\n        this.segmentHelper = segmentHelper;\n        this.connectionFactory = connectionFactory;\n        this.setReady();\n    }\n\n    /**\n     * Create stream.\n     *\n     * @param scope           scope.\n     * @param stream          stream name.\n     * @param config          stream configuration.\n     * @param createTimestamp creation timestamp.\n     * @return creation status.\n     */\n    @Task(name = \"createStream\", version = \"1.0\", resource = \"{scope}/{stream}\")\n    public CompletableFuture<CreateStreamStatus.Status> createStream(String scope, String stream, StreamConfiguration config, long createTimestamp) {\n        return execute(\n                new Resource(scope, stream),\n                new Serializable[]{scope, stream, config, createTimestamp, null},\n                () -> createStreamBody(scope, stream, config, createTimestamp));\n    }\n\n\n    /**\n     * Update stream's configuration.\n     *\n     * @param scope      scope.\n     * @param stream     stream name.\n     * @param config     modified stream configuration.\n     * @param contextOpt optional context\n     * @return update status.\n     */\n    @Task(name = \"updateConfig\", version = \"1.0\", resource = \"{scope}/{stream}\")\n    public CompletableFuture<UpdateStreamStatus.Status> alterStream(String scope, String stream, StreamConfiguration config, OperationContext contextOpt) {\n        return execute(\n                new Resource(scope, stream),\n                new Serializable[]{scope, stream, config, null},\n                () -> updateStreamConfigBody(scope, stream, config, contextOpt));\n    }\n\n    /**\n     * Seal a stream.\n     *\n     * @param scope      scope.\n     * @param stream     stream name.\n     * @param contextOpt optional context\n     * @return update status.\n     */\n    @Task(name = \"sealStream\", version = \"1.0\", resource = \"{scope}/{stream}\")\n    public CompletableFuture<UpdateStreamStatus.Status> sealStream(String scope, String stream, OperationContext contextOpt) {\n        return execute(\n                new Resource(scope, stream),\n                new Serializable[]{scope, stream, null},\n                () -> sealStreamBody(scope, stream, contextOpt));\n    }\n\n    /**\n     * Delete a stream. Precondition for deleting a stream is that the stream sholud be sealed.\n     *\n     * @param scope      scope.\n     * @param stream     stream name.\n     * @param contextOpt optional context\n     * @return           delete status.\n     */\n    @Task(name = \"deleteStream\", version = \"1.0\", resource = \"{scope}/{stream}\")\n    public CompletableFuture<DeleteStreamStatus.Status> deleteStream(final String scope, final String stream,\n                                                                     final OperationContext contextOpt) {\n        return execute(\n                new Resource(scope, stream),\n                new Serializable[]{scope, stream, null},\n                () -> deleteStreamBody(scope, stream, contextOpt));\n    }\n\n    /**\n     * Scales stream segments.\n     *\n     * @param scope          scope.\n     * @param stream         stream name.\n     * @param sealedSegments segments to be sealed.\n     * @param newRanges      key ranges for new segments.\n     * @param scaleTimestamp scaling time stamp.\n     * @param contextOpt     optional context\n     * @return returns the newly created segments.\n     */\n    @Task(name = \"scaleStream\", version = \"1.0\", resource = \"{scope}/{stream}\")\n    public CompletableFuture<ScaleResponse> scale(String scope, String stream, ArrayList<Integer> sealedSegments,\n            ArrayList<AbstractMap.SimpleEntry<Double, Double>> newRanges, long scaleTimestamp,\n            OperationContext contextOpt) {\n        return execute(\n                new Resource(scope, stream),\n                new Serializable[]{scope, stream, sealedSegments, newRanges, scaleTimestamp, null},\n                () -> scaleBody(scope, stream, sealedSegments, newRanges, scaleTimestamp, contextOpt));\n    }\n\n    private CompletableFuture<CreateStreamStatus.Status> createStreamBody(String scope, String stream,\n            StreamConfiguration config, long timestamp) {\n        if (!validateName(stream)) {\n            log.debug(\"Create stream failed due to invalid stream name {}\", stream);\n            return CompletableFuture.completedFuture(CreateStreamStatus.Status.INVALID_STREAM_NAME);\n        }\n        return this.streamMetadataStore.createStream(scope, stream, config, timestamp, null, executor)\n                .thenComposeAsync(created -> {\n                    log.debug(\"{}/{} created in metadata store\", scope, stream);\n                    if (created) {\n                        List<Integer> newSegments = IntStream.range(0, config.getScalingPolicy().getMinNumSegments()).boxed().collect(Collectors.toList());\n                        return notifyNewSegments(config.getScope(), stream, config, newSegments)\n                                .thenApply(y -> CreateStreamStatus.Status.SUCCESS);\n                    } else {\n                        return CompletableFuture.completedFuture(CreateStreamStatus.Status.FAILURE);\n                    }\n                }, executor)\n                .thenCompose(status -> {\n                    if (status == CreateStreamStatus.Status.FAILURE) {\n                        return CompletableFuture.completedFuture(status);\n                    } else {\n                        final OperationContext context = streamMetadataStore.createContext(scope, stream);\n\n                        return withRetries(() -> streamMetadataStore.setState(scope,\n                                stream, State.ACTIVE, context, executor), executor)\n                                .thenApply(v -> status);\n                    }\n                })\n                .handle((result, ex) -> {\n                    if (ex != null) {\n                        Throwable cause = ExceptionHelpers.getRealException(ex);\n                        if (cause instanceof StoreException && ((StoreException) ex.getCause()).getType() == NODE_EXISTS) {\n                            return CreateStreamStatus.Status.STREAM_EXISTS;\n                        } else if (ex.getCause() instanceof StoreException && ((StoreException) ex.getCause()).getType() == NODE_NOT_FOUND) {\n                            return CreateStreamStatus.Status.SCOPE_NOT_FOUND;\n                        } else {\n                            log.warn(\"Create stream failed due to \", ex);\n                            return CreateStreamStatus.Status.FAILURE;\n                        }\n                    } else {\n                        return result;\n                    }\n                });\n    }\n\n    private static boolean validateName(final String path) {\n        return (path.indexOf('\\\\') >= 0 || path.indexOf('/') >= 0) ? false : true;\n    }\n\n    private CompletableFuture<UpdateStreamStatus.Status> updateStreamConfigBody(String scope, String stream,\n                                                                         StreamConfiguration config, OperationContext contextOpt) {\n        final OperationContext context = contextOpt == null ? streamMetadataStore.createContext(scope, stream) : contextOpt;\n\n        return streamMetadataStore.updateConfiguration(scope, stream, config, context, executor)\n                .thenCompose(updated -> {\n                    log.debug(\"{}/{} created in metadata store\", scope, stream);\n                    if (updated) {\n                        // we are at a point of no return. Metadata has been updated, we need to notify hosts.\n                        // wrap subsequent steps in retries.\n                        return withRetries(() -> streamMetadataStore.getActiveSegments(scope, stream, context, executor), executor)\n                                .thenCompose(activeSegments -> notifyPolicyUpdates(config.getScope(), stream, activeSegments, config.getScalingPolicy()))\n                                .handle((res, ex) -> {\n                                    if (ex == null) {\n                                        return true;\n                                    } else {\n                                        throw new CompletionException(ex);\n                                    }\n                                });\n                    } else {\n                        return CompletableFuture.completedFuture(false);\n                    }\n                })\n                .handle((result, ex) -> {\n                    if (ex != null) {\n                        return handleUpdateStreamError(ex);\n                    } else {\n                        return result ? UpdateStreamStatus.Status.SUCCESS\n                                : UpdateStreamStatus.Status.FAILURE;\n                    }\n                });\n    }\n\n    CompletableFuture<UpdateStreamStatus.Status> sealStreamBody(String scope, String stream, OperationContext contextOpt) {\n        final OperationContext context = contextOpt == null ? streamMetadataStore.createContext(scope, stream) : contextOpt;\n\n        return withRetries(() -> streamMetadataStore.getActiveSegments(scope, stream, context, executor), executor)\n                .thenCompose(activeSegments -> {\n                    if (activeSegments.isEmpty()) { //if active segments are empty then the stream is sealed.\n                        //Do not update the state if the stream is already sealed.\n                        return CompletableFuture.completedFuture(UpdateStreamStatus.Status.SUCCESS);\n                    } else {\n                        List<Integer> segmentsToBeSealed = activeSegments.stream().map(Segment::getNumber).\n                                collect(Collectors.toList());\n                        return notifySealedSegments(scope, stream, segmentsToBeSealed)\n                                .thenCompose(v -> withRetries(() ->\n                                        streamMetadataStore.setSealed(scope, stream, context, executor), executor))\n                                .handle((result, ex) -> {\n                                    if (ex != null) {\n                                        log.warn(\"Exception thrown in trying to notify sealed segments {}\", ex.getMessage());\n                                        return handleUpdateStreamError(ex);\n                                    } else {\n                                        return result ? UpdateStreamStatus.Status.SUCCESS\n                                                : UpdateStreamStatus.Status.FAILURE;\n                                    }\n                                });\n                    }\n                }).exceptionally(this::handleUpdateStreamError);\n    }\n\n    CompletableFuture<DeleteStreamStatus.Status> deleteStreamBody(final String scope, final String stream,\n                                                                  final OperationContext contextOpt) {\n        return withRetries(() -> streamMetadataStore.isSealed(scope, stream, contextOpt, executor), executor)\n                .thenComposeAsync(sealed -> {\n                    if (!sealed) {\n                        return CompletableFuture.completedFuture(DeleteStreamStatus.Status.STREAM_NOT_SEALED);\n                    }\n                    return withRetries(\n                            () -> streamMetadataStore.getSegmentCount(scope, stream, contextOpt, executor), executor)\n                            .thenComposeAsync(count ->\n                                    notifyDeleteSegments(scope, stream, count)\n                                            .thenComposeAsync(x -> withRetries(() ->\n                                                            streamMetadataStore.deleteStream(scope, stream, contextOpt,\n                                                                    executor), executor), executor)\n                                            .handleAsync((result, ex) -> {\n                                                if (ex != null) {\n                                                    log.warn(\"Exception thrown while deleting stream\", ex.getMessage());\n                                                    return handleDeleteStreamError(ex);\n                                                } else {\n                                                    return DeleteStreamStatus.Status.SUCCESS;\n                                                }\n                                            }, executor), executor);\n                }, executor).exceptionally(this::handleDeleteStreamError);\n    }\n\n    CompletableFuture<ScaleResponse> scaleBody(final String scope, final String stream, final List<Integer> segmentsToSeal,\n                                               final List<AbstractMap.SimpleEntry<Double, Double>> newRanges, final long scaleTimestamp,\n                                               final OperationContext contextOpt) {\n        // Abort scaling operation in the following error scenarios\n        // 1. if the active segments in the stream have ts greater than scaleTimestamp -- ScaleStreamStatus.PRECONDITION_FAILED\n        // 2. if active segments having creation timestamp as scaleTimestamp have different key ranges than the ones specified\n        // in newRanges\n        // 3. Transaction is active on the stream\n        // 4. sealedSegments should be a subset of activeSegments.\n        //\n        // If there is intermittent network issue before during precondition check (e.g. for metadata store reads) we will throw\n        // exception and fail the task.\n        // However, once preconditions pass and scale task starts, all steps are wrapped inside Retry block\n        // with exponential back offs. We will have significant number of retries (default: 100).\n        // This is because we dont have roll backs for scale operations. So once started, we should try to complete it by\n        // retrying against all intermittent failures.\n        // Also, we cant leave with intermediate failures as the system state will be inconsistent -\n        // for example: existing segments are sealed, but we are not able to create new segments in metadata store.\n        // So we need to retry and complete all steps.\n        // However, after sufficient retries, if we are still not able to complete all steps in scale task,\n        // we should stop retrying indefinitely and notify administrator.\n        final OperationContext context = contextOpt == null ? streamMetadataStore.createContext(scope, stream) : contextOpt;\n\n        CompletableFuture<Boolean> checkValidity = withRetries(\n                () -> streamMetadataStore.getActiveSegments(scope, stream, context, executor), executor)\n                .thenApply(activeSegments -> {\n                    boolean result = false;\n                    Set<Integer> activeNum = activeSegments.stream().mapToInt(Segment::getNumber).boxed().collect(Collectors.toSet());\n                    if (activeNum.containsAll(segmentsToSeal)) {\n                        result = true;\n                    } else if (activeSegments.size() > 0 && activeSegments\n                            .stream()\n                            .mapToLong(Segment::getStart)\n                            .max()\n                            .getAsLong() == scaleTimestamp) {\n                        result = true;\n                    }\n                    return result;\n                });\n\n        return checkValidity.thenCompose(valid -> {\n            if (valid) {\n                // keep checking until no transactions are running.\n                CompletableFuture<Boolean> check = new CompletableFuture<>();\n                FutureHelpers.loop(() -> !check.isDone(), () -> withRetries(() -> streamMetadataStore.isTransactionOngoing(scope, stream, context, executor)\n                        .thenAccept(txnOngoing -> {\n                            if (!txnOngoing) {\n                                check.complete(true);\n                            }\n                        }), executor), executor);\n                return check;\n            } else {\n                return CompletableFuture.completedFuture(false);\n            }\n        }).thenCompose(valid -> {\n                    if (valid) {\n                        return notifySealedSegments(scope, stream, segmentsToSeal)\n                                .thenCompose(results -> withRetries(\n                                        () -> streamMetadataStore.scale(scope, stream,\n                                                segmentsToSeal, newRanges, scaleTimestamp, context, executor), executor))\n                                .thenCompose((List<Segment> newSegments) -> notifyNewSegments(scope, stream, newSegments, context)\n                                        .thenApply((Void v) -> newSegments))\n                                .thenApply((List<Segment> newSegments) -> {\n                                    ScaleResponse.Builder response = ScaleResponse.newBuilder();\n                                    response.setStatus(ScaleResponse.ScaleStreamStatus.SUCCESS);\n                                    response.addAllSegments(\n                                            newSegments\n                                                    .stream()\n                                                    .map(segment -> convert(scope, stream, segment))\n                                                    .collect(Collectors.toList()));\n                                    return response.build();\n                                });\n                    } else {\n                        ScaleResponse.Builder response = ScaleResponse.newBuilder();\n                        response.setStatus(ScaleResponse.ScaleStreamStatus.PRECONDITION_FAILED);\n                        response.addAllSegments(Collections.emptyList());\n                        return CompletableFuture.completedFuture(response.build());\n                    }\n                }\n        );\n    }\n\n    private CompletableFuture<Void> notifyNewSegments(String scope, String stream, List<Segment> segmentNumbers, OperationContext context) {\n        return withRetries(() -> streamMetadataStore.getConfiguration(scope, stream, context, executor), executor)\n                .thenCompose(configuration -> notifyNewSegments(scope, stream, configuration,\n                        segmentNumbers.stream().map(Segment::getNumber).collect(Collectors.toList())));\n    }\n\n    private CompletableFuture<Void> notifyNewSegments(String scope, String stream, StreamConfiguration configuration, List<Integer> segmentNumbers) {\n        return FutureHelpers.toVoid(FutureHelpers.allOfWithResults(segmentNumbers\n                .stream()\n                .parallel()\n                .map(segment -> notifyNewSegment(scope, stream, segment, configuration.getScalingPolicy()))\n                .collect(Collectors.toList())));\n    }\n\n    private CompletableFuture<Void> notifyNewSegment(String scope, String stream, int segmentNumber, ScalingPolicy policy) {\n        return FutureHelpers.toVoid(withRetries(() -> segmentHelper.createSegment(scope,\n                stream, segmentNumber, policy, hostControllerStore, this.connectionFactory), executor));\n    }\n\n    private CompletableFuture<Void> notifyDeleteSegments(String scope, String stream, int count) {\n        return FutureHelpers.allOf(IntStream.range(0, count)\n                .parallel()\n                .mapToObj(segment -> notifyDeleteSegment(scope, stream, segment))\n                .collect(Collectors.toList()));\n    }\n\n    private CompletableFuture<Void> notifyDeleteSegment(String scope, String stream, int segmentNumber) {\n        return FutureHelpers.toVoid(withRetries(() -> segmentHelper.deleteSegment(scope,\n                stream, segmentNumber, hostControllerStore, this.connectionFactory), executor));\n    }\n\n    private CompletableFuture<Void> notifySealedSegments(String scope, String stream, List<Integer> sealedSegments) {\n        return FutureHelpers.allOf(\n                sealedSegments\n                        .stream()\n                        .parallel()\n                        .map(number -> notifySealedSegment(scope, stream, number))\n                        .collect(Collectors.toList()));\n    }\n\n    private CompletableFuture<Void> notifySealedSegment(final String scope, final String stream, final int sealedSegment) {\n\n        return FutureHelpers.toVoid(withRetries(() -> segmentHelper.sealSegment(\n                scope,\n                stream,\n                sealedSegment,\n                hostControllerStore,\n                this.connectionFactory), executor));\n    }\n\n    private CompletableFuture<Void> notifyPolicyUpdates(String scope, String stream, List<Segment> activeSegments,\n                                                        ScalingPolicy policy) {\n        return FutureHelpers.toVoid(FutureHelpers.allOfWithResults(activeSegments\n                .stream()\n                .parallel()\n                .map(segment -> notifyPolicyUpdate(scope, stream, policy, segment.getNumber()))\n                .collect(Collectors.toList())));\n    }\n\n    private CompletableFuture<Void> notifyPolicyUpdate(String scope, String stream, ScalingPolicy policy, int segmentNumber) {\n\n        return withRetries(() -> segmentHelper.updatePolicy(\n                scope,\n                stream,\n                policy,\n                segmentNumber,\n                hostControllerStore,\n                this.connectionFactory), executor);\n    }\n\n    private SegmentRange convert(String scope, String stream, com.emc.pravega.controller.store.stream.Segment segment) {\n\n        return ModelHelper.createSegmentRange(scope, stream, segment.getNumber(), segment.getKeyEnd(),\n                                              segment.getKeyEnd());\n    }\n\n    private UpdateStreamStatus.Status handleUpdateStreamError(Throwable ex) {\n        Throwable cause = ExceptionHelpers.getRealException(ex);\n        if (cause instanceof DataNotFoundException) {\n            return UpdateStreamStatus.Status.STREAM_NOT_FOUND;\n        } else if (ex instanceof StoreException && ((StoreException) ex).getType() == NODE_NOT_FOUND) {\n            return UpdateStreamStatus.Status.SCOPE_NOT_FOUND;\n        } else {\n            log.warn(\"Update stream failed due to \", ex);\n            return UpdateStreamStatus.Status.FAILURE;\n        }\n    }\n\n    private DeleteStreamStatus.Status handleDeleteStreamError(Throwable ex) {\n        Throwable cause = ExceptionHelpers.getRealException(ex);\n        if (cause instanceof DataNotFoundException ||\n                (ex instanceof StoreException && ((StoreException) ex).getType() == NODE_NOT_FOUND)) {\n            return DeleteStreamStatus.Status.STREAM_NOT_FOUND;\n        } else {\n            log.warn(\"Update stream failed.\", ex);\n            return DeleteStreamStatus.Status.FAILURE;\n        }\n    }\n\n    @Override\n    public TaskBase copyWithContext(Context context) {\n        return new StreamMetadataTasks(streamMetadataStore,\n                                       hostControllerStore,\n                                       taskMetadataStore,\n                                       segmentHelper,\n                                       executor,\n                                       context,\n                                       connectionFactory);\n    }\n}\n"
    },
    {
      "path": "controller/server/src/main/java/com/emc/pravega/controller/task/Stream/StreamTransactionMetadataTasks.java",
      "content": "/**\n * Copyright (c) 2017 Dell Inc., or its subsidiaries.\n */\npackage com.emc.pravega.controller.task.Stream;\n\nimport com.emc.pravega.ClientFactory;\nimport com.emc.pravega.common.concurrent.FutureHelpers;\nimport com.emc.pravega.controller.server.SegmentHelper;\nimport com.emc.pravega.controller.server.eventProcessor.AbortEvent;\nimport com.emc.pravega.controller.server.eventProcessor.CommitEvent;\nimport com.emc.pravega.controller.server.eventProcessor.ControllerEventProcessorConfig;\nimport com.emc.pravega.controller.server.eventProcessor.ControllerEventProcessors;\nimport com.emc.pravega.controller.store.host.HostControllerStore;\nimport com.emc.pravega.controller.store.stream.OperationContext;\nimport com.emc.pravega.controller.store.stream.StreamMetadataStore;\nimport com.emc.pravega.controller.store.stream.TxnStatus;\nimport com.emc.pravega.controller.store.stream.VersionedTransactionData;\nimport com.emc.pravega.controller.store.task.Resource;\nimport com.emc.pravega.controller.store.task.TaskMetadataStore;\nimport com.emc.pravega.controller.task.Task;\nimport com.emc.pravega.controller.task.TaskBase;\nimport com.emc.pravega.stream.EventStreamWriter;\nimport com.emc.pravega.stream.EventWriterConfig;\nimport com.emc.pravega.stream.impl.ClientFactoryImpl;\nimport com.emc.pravega.stream.impl.Controller;\nimport com.emc.pravega.stream.impl.netty.ConnectionFactory;\nimport lombok.extern.slf4j.Slf4j;\n\nimport java.io.Serializable;\nimport java.util.Optional;\nimport java.util.UUID;\nimport java.util.concurrent.CompletableFuture;\nimport java.util.concurrent.ScheduledExecutorService;\nimport java.util.stream.Collectors;\n\nimport static com.emc.pravega.controller.task.Stream.TaskStepsRetryHelper.withRetries;\n\n/**\n * Collection of metadata update tasks on stream.\n * Task methods are annotated with @Task annotation.\n * <p>\n * Any update to the task method signature should be avoided, since it can cause problems during upgrade.\n * Instead, a new overloaded method may be created with the same task annotation name but a new version.\n */\n@Slf4j\npublic class StreamTransactionMetadataTasks extends TaskBase {\n\n    private final StreamMetadataStore streamMetadataStore;\n    private final HostControllerStore hostControllerStore;\n    private final SegmentHelper segmentHelper;\n    private final ConnectionFactory connectionFactory;\n\n    private EventStreamWriter<CommitEvent> commitEventEventStreamWriter;\n    private EventStreamWriter<AbortEvent> abortEventEventStreamWriter;\n\n    public StreamTransactionMetadataTasks(final StreamMetadataStore streamMetadataStore,\n                                          final HostControllerStore hostControllerStore,\n                                          final TaskMetadataStore taskMetadataStore,\n                                          final SegmentHelper segmentHelper, final ScheduledExecutorService executor,\n                                          final String hostId,\n                                          final ConnectionFactory connectionFactory) {\n        this(streamMetadataStore, hostControllerStore, taskMetadataStore, segmentHelper, executor, new Context(hostId), connectionFactory);\n    }\n\n    private StreamTransactionMetadataTasks(final StreamMetadataStore streamMetadataStore,\n                                           final HostControllerStore hostControllerStore,\n                                           final TaskMetadataStore taskMetadataStore,\n                                           SegmentHelper segmentHelper, final ScheduledExecutorService executor,\n                                           final Context context,\n                                           final ConnectionFactory connectionFactory) {\n        super(taskMetadataStore, executor, context);\n        this.streamMetadataStore = streamMetadataStore;\n        this.hostControllerStore = hostControllerStore;\n        this.segmentHelper = segmentHelper;\n        this.connectionFactory = connectionFactory;\n    }\n\n    /**\n     * Initializes stream writers for commit and abort streams.\n     * This method should be called immediately after creating StreamTransactionMetadataTasks object.\n     *\n     * @param controller Local controller reference\n     * @param config Controller event processor configuration.\n     */\n    public Void initializeStreamWriters(Controller controller, ControllerEventProcessorConfig config) {\n\n        ClientFactory clientFactory = new ClientFactoryImpl(config.getScopeName(), controller);\n\n        this.commitEventEventStreamWriter = clientFactory.createEventWriter(\n                config.getCommitStreamName(),\n                ControllerEventProcessors.COMMIT_EVENT_SERIALIZER,\n                EventWriterConfig.builder().build());\n\n        this.abortEventEventStreamWriter = clientFactory.createEventWriter(\n                config.getAbortStreamName(),\n                ControllerEventProcessors.ABORT_EVENT_SERIALIZER,\n                EventWriterConfig.builder().build());\n\n        this.setReady();\n        return null;\n    }\n\n    /**\n     * Create transaction.\n     *\n     * @param scope            stream scope.\n     * @param stream           stream name.\n     * @param lease            Time for which transaction shall remain open with sending any heartbeat.\n     * @param maxExecutionTime Maximum time for which client may extend txn lease.\n     * @param scaleGracePeriod Maximum time for which client may extend txn lease once\n     *                         the scaling operation is initiated on the txn stream.\n     * @param contextOpt       operational context\n     * @return transaction id.\n     */\n    @Task(name = \"createTransaction\", version = \"1.0\", resource = \"{scope}/{stream}\")\n    public CompletableFuture<VersionedTransactionData> createTxn(final String scope, final String stream, final long lease,\n                                            final long maxExecutionTime, final long scaleGracePeriod, final OperationContext contextOpt) {\n        final OperationContext context =\n                contextOpt == null ? streamMetadataStore.createContext(scope, stream) : contextOpt;\n\n        return execute(\n                new Resource(scope, stream),\n                new Serializable[]{scope, stream},\n                () -> createTxnBody(scope, stream, lease, maxExecutionTime, scaleGracePeriod, context));\n    }\n\n    /**\n     * Transaction heartbeat, that increases transaction timeout by lease number of milliseconds.\n     *\n     * @param scope Stream scope.\n     * @param stream Stream name.\n     * @param txId Transaction identifier.\n     * @param lease Amount of time in milliseconds by which to extend the transaction lease.\n     * @param contextOpt       operational context\n     * @return Transaction metadata along with the version of it record in the store.\n     */\n    public CompletableFuture<VersionedTransactionData> pingTxn(final String scope, final String stream,\n                                                               final UUID txId, final long lease,\n                                                               final OperationContext contextOpt) {\n        final OperationContext context =\n                contextOpt == null ? streamMetadataStore.createContext(scope, stream) : contextOpt;\n\n        return execute(\n                new Resource(scope, stream, txId.toString()),\n                new Serializable[]{scope, stream, txId},\n                () -> pingTxnBody(scope, stream, txId, lease, context));\n    }\n\n    /**\n     * Abort transaction.\n     *\n     * @param scope  stream scope.\n     * @param stream stream name.\n     * @param txId   transaction id.\n     * @param version Expected version of the transaction record in the store.\n     * @param contextOpt       operational context\n     * @return true/false.\n     */\n    @Task(name = \"abortTransaction\", version = \"1.0\", resource = \"{scope}/{stream}/{txId}\")\n    public CompletableFuture<TxnStatus> abortTxn(final String scope, final String stream, final UUID txId,\n                                                final Optional<Integer> version, final OperationContext contextOpt) {\n        final OperationContext context = contextOpt == null ? streamMetadataStore.createContext(scope, stream) : contextOpt;\n\n        return execute(\n                new Resource(scope, stream, txId.toString()),\n                new Serializable[]{scope, stream, txId},\n                () -> abortTxnBody(scope, stream, txId, version, context));\n    }\n\n    /**\n     * Commit transaction.\n     *\n     * @param scope      stream scope.\n     * @param stream     stream name.\n     * @param txId       transaction id.\n     * @param contextOpt optional context\n     * @return true/false.\n     */\n    @Task(name = \"commitTransaction\", version = \"1.0\", resource = \"{scope}/{stream}/{txId}\")\n    public CompletableFuture<TxnStatus> commitTxn(final String scope, final String stream, final UUID txId,\n                                                  final OperationContext contextOpt) {\n        final OperationContext context = contextOpt == null ? streamMetadataStore.createContext(scope, stream) : contextOpt;\n\n        return execute(\n                new Resource(scope, stream, txId.toString()),\n                new Serializable[]{scope, stream, txId},\n                () -> commitTxnBody(scope, stream, txId, context));\n    }\n\n    private CompletableFuture<VersionedTransactionData> createTxnBody(final String scope, final String stream,\n                                                                      final long lease, final long maxExecutionPeriod,\n                                                                      final long scaleGracePeriod,\n                                                                      final OperationContext context) {\n        return streamMetadataStore.createTransaction(scope, stream, lease, maxExecutionPeriod, scaleGracePeriod, context, executor)\n                .thenCompose(txData ->\n                        streamMetadataStore.getActiveSegments(scope, stream, context, executor)\n                                .thenCompose(activeSegments ->\n                                        FutureHelpers.allOf(\n                                                activeSegments.stream()\n                                                        .parallel()\n                                                        .map(segment ->\n                                                                notifyTxCreation(scope,\n                                                                        stream,\n                                                                        segment.getNumber(),\n                                                                        txData.getId()))\n                                                        .collect(Collectors.toList())))\n                                .thenApply(x -> txData));\n    }\n\n    private CompletableFuture<VersionedTransactionData> pingTxnBody(String scope, String stream, UUID txId, long lease,\n                                                                    final OperationContext context) {\n        return streamMetadataStore.pingTransaction(scope, stream, txId, lease, context, executor);\n    }\n\n    private CompletableFuture<TxnStatus> abortTxnBody(final String scope, final String stream, final UUID txid,\n                                                      final Optional<Integer> version, final OperationContext context) {\n        return streamMetadataStore.sealTransaction(scope, stream, txid, false, version, context, executor)\n                .thenApplyAsync(status -> {\n                    this.abortEventEventStreamWriter\n                            .writeEvent(txid.toString(), new AbortEvent(scope, stream, txid));\n                    return status;\n                }, executor);\n    }\n\n    private CompletableFuture<TxnStatus> commitTxnBody(final String scope, final String stream, final UUID txid,\n                                                       final OperationContext context) {\n        return streamMetadataStore.sealTransaction(scope, stream, txid, true, Optional.empty(), context, executor)\n                .thenApplyAsync(status -> {\n                    // Todo: this returns an ack future that we dont wait for. How do we know this was complete?\n                    // And the problem is its Future and not completable future. So we cant chain to it here.\n                    this.commitEventEventStreamWriter\n                            .writeEvent(scope + stream, new CommitEvent(scope, stream, txid));\n                    return status;\n                }, executor);\n    }\n\n\n    private CompletableFuture<UUID> notifyTxCreation(final String scope, final String stream, final int segmentNumber, final UUID txid) {\n        return withRetries(() -> segmentHelper.createTransaction(scope,\n                stream,\n                segmentNumber,\n                txid,\n                this.hostControllerStore,\n                this.connectionFactory), executor);\n    }\n\n    @Override\n    public TaskBase copyWithContext(Context context) {\n        return new StreamTransactionMetadataTasks(streamMetadataStore,\n                hostControllerStore,\n                taskMetadataStore,\n                segmentHelper, executor,\n                context,\n                connectionFactory);\n    }\n}\n"
    },
    {
      "path": "controller/server/src/main/java/com/emc/pravega/controller/task/Stream/TestTasks.java",
      "content": "/**\n *\n *  Copyright (c) 2017 Dell Inc., or its subsidiaries.\n *\n */\npackage com.emc.pravega.controller.task.Stream;\n\nimport com.emc.pravega.controller.store.task.Resource;\nimport com.emc.pravega.controller.store.task.TaskMetadataStore;\nimport com.emc.pravega.controller.task.Task;\nimport com.emc.pravega.controller.task.TaskBase;\n\nimport java.io.Serializable;\nimport java.util.concurrent.CompletableFuture;\nimport java.util.concurrent.ScheduledExecutorService;\n\n/**\n * Set of tasks for test purposes.\n */\npublic class TestTasks extends TaskBase {\n\n    public TestTasks(TaskMetadataStore taskMetadataStore, ScheduledExecutorService executor, String hostId) {\n        super(taskMetadataStore, executor, hostId);\n        this.setReady();\n    }\n\n    public TestTasks(TaskMetadataStore taskMetadataStore, ScheduledExecutorService executor, Context context) {\n        super(taskMetadataStore, executor, context);\n        this.setReady();\n    }\n\n    @Task(name = \"test\", version = \"1.0\", resource = \"{scope}/{stream}\")\n    public CompletableFuture<Void> testStreamLock(String scope, String stream) {\n        return execute(\n                new Resource(scope, stream),\n                new Serializable[]{scope, stream},\n                () -> {\n                    try {\n                        Thread.sleep(10000);\n                    } catch (InterruptedException e) {\n                        Thread.currentThread().interrupt();\n                        throw new RuntimeException(e);\n                    }\n                    return  CompletableFuture.completedFuture(null);\n                });\n    }\n\n    @Override\n    public TaskBase copyWithContext(Context context) {\n        return new TestTasks(taskMetadataStore, executor, context);\n    }\n}\n"
    },
    {
      "path": "controller/server/src/main/java/com/emc/pravega/controller/task/TaskBase.java",
      "content": "/**\n *\n *  Copyright (c) 2017 Dell Inc., or its subsidiaries.\n *\n */\npackage com.emc.pravega.controller.task;\n\nimport com.emc.pravega.common.concurrent.FutureHelpers;\nimport com.emc.pravega.controller.store.task.Resource;\nimport com.emc.pravega.controller.store.task.TaggedResource;\nimport com.emc.pravega.controller.store.task.TaskMetadataStore;\n\nimport java.io.Serializable;\nimport java.lang.annotation.Annotation;\nimport java.lang.reflect.Method;\nimport java.util.UUID;\nimport java.util.concurrent.CompletableFuture;\nimport java.util.concurrent.CountDownLatch;\nimport java.util.concurrent.ScheduledExecutorService;\nimport java.util.concurrent.TimeUnit;\n\nimport com.google.common.annotations.VisibleForTesting;\nimport lombok.Data;\nimport lombok.extern.slf4j.Slf4j;\n\n/**\n * TaskBase contains the following.\n * 1. Environment variables used by tasks.\n * 2. Wrapper method that has boilerplate code for locking, persisting task data and executing the task\n *\n * Actual tasks are implemented in sub-classes of TaskBase and annotated with @Task annotation.\n */\n@Slf4j\npublic abstract class TaskBase {\n\n    public interface FutureOperation<T> {\n        CompletableFuture<T> apply();\n    }\n\n    @Data\n    public static class Context {\n        private final String hostId;\n        private final String oldHostId;\n        private final String oldTag;\n        private final Resource oldResource;\n\n        public Context(final String hostId) {\n            this.hostId = hostId;\n            this.oldHostId = null;\n            this.oldTag = null;\n            this.oldResource = null;\n        }\n\n        public Context(final String hostId, final String oldHost, final String oldTag, final Resource oldResource) {\n            this.hostId = hostId;\n            this.oldHostId = oldHost;\n            this.oldTag = oldTag;\n            this.oldResource = oldResource;\n        }\n    }\n\n    protected final ScheduledExecutorService executor;\n\n    protected final Context context;\n\n    protected final TaskMetadataStore taskMetadataStore;\n\n    private volatile boolean ready;\n    private final CountDownLatch readyLatch;\n\n    public TaskBase(final TaskMetadataStore taskMetadataStore, final ScheduledExecutorService executor,\n                    final String hostId) {\n        this(taskMetadataStore, executor, new Context(hostId));\n    }\n\n    protected TaskBase(final TaskMetadataStore taskMetadataStore, final ScheduledExecutorService executor,\n                       final Context context) {\n        this.taskMetadataStore = taskMetadataStore;\n        this.executor = executor;\n        this.context = context;\n        this.ready = false;\n        readyLatch = new CountDownLatch(1);\n    }\n    \n    public abstract TaskBase copyWithContext(Context context);\n\n    public Context getContext() {\n        return this.context;\n    }\n\n    /**\n     * Wrapper method that initially obtains lock then executes the passed method, and finally releases lock.\n     *\n     * @param resource resource to be updated by the task.\n     * @param parameters method parameters.\n     * @param operation lambda operation that is the actual task.\n     * @param <T> type parameter of return value of operation to be executed.\n     * @return return value of task execution.\n     */\n    public <T> CompletableFuture<T> execute(final Resource resource, final Serializable[] parameters, final FutureOperation<T> operation) {\n        if (!ready) {\n            return FutureHelpers.failedFuture(new IllegalStateException(getClass().getName() + \" not yet ready\"));\n        }\n        final String tag = UUID.randomUUID().toString();\n        final TaskData taskData = getTaskData(parameters);\n        final CompletableFuture<T> result = new CompletableFuture<>();\n        final TaggedResource taggedResource = new TaggedResource(tag, resource);\n\n        log.debug(\"Host={}, Tag={} starting to execute task on resource {}\", context.hostId, tag, resource);\n        // PutChild (HostId, resource)\n        // Initially store the fact that I am about the update the resource.\n        // Since multiple threads within this process could concurrently attempt to modify same resource,\n        // we tag the resource name with a random GUID so as not to interfere with other thread's\n        // creation or deletion of resource children under HostId node.\n        taskMetadataStore.putChild(context.hostId, taggedResource)\n                // After storing that fact, lock the resource, execute task and unlock the resource\n                .thenComposeAsync(x -> executeTask(resource, taskData, tag, operation), executor)\n                // finally delete the resource child created under the controller's HostId\n                .whenCompleteAsync((value, e) ->\n                                taskMetadataStore.removeChild(context.hostId, taggedResource, true)\n                                        .whenComplete((innerValue, innerE) -> {\n                                            // ignore the result of removeChile operations, since it is an optimization\n                                            if (e != null) {\n                                                result.completeExceptionally(e);\n                                            } else {\n                                                result.complete(value);\n                                            }\n                                        }),\n                        executor);\n\n        return result;\n    }\n\n    protected void setReady() {\n        ready = true;\n        readyLatch.countDown();\n    }\n\n    @VisibleForTesting\n    public boolean awaitInitialization(long timeout, TimeUnit timeUnit) throws InterruptedException {\n        return readyLatch.await(timeout, timeUnit);\n    }\n\n    private <T> CompletableFuture<T> executeTask(final Resource resource,\n                                                 final TaskData taskData,\n                                                 final String tag,\n                                                 final FutureOperation<T> operation) {\n        final CompletableFuture<T> result = new CompletableFuture<>();\n\n        final CompletableFuture<Void> lockResult = new CompletableFuture<>();\n\n        taskMetadataStore\n                .lock(resource, taskData, context.hostId, tag, context.oldHostId, context.oldTag)\n\n                // On acquiring lock, the following invariants hold\n                // Invariant 1. No other thread within any controller process is running an update task on the resource\n                // Invariant 2. We have denoted the fact that current controller's HostId is updating the resource. This\n                // fact can be used in case current controller instance crashes.\n                // Invariant 3. Any other controller that had created resource child under its HostId, can now be safely\n                // deleted, since that information is redundant and is not useful during that HostId's fail over.\n                .whenComplete((value, e) -> {\n                    // Once I acquire the lock, safe to delete context.oldResource from oldHost, if available\n                    if (e != null) {\n\n                        log.debug(\"Host={}, Tag={} lock attempt on resource {} failed\", context.hostId, tag, resource);\n                        lockResult.completeExceptionally(e);\n\n                    } else {\n\n                        log.debug(\"Host={}, Tag={} acquired lock on resource {}\", context.hostId, tag, resource);\n                        removeOldHostChild(tag).whenComplete((x, y) -> lockResult.complete(value));\n                    }\n                });\n\n        lockResult\n                // Exclusively execute the update task on the resource\n                .thenCompose(y -> operation.apply())\n\n                // If lock had been obtained, unlock it before completing the task.\n                .whenComplete((T value, Throwable e) -> {\n                    if (lockResult.isCompletedExceptionally()) {\n                        // If lock was not obtained, complete the operation with error\n                        result.completeExceptionally(e);\n\n                    } else {\n                        // If lock was obtained, irrespective of result of operation execution,\n                        // release lock before completing operation.\n                        log.debug(\"Host={}, Tag={} completed executing task on resource {}\", context.hostId, tag, resource);\n                        taskMetadataStore.unlock(resource, context.hostId, tag)\n                                .whenComplete((innerValue, innerE) -> {\n                                    log.debug(\"Host={}, Tag={} unlock attempt completed on resource {}\", context.hostId, tag, resource);\n                                    // If lock was acquired above, unlock operation retries until it is released.\n                                    // It throws exception only if non-lock holder tries to release it.\n                                    // Hence ignore result of unlock operation and complete future with previous result.\n                                    if (e != null) {\n                                        result.completeExceptionally(e);\n                                    } else {\n                                        result.complete(value);\n                                    }\n                                });\n                    }\n                });\n        return result;\n    }\n\n    private CompletableFuture<Void> removeOldHostChild(final String tag) {\n        if (context.oldHostId != null && !context.oldHostId.isEmpty()) {\n            log.debug(\"Host={}, Tag={} removing child <{}, {}> of {}\",\n                    context.hostId, tag, context.oldResource, context.oldTag, context.oldHostId);\n            return taskMetadataStore.removeChild(\n                    context.oldHostId,\n                    new TaggedResource(context.oldTag, context.oldResource),\n                    true);\n        } else {\n            return CompletableFuture.completedFuture(null);\n        }\n    }\n\n    private TaskData getTaskData(final Serializable[] parameters) {\n        // Quirk of using stack trace shall be rendered redundant when Task Annotation's handler is coded up.\n        StackTraceElement[] stacktrace = Thread.currentThread().getStackTrace();\n        StackTraceElement e = stacktrace[3];\n        Task annotation = getTaskAnnotation(e.getMethodName());\n        return new TaskData(annotation.name(), annotation.version(), parameters);\n    }\n\n    private Task getTaskAnnotation(final String method) {\n        for (Method m : this.getClass().getMethods()) {\n            if (m.getName().equals(method)) {\n                for (Annotation annotation : m.getDeclaredAnnotations()) {\n                    if (annotation instanceof Task) {\n                        return (Task) annotation;\n                    }\n                }\n                break;\n            }\n        }\n        throw new TaskAnnotationNotFoundException(method);\n    }\n}\n"
    },
    {
      "path": "controller/server/src/test/java/com/emc/pravega/controller/request/RequestTest.java",
      "content": "/**\n * Copyright (c) 2017 Dell Inc., or its subsidiaries.\n */\npackage com.emc.pravega.controller.request;\n\nimport com.emc.pravega.common.concurrent.FutureHelpers;\nimport com.emc.pravega.controller.mocks.SegmentHelperMock;\nimport com.emc.pravega.controller.requesthandler.ScaleRequestHandler;\nimport com.emc.pravega.controller.requests.ScaleRequest;\nimport com.emc.pravega.controller.server.SegmentHelper;\nimport com.emc.pravega.controller.store.host.HostControllerStore;\nimport com.emc.pravega.controller.store.host.HostStoreFactory;\nimport com.emc.pravega.controller.store.host.impl.HostMonitorConfigImpl;\nimport com.emc.pravega.controller.store.stream.Segment;\nimport com.emc.pravega.controller.store.stream.StreamMetadataStore;\nimport com.emc.pravega.controller.store.stream.StreamStoreFactory;\nimport com.emc.pravega.controller.store.task.TaskMetadataStore;\nimport com.emc.pravega.controller.store.task.TaskStoreFactory;\nimport com.emc.pravega.controller.task.Stream.StreamMetadataTasks;\nimport com.emc.pravega.controller.task.Stream.StreamTransactionMetadataTasks;\nimport com.emc.pravega.stream.ScalingPolicy;\nimport com.emc.pravega.stream.StreamConfiguration;\nimport com.emc.pravega.stream.impl.netty.ConnectionFactoryImpl;\nimport org.apache.curator.framework.CuratorFramework;\nimport org.apache.curator.framework.CuratorFrameworkFactory;\nimport org.apache.curator.retry.ExponentialBackoffRetry;\nimport org.apache.curator.test.TestingServer;\nimport org.junit.Before;\nimport org.junit.Test;\n\nimport java.net.InetAddress;\nimport java.net.UnknownHostException;\nimport java.util.List;\nimport java.util.UUID;\nimport java.util.concurrent.ExecutionException;\nimport java.util.concurrent.Executors;\nimport java.util.concurrent.ScheduledExecutorService;\n\nimport static org.junit.Assert.assertTrue;\n\npublic class RequestTest {\n    private final String scope = \"scope\";\n    private final String stream = \"stream\";\n    StreamConfiguration config = StreamConfiguration.builder().scope(scope).streamName(stream).scalingPolicy(\n            ScalingPolicy.byEventRate(0, 2, 3)).build();\n\n    private ScheduledExecutorService executor = Executors.newScheduledThreadPool(100);\n    private StreamMetadataStore streamStore;\n    private TaskMetadataStore taskMetadataStore;\n    private HostControllerStore hostStore;\n    private StreamMetadataTasks streamMetadataTasks;\n    private StreamTransactionMetadataTasks streamTransactionMetadataTasks;\n\n    private TestingServer zkServer;\n\n    private CuratorFramework zkClient;\n\n    @Before\n    public void createStream() throws Exception {\n        zkServer = new TestingServer();\n        zkServer.start();\n\n        zkClient = CuratorFrameworkFactory.newClient(zkServer.getConnectString(),\n                new ExponentialBackoffRetry(20, 1, 50));\n\n        zkClient.start();\n\n        String hostId;\n        try {\n            //On each controller process restart, it gets a fresh hostId,\n            //which is a combination of hostname and random GUID.\n            hostId = InetAddress.getLocalHost().getHostAddress() + UUID.randomUUID().toString();\n        } catch (UnknownHostException e) {\n            hostId = UUID.randomUUID().toString();\n        }\n\n        streamStore = StreamStoreFactory.createZKStore(zkClient, executor);\n\n        taskMetadataStore = TaskStoreFactory.createZKStore(zkClient, executor);\n\n        hostStore = HostStoreFactory.createInMemoryStore(HostMonitorConfigImpl.dummyConfig());\n\n        SegmentHelper segmentHelper = SegmentHelperMock.getSegmentHelperMock();\n        ConnectionFactoryImpl connectionFactory = new ConnectionFactoryImpl(false);\n        streamMetadataTasks = new StreamMetadataTasks(streamStore, hostStore, taskMetadataStore, segmentHelper,\n                executor, hostId, connectionFactory);\n        streamTransactionMetadataTasks = new StreamTransactionMetadataTasks(streamStore, hostStore, taskMetadataStore,\n                segmentHelper, executor, hostId, connectionFactory);\n\n        long createTimestamp = System.currentTimeMillis();\n\n        // add a host in zk\n        // mock pravega\n        // create a stream\n        streamStore.createScope(scope);\n        streamMetadataTasks.createStream(scope, stream, config, createTimestamp).get();\n    }\n\n    @Test(timeout = 10000)\n    public void testScaleRequest() throws ExecutionException, InterruptedException {\n        ScaleRequestHandler requestHandler = new ScaleRequestHandler(streamMetadataTasks, streamStore, streamTransactionMetadataTasks, executor);\n        ScaleRequest request = new ScaleRequest(scope, stream, 2, ScaleRequest.UP, System.currentTimeMillis(), 2, false);\n\n        assertTrue(FutureHelpers.await(requestHandler.process(request)));\n        List<Segment> activeSegments = streamStore.getActiveSegments(scope, stream, null, executor).get();\n\n        assertTrue(activeSegments.stream().noneMatch(z -> z.getNumber() == 2));\n        assertTrue(activeSegments.stream().anyMatch(z -> z.getNumber() == 3));\n        assertTrue(activeSegments.stream().anyMatch(z -> z.getNumber() == 4));\n        assertTrue(activeSegments.size() == 4);\n\n        request = new ScaleRequest(scope, stream, 4, ScaleRequest.DOWN, System.currentTimeMillis(), 0, false);\n\n        assertTrue(FutureHelpers.await(requestHandler.process(request)));\n        activeSegments = streamStore.getActiveSegments(scope, stream, null, executor).get();\n\n        assertTrue(activeSegments.stream().anyMatch(z -> z.getNumber() == 4));\n        assertTrue(activeSegments.size() == 4);\n\n        request = new ScaleRequest(scope, stream, 3, ScaleRequest.DOWN, System.currentTimeMillis(), 0, false);\n\n        assertTrue(FutureHelpers.await(requestHandler.process(request)));\n        activeSegments = streamStore.getActiveSegments(scope, stream, null, executor).get();\n\n        assertTrue(activeSegments.stream().noneMatch(z -> z.getNumber() == 3));\n        assertTrue(activeSegments.stream().noneMatch(z -> z.getNumber() == 4));\n        assertTrue(activeSegments.stream().anyMatch(z -> z.getNumber() == 5));\n        assertTrue(activeSegments.size() == 3);\n    }\n}\n"
    },
    {
      "path": "controller/server/src/test/java/com/emc/pravega/controller/server/v1/ControllerServiceImplTest.java",
      "content": "/**\n *\n *  Copyright (c) 2017 Dell Inc., or its subsidiaries.\n *\n */\npackage com.emc.pravega.controller.server.v1;\n\nimport com.emc.pravega.controller.server.rpc.grpc.v1.ControllerServiceImpl;\nimport com.emc.pravega.controller.stream.api.grpc.v1.Controller;\nimport com.emc.pravega.controller.stream.api.grpc.v1.Controller.CreateScopeStatus;\nimport com.emc.pravega.controller.stream.api.grpc.v1.Controller.CreateStreamStatus;\nimport com.emc.pravega.controller.stream.api.grpc.v1.Controller.DeleteScopeStatus;\nimport com.emc.pravega.controller.stream.api.grpc.v1.Controller.DeleteStreamStatus;\nimport com.emc.pravega.controller.stream.api.grpc.v1.Controller.UpdateStreamStatus;\nimport com.emc.pravega.stream.ScalingPolicy;\nimport com.emc.pravega.stream.StreamConfiguration;\nimport com.emc.pravega.stream.impl.ModelHelper;\nimport io.grpc.stub.StreamObserver;\n\nimport java.io.IOException;\n\nimport org.junit.After;\nimport org.junit.Assert;\nimport org.junit.Before;\nimport org.junit.Rule;\nimport org.junit.Test;\nimport org.junit.rules.Timeout;\n\nimport java.util.concurrent.TimeUnit;\nimport java.util.concurrent.atomic.AtomicBoolean;\n\nimport static org.junit.Assert.assertEquals;\n\n/**\n * Controller Service Implementation tests.\n * <p>\n * Every test is run twice for both streamStore (Zookeeper and InMemory) types.\n */\npublic abstract class ControllerServiceImplTest {\n\n    private static final String SCOPE1 = \"scope1\";\n    private static final String SCOPE2 = \"scope2\";\n    private static final String SCOPE3 = \"scope3\";\n    private static final String STREAM1 = \"stream1\";\n    private static final String STREAM2 = \"stream2\";\n\n    //Ensure each test completes within 10 seconds.\n    @Rule\n    public final Timeout globalTimeout = new Timeout(10, TimeUnit.SECONDS);\n\n    ControllerServiceImpl controllerService;\n\n    @Before\n    public abstract void setupStore() throws Exception;\n\n    @After\n    public abstract void cleanupStore() throws IOException;\n\n    @Test\n    public void createScopeTests() {\n        CreateScopeStatus status;\n\n        // region createScope\n        ResultObserver<CreateScopeStatus> result1 = new ResultObserver<>();\n        this.controllerService.createScope(ModelHelper.createScopeInfo(SCOPE1), result1);\n        status = result1.get();\n        assertEquals(status.getStatus(), CreateScopeStatus.Status.SUCCESS);\n\n        ResultObserver<CreateScopeStatus> result2 = new ResultObserver<>();\n        this.controllerService.createScope(ModelHelper.createScopeInfo(SCOPE2), result2);\n        status = result2.get();\n        assertEquals(status.getStatus(), CreateScopeStatus.Status.SUCCESS);\n        // endregion\n\n        // region duplicate create scope\n        ResultObserver<CreateScopeStatus> result3 = new ResultObserver<>();\n        this.controllerService.createScope(ModelHelper.createScopeInfo(SCOPE2), result3);\n        status = result3.get();\n        assertEquals(status.getStatus(), CreateScopeStatus.Status.SCOPE_EXISTS);\n        // endregion\n\n        // region with invalid scope with name \"abc/def'\n        ResultObserver<CreateScopeStatus> result4 = new ResultObserver<>();\n        this.controllerService.createScope(ModelHelper.createScopeInfo(\"abc/def\"), result4);\n        status = result4.get();\n        assertEquals(status.getStatus(), CreateScopeStatus.Status.INVALID_SCOPE_NAME);\n        // endregion\n    }\n\n    @Test\n    public void deleteScopeTests() {\n        CreateScopeStatus createScopeStatus;\n        DeleteScopeStatus deleteScopeStatus;\n        CreateStreamStatus createStreamStatus;\n\n        // Delete empty scope (containing no streams) SCOPE3\n        ResultObserver<CreateScopeStatus> result1 = new ResultObserver<>();\n        this.controllerService.createScope(ModelHelper.createScopeInfo(SCOPE3), result1);\n        createScopeStatus = result1.get();\n        assertEquals(\"Create Scope\", CreateScopeStatus.Status.SUCCESS, createScopeStatus.getStatus());\n\n        ResultObserver<DeleteScopeStatus> result2 = new ResultObserver<>();\n        this.controllerService.deleteScope(ModelHelper.createScopeInfo(SCOPE3), result2);\n        deleteScopeStatus = result2.get();\n        assertEquals(\"Delete Empty scope\", DeleteScopeStatus.Status.SUCCESS, deleteScopeStatus.getStatus());\n\n        // To verify that SCOPE3 is infact deleted in above delete call\n        ResultObserver<DeleteScopeStatus> result7 = new ResultObserver<>();\n        this.controllerService.deleteScope(ModelHelper.createScopeInfo(SCOPE3), result7);\n        deleteScopeStatus = result7.get();\n        assertEquals(\"Verify that Scope3 is infact deleted\", DeleteScopeStatus.Status.SCOPE_NOT_FOUND,\n                     deleteScopeStatus.getStatus());\n\n        // Delete Non-empty Scope SCOPE2\n        ResultObserver<CreateScopeStatus> result3 = new ResultObserver<>();\n        this.controllerService.createScope(ModelHelper.createScopeInfo(SCOPE2), result3);\n        createScopeStatus = result3.get();\n        assertEquals(\"Create Scope\", CreateScopeStatus.Status.SUCCESS, createScopeStatus.getStatus());\n\n        final ScalingPolicy policy1 = ScalingPolicy.fixed(2);\n        final StreamConfiguration configuration1 =\n                StreamConfiguration.builder().scope(SCOPE2).streamName(STREAM1).scalingPolicy(policy1).build();\n        ResultObserver<CreateStreamStatus> result4 = new ResultObserver<>();\n        this.controllerService.createStream(ModelHelper.decode(configuration1), result4);\n        createStreamStatus = result4.get();\n        assertEquals(createStreamStatus.getStatus(), CreateStreamStatus.Status.SUCCESS);\n\n        ResultObserver<DeleteScopeStatus> result5 = new ResultObserver<>();\n        this.controllerService.deleteScope(ModelHelper.createScopeInfo(SCOPE2), result5);\n        deleteScopeStatus = result5.get();\n        assertEquals(\"Delete non empty scope\", DeleteScopeStatus.Status.SCOPE_NOT_EMPTY, deleteScopeStatus.getStatus());\n\n        // Delete Non-existent scope SCOPE3\n        ResultObserver<DeleteScopeStatus> result6 = new ResultObserver<>();\n        this.controllerService.deleteScope(ModelHelper.createScopeInfo(\"SCOPE3\"), result6);\n        deleteScopeStatus = result6.get();\n        assertEquals(\"Delete non existent scope\", DeleteScopeStatus.Status.SCOPE_NOT_FOUND,\n                     deleteScopeStatus.getStatus());\n    }\n\n    @Test\n    public void createStreamTests() {\n        final ScalingPolicy policy1 = ScalingPolicy.fixed(2);\n        final ScalingPolicy policy2 = ScalingPolicy.fixed(3);\n        final StreamConfiguration configuration1 =\n                StreamConfiguration.builder().scope(SCOPE1).streamName(STREAM1).scalingPolicy(policy1).build();\n        final StreamConfiguration configuration2 =\n                StreamConfiguration.builder().scope(SCOPE1).streamName(STREAM2).scalingPolicy(policy2).build();\n        final StreamConfiguration configuration3 =\n                StreamConfiguration.builder().scope(\"SCOPE3\").streamName(STREAM2).scalingPolicy(policy2).build();\n\n        CreateStreamStatus status;\n\n        // region checkStream\n        ResultObserver<CreateScopeStatus> result = new ResultObserver<>();\n        this.controllerService.createScope(Controller.ScopeInfo.newBuilder().setScope(SCOPE1).build(), result);\n        Assert.assertEquals(result.get().getStatus(), CreateScopeStatus.Status.SUCCESS);\n\n        ResultObserver<CreateStreamStatus> result1 = new ResultObserver<>();\n        this.controllerService.createStream(ModelHelper.decode(configuration1), result1);\n        status = result1.get();\n        Assert.assertEquals(status.getStatus(), CreateStreamStatus.Status.SUCCESS);\n\n        ResultObserver<CreateStreamStatus> result2 = new ResultObserver<>();\n        this.controllerService.createStream(ModelHelper.decode(configuration2), result2);\n        status = result2.get();\n        Assert.assertEquals(status.getStatus(), CreateStreamStatus.Status.SUCCESS);\n        // endregion\n\n        // region duplicate create stream\n        ResultObserver<CreateStreamStatus> result3 = new ResultObserver<>();\n        this.controllerService.createStream(ModelHelper.decode(configuration1), result3);\n        status = result3.get();\n        Assert.assertEquals(status.getStatus(), CreateStreamStatus.Status.STREAM_EXISTS);\n        // endregion\n\n        // create stream for non-existent scope\n        ResultObserver<CreateStreamStatus> result4 = new ResultObserver<>();\n        this.controllerService.createStream(ModelHelper.decode(configuration3), result4);\n        status = result4.get();\n        Assert.assertEquals(status.getStatus(), CreateStreamStatus.Status.SCOPE_NOT_FOUND);\n\n        //create stream with invalid stream name \"abc/def\"\n        ResultObserver<CreateStreamStatus> result5 = new ResultObserver<>();\n        final StreamConfiguration configuration4 =\n                StreamConfiguration.builder().scope(\"SCOPE3\").streamName(\"abc/def\").scalingPolicy(policy2).build();\n        this.controllerService.createStream(ModelHelper.decode(configuration4), result5);\n        status = result5.get();\n        assertEquals(status.getStatus(), CreateStreamStatus.Status.INVALID_STREAM_NAME);\n    }\n\n    @Test\n    public void deleteStreamTests() {\n        CreateScopeStatus createScopeStatus;\n        CreateStreamStatus createStreamStatus;\n        DeleteStreamStatus deleteStreamStatus;\n        final StreamConfiguration configuration1 =\n                StreamConfiguration.builder().scope(SCOPE1).streamName(STREAM1).scalingPolicy(ScalingPolicy.fixed(4))\n                        .build();\n\n        // Create a test scope.\n        ResultObserver<CreateScopeStatus> result1 = new ResultObserver<>();\n        this.controllerService.createScope(ModelHelper.createScopeInfo(SCOPE1), result1);\n        createScopeStatus = result1.get();\n        assertEquals(\"Create Scope\", CreateScopeStatus.Status.SUCCESS, createScopeStatus.getStatus());\n\n        // Try deleting a non-existent stream.\n        ResultObserver<DeleteStreamStatus> result2 = new ResultObserver<>();\n        this.controllerService.deleteStream(ModelHelper.createStreamInfo(SCOPE3, \"dummyStream\"), result2);\n        deleteStreamStatus = result2.get();\n        assertEquals(\"Delete Non-existent stream\",\n                DeleteStreamStatus.Status.STREAM_NOT_FOUND, deleteStreamStatus.getStatus());\n\n        // Try deleting a non-existent stream.\n        ResultObserver<DeleteStreamStatus> result3 = new ResultObserver<>();\n        this.controllerService.deleteStream(ModelHelper.createStreamInfo(\"dummyScope\", \"dummyStream\"), result3);\n        deleteStreamStatus = result3.get();\n        assertEquals(\"Delete Non-existent stream\",\n                DeleteStreamStatus.Status.STREAM_NOT_FOUND, deleteStreamStatus.getStatus());\n\n        // Create a test stream.\n        ResultObserver<CreateStreamStatus> result4 = new ResultObserver<>();\n        this.controllerService.createStream(ModelHelper.decode(configuration1), result4);\n        createStreamStatus = result4.get();\n        Assert.assertEquals(\"Create stream\",\n                CreateStreamStatus.Status.SUCCESS, createStreamStatus.getStatus());\n\n        // Try deleting the test stream without sealing it first.\n        ResultObserver<DeleteStreamStatus> result5 = new ResultObserver<>();\n        this.controllerService.deleteStream(ModelHelper.createStreamInfo(SCOPE1, STREAM1), result5);\n        deleteStreamStatus = result5.get();\n        assertEquals(\"Delete non-sealed stream\",\n                DeleteStreamStatus.Status.STREAM_NOT_SEALED, deleteStreamStatus.getStatus());\n\n        // Seal the test stream.\n        ResultObserver<UpdateStreamStatus> result6 = new ResultObserver<>();\n        this.controllerService.sealStream(ModelHelper.createStreamInfo(SCOPE1, STREAM1), result6);\n        UpdateStreamStatus updateStreamStatus = result6.get();\n        assertEquals(\"Seal stream\", UpdateStreamStatus.Status.SUCCESS, updateStreamStatus.getStatus());\n\n        // Delete the sealed stream.\n        ResultObserver<DeleteStreamStatus> result7 = new ResultObserver<>();\n        this.controllerService.deleteStream(ModelHelper.createStreamInfo(SCOPE1, STREAM1), result7);\n        deleteStreamStatus = result7.get();\n        assertEquals(\"Delete sealed stream\", DeleteStreamStatus.Status.SUCCESS, deleteStreamStatus.getStatus());\n    }\n\n    @Test\n    public void sealStreamTests() {\n        CreateScopeStatus createScopeStatus;\n        CreateStreamStatus createStreamStatus;\n        UpdateStreamStatus updateStreamStatus;\n\n        final StreamConfiguration configuration1 =\n                StreamConfiguration.builder().scope(SCOPE1).streamName(STREAM1).scalingPolicy(ScalingPolicy.fixed(4))\n                        .build();\n\n        // Create a test scope.\n        ResultObserver<CreateScopeStatus> result1 = new ResultObserver<>();\n        this.controllerService.createScope(ModelHelper.createScopeInfo(SCOPE1), result1);\n        createScopeStatus = result1.get();\n        assertEquals(\"Create Scope\", CreateScopeStatus.Status.SUCCESS, createScopeStatus.getStatus());\n\n        // Create a test stream.\n        ResultObserver<CreateStreamStatus> result2 = new ResultObserver<>();\n        this.controllerService.createStream(ModelHelper.decode(configuration1), result2);\n        createStreamStatus = result2.get();\n        assertEquals(\"Create stream\", CreateStreamStatus.Status.SUCCESS, createStreamStatus.getStatus());\n\n        // Seal a test stream.\n        ResultObserver<UpdateStreamStatus> result3 = new ResultObserver<>();\n        this.controllerService.sealStream(ModelHelper.createStreamInfo(SCOPE1, STREAM1), result3);\n        updateStreamStatus = result3.get();\n        assertEquals(\"Seal Stream\", UpdateStreamStatus.Status.SUCCESS, updateStreamStatus.getStatus());\n\n        // Seal a non-existent stream.\n        ResultObserver<UpdateStreamStatus> result4 = new ResultObserver<>();\n        this.controllerService.sealStream(ModelHelper.createStreamInfo(SCOPE1, \"dummyStream\"), result4);\n        updateStreamStatus = result4.get();\n        assertEquals(\"Seal non-existent stream\",\n                UpdateStreamStatus.Status.STREAM_NOT_FOUND, updateStreamStatus.getStatus());\n\n        // Seal a non-existent stream.\n        ResultObserver<UpdateStreamStatus> result5 = new ResultObserver<>();\n        this.controllerService.sealStream(ModelHelper.createStreamInfo(\"dummyScope\", STREAM1), result5);\n        updateStreamStatus = result5.get();\n        assertEquals(\"Seal non-existent stream\",\n                UpdateStreamStatus.Status.STREAM_NOT_FOUND, updateStreamStatus.getStatus());\n    }\n\n    private static class ResultObserver<T> implements StreamObserver<T> {\n        private T result = null;\n        private final AtomicBoolean completed = new AtomicBoolean(false);\n\n        @Override\n        public void onNext(T value) {\n            result = value;\n        }\n\n        @Override\n        public void onError(Throwable t) {\n        }\n\n        @Override\n        public void onCompleted() {\n            synchronized (this) {\n                completed.set(true);\n                this.notifyAll();\n            }\n        }\n\n        public T get() {\n            synchronized (this) {\n                while (!completed.get()) {\n                    try {\n                        this.wait();\n                    } catch (InterruptedException e) {\n                        return null;\n                    }\n                }\n            }\n            return result;\n        }\n    }\n}\n"
    },
    {
      "path": "controller/server/src/test/java/com/emc/pravega/controller/server/v1/ControllerServiceTest.java",
      "content": "/**\n * Copyright (c) 2017 Dell Inc., or its subsidiaries.\n */\npackage com.emc.pravega.controller.server.v1;\n\nimport com.emc.pravega.controller.mocks.SegmentHelperMock;\nimport com.emc.pravega.controller.server.ControllerService;\nimport com.emc.pravega.controller.server.SegmentHelper;\nimport com.emc.pravega.controller.store.host.HostControllerStore;\nimport com.emc.pravega.controller.store.host.HostStoreFactory;\nimport com.emc.pravega.controller.store.host.impl.HostMonitorConfigImpl;\nimport com.emc.pravega.controller.store.stream.StreamMetadataStore;\nimport com.emc.pravega.controller.store.stream.StreamStoreFactory;\nimport com.emc.pravega.controller.store.task.TaskMetadataStore;\nimport com.emc.pravega.controller.store.task.TaskStoreFactory;\nimport com.emc.pravega.controller.stream.api.grpc.v1.Controller.SegmentId;\nimport com.emc.pravega.controller.task.Stream.StreamMetadataTasks;\nimport com.emc.pravega.controller.task.Stream.StreamTransactionMetadataTasks;\nimport com.emc.pravega.controller.timeout.TimeoutService;\nimport com.emc.pravega.controller.timeout.TimeoutServiceConfig;\nimport com.emc.pravega.controller.timeout.TimerWheelTimeoutService;\nimport com.emc.pravega.stream.ScalingPolicy;\nimport com.emc.pravega.stream.StreamConfiguration;\nimport com.emc.pravega.stream.impl.ModelHelper;\nimport com.emc.pravega.stream.impl.netty.ConnectionFactoryImpl;\nimport java.io.IOException;\nimport java.util.AbstractMap.SimpleEntry;\nimport java.util.Arrays;\nimport java.util.Collections;\nimport java.util.Map;\nimport java.util.concurrent.ExecutionException;\nimport java.util.concurrent.Executors;\nimport java.util.concurrent.ScheduledExecutorService;\nimport org.apache.curator.framework.CuratorFramework;\nimport org.apache.curator.framework.CuratorFrameworkFactory;\nimport org.apache.curator.retry.ExponentialBackoffRetry;\nimport org.apache.curator.test.TestingServer;\nimport org.junit.After;\nimport org.junit.Before;\nimport org.junit.Test;\n\nimport static org.junit.Assert.assertEquals;\n\n/**\n * Controller service implementation test.\n */\npublic class ControllerServiceTest {\n\n    private static final String SCOPE = \"scope\";\n    private final String stream1 = \"stream1\";\n    private final String stream2 = \"stream2\";\n    private final ScheduledExecutorService executor = Executors.newScheduledThreadPool(10);\n\n    private final StreamMetadataStore streamStore = StreamStoreFactory.createInMemoryStore(executor);\n\n    private final ControllerService consumer;\n\n    private final TestingServer zkServer;\n\n    public ControllerServiceTest() throws Exception {\n        zkServer = new TestingServer();\n        zkServer.start();\n        CuratorFramework zkClient = CuratorFrameworkFactory.newClient(zkServer.getConnectString(),\n                new ExponentialBackoffRetry(200, 10, 5000));\n        zkClient.start();\n\n        final TaskMetadataStore taskMetadataStore = TaskStoreFactory.createZKStore(zkClient, executor);\n        final HostControllerStore hostStore = HostStoreFactory.createInMemoryStore(HostMonitorConfigImpl.dummyConfig());\n\n        SegmentHelper segmentHelper = SegmentHelperMock.getSegmentHelperMock();\n        ConnectionFactoryImpl connectionFactory = new ConnectionFactoryImpl(false);\n        StreamMetadataTasks streamMetadataTasks = new StreamMetadataTasks(streamStore, hostStore,\n                taskMetadataStore, segmentHelper, executor, \"host\", connectionFactory);\n        StreamTransactionMetadataTasks streamTransactionMetadataTasks = new StreamTransactionMetadataTasks(streamStore,\n                hostStore, taskMetadataStore, segmentHelper, executor, \"host\", connectionFactory);\n        TimeoutService timeoutService = new TimerWheelTimeoutService(streamTransactionMetadataTasks,\n                TimeoutServiceConfig.defaultConfig());\n\n        consumer = new ControllerService(streamStore, hostStore, streamMetadataTasks, streamTransactionMetadataTasks,\n                timeoutService, new SegmentHelper(), executor);\n    }\n\n    @Before\n    public void prepareStreamStore() throws ExecutionException, InterruptedException {\n\n        final ScalingPolicy policy1 = ScalingPolicy.fixed(2);\n        final ScalingPolicy policy2 = ScalingPolicy.fixed(3);\n        final StreamConfiguration configuration1 = StreamConfiguration.builder().scope(SCOPE).streamName(stream1).scalingPolicy(policy1).build();\n        final StreamConfiguration configuration2 = StreamConfiguration.builder().scope(SCOPE).streamName(stream2).scalingPolicy(policy2).build();\n\n        // createScope\n        streamStore.createScope(SCOPE).get();\n\n        // region createStream\n        streamStore.createStream(SCOPE, stream1, configuration1, System.currentTimeMillis(), null, executor).get();\n        streamStore.createStream(SCOPE, stream2, configuration2, System.currentTimeMillis(), null, executor).get();\n        // endregion\n\n        // region scaleSegments\n\n        SimpleEntry<Double, Double> segment1 = new SimpleEntry<>(0.5, 0.75);\n        SimpleEntry<Double, Double> segment2 = new SimpleEntry<>(0.75, 1.0);\n        streamStore.scale(SCOPE, stream1, Collections.singletonList(1), Arrays.asList(segment1, segment2), 20, null, executor).get();\n\n        SimpleEntry<Double, Double> segment3 = new SimpleEntry<>(0.0, 0.5);\n        SimpleEntry<Double, Double> segment4 = new SimpleEntry<>(0.5, 0.75);\n        SimpleEntry<Double, Double> segment5 = new SimpleEntry<>(0.75, 1.0);\n        streamStore.scale(SCOPE, stream2, Arrays.asList(0, 1, 2), Arrays.asList(segment3, segment4, segment5), 20, null, executor).get();\n        // endregion\n    }\n\n    @After\n    public void stopZKServer() throws IOException {\n        zkServer.close();\n    }\n\n    @Test\n    public void testMethods() throws InterruptedException, ExecutionException {\n        Map<SegmentId, Long> segments;\n\n        segments = consumer.getSegmentsAtTime(SCOPE, stream1, 10).get();\n        assertEquals(2, segments.size());\n        assertEquals(Long.valueOf(0), segments.get(ModelHelper.createSegmentId(SCOPE, stream1, 0)));\n        assertEquals(Long.valueOf(0), segments.get(ModelHelper.createSegmentId(SCOPE, stream1, 1)));\n\n        segments = consumer.getSegmentsAtTime(SCOPE, stream2, 10).get();\n        assertEquals(3, segments.size());\n        assertEquals(Long.valueOf(0), segments.get(ModelHelper.createSegmentId(SCOPE, stream2, 0)));\n        assertEquals(Long.valueOf(0), segments.get(ModelHelper.createSegmentId(SCOPE, stream2, 1)));\n        assertEquals(Long.valueOf(0), segments.get(ModelHelper.createSegmentId(SCOPE, stream2, 2)));\n\n        segments = consumer.getSegmentsAtTime(SCOPE, stream1, 25).get();\n        assertEquals(3, segments.size());\n        assertEquals(Long.valueOf(0), segments.get(ModelHelper.createSegmentId(SCOPE, stream1, 0)));\n        assertEquals(Long.valueOf(0), segments.get(ModelHelper.createSegmentId(SCOPE, stream1, 2)));\n        assertEquals(Long.valueOf(0), segments.get(ModelHelper.createSegmentId(SCOPE, stream1, 3)));\n\n        segments = consumer.getSegmentsAtTime(SCOPE, stream2, 25).get();\n        assertEquals(3, segments.size());\n        assertEquals(Long.valueOf(0), segments.get(ModelHelper.createSegmentId(SCOPE, stream2, 3)));\n        assertEquals(Long.valueOf(0), segments.get(ModelHelper.createSegmentId(SCOPE, stream2, 4)));\n        assertEquals(Long.valueOf(0), segments.get(ModelHelper.createSegmentId(SCOPE, stream2, 5)));\n    }\n}\n"
    },
    {
      "path": "controller/server/src/test/java/com/emc/pravega/controller/server/v1/InMemoryControllerServiceAsyncImplTest.java",
      "content": "/**\n * Copyright (c) 2017 Dell Inc., or its subsidiaries.\n */\npackage com.emc.pravega.controller.server.v1;\n\nimport com.emc.pravega.controller.mocks.SegmentHelperMock;\nimport com.emc.pravega.controller.server.ControllerService;\nimport com.emc.pravega.controller.server.SegmentHelper;\nimport com.emc.pravega.controller.server.rpc.grpc.v1.ControllerServiceImpl;\nimport com.emc.pravega.controller.store.host.HostControllerStore;\nimport com.emc.pravega.controller.store.host.HostStoreFactory;\nimport com.emc.pravega.controller.store.host.impl.HostMonitorConfigImpl;\nimport com.emc.pravega.controller.store.stream.StreamMetadataStore;\nimport com.emc.pravega.controller.store.stream.StreamStoreFactory;\nimport com.emc.pravega.controller.store.task.TaskMetadataStore;\nimport com.emc.pravega.controller.store.task.TaskStoreFactory;\nimport com.emc.pravega.controller.task.Stream.StreamMetadataTasks;\nimport com.emc.pravega.controller.task.Stream.StreamTransactionMetadataTasks;\nimport com.emc.pravega.controller.timeout.TimeoutService;\nimport com.emc.pravega.controller.timeout.TimeoutServiceConfig;\nimport com.emc.pravega.controller.timeout.TimerWheelTimeoutService;\nimport com.emc.pravega.stream.impl.netty.ConnectionFactoryImpl;\nimport com.google.common.util.concurrent.ThreadFactoryBuilder;\n\nimport java.io.IOException;\nimport java.util.concurrent.Executors;\nimport java.util.concurrent.ScheduledExecutorService;\n\n/**\n * InMemory stream store configuration.\n */\npublic class InMemoryControllerServiceAsyncImplTest extends ControllerServiceImplTest {\n\n    private TaskMetadataStore taskMetadataStore;\n    private HostControllerStore hostStore;\n    private StreamMetadataTasks streamMetadataTasks;\n    private ScheduledExecutorService executorService;\n    private StreamTransactionMetadataTasks streamTransactionMetadataTasks;\n    private StreamMetadataStore streamStore;\n    private SegmentHelper segmentHelper;\n    private TimeoutService timeoutService;\n\n    @Override\n    public void setupStore() throws Exception {\n\n        executorService = Executors.newScheduledThreadPool(20,\n                new ThreadFactoryBuilder().setNameFormat(\"testpool-%d\").build());\n        taskMetadataStore = TaskStoreFactory.createInMemoryStore(executorService);\n        hostStore = HostStoreFactory.createInMemoryStore(HostMonitorConfigImpl.dummyConfig());\n        streamStore = StreamStoreFactory.createInMemoryStore(executorService);\n        segmentHelper = SegmentHelperMock.getSegmentHelperMock();\n\n        ConnectionFactoryImpl connectionFactory = new ConnectionFactoryImpl(false);\n        streamMetadataTasks = new StreamMetadataTasks(streamStore, hostStore, taskMetadataStore, segmentHelper,\n                executorService, \"host\", connectionFactory);\n\n        streamTransactionMetadataTasks = new StreamTransactionMetadataTasks(\n                streamStore, hostStore, taskMetadataStore, segmentHelper, executorService, \"host\", connectionFactory);\n\n        timeoutService = new TimerWheelTimeoutService(streamTransactionMetadataTasks,\n                TimeoutServiceConfig.defaultConfig());\n        controllerService = new ControllerServiceImpl(\n                new ControllerService(streamStore, hostStore, streamMetadataTasks, streamTransactionMetadataTasks,\n                                      timeoutService, new SegmentHelper(), executorService));\n    }\n\n    @Override\n    public void cleanupStore() throws IOException {\n        executorService.shutdown();\n    }\n}\n"
    },
    {
      "path": "controller/server/src/test/java/com/emc/pravega/controller/server/v1/ZKControllerServiceAsyncImplTest.java",
      "content": "/**\n * Copyright (c) 2017 Dell Inc., or its subsidiaries.\n */\npackage com.emc.pravega.controller.server.v1;\n\nimport com.emc.pravega.controller.mocks.SegmentHelperMock;\nimport com.emc.pravega.controller.server.ControllerService;\nimport com.emc.pravega.controller.server.SegmentHelper;\nimport com.emc.pravega.controller.server.rpc.grpc.v1.ControllerServiceImpl;\nimport com.emc.pravega.controller.store.client.StoreClient;\nimport com.emc.pravega.controller.store.client.StoreClientFactory;\nimport com.emc.pravega.controller.store.host.HostControllerStore;\nimport com.emc.pravega.controller.store.host.HostStoreFactory;\nimport com.emc.pravega.controller.store.host.impl.HostMonitorConfigImpl;\nimport com.emc.pravega.controller.store.stream.StreamMetadataStore;\nimport com.emc.pravega.controller.store.stream.StreamStoreFactory;\nimport com.emc.pravega.controller.store.task.TaskMetadataStore;\nimport com.emc.pravega.controller.store.task.TaskStoreFactory;\nimport com.emc.pravega.controller.task.Stream.StreamMetadataTasks;\nimport com.emc.pravega.controller.task.Stream.StreamTransactionMetadataTasks;\nimport com.emc.pravega.controller.timeout.TimeoutService;\nimport com.emc.pravega.controller.timeout.TimeoutServiceConfig;\nimport com.emc.pravega.controller.timeout.TimerWheelTimeoutService;\nimport com.emc.pravega.stream.impl.netty.ConnectionFactoryImpl;\nimport com.google.common.util.concurrent.ThreadFactoryBuilder;\nimport org.apache.curator.framework.CuratorFramework;\nimport org.apache.curator.framework.CuratorFrameworkFactory;\nimport org.apache.curator.retry.ExponentialBackoffRetry;\nimport org.apache.curator.test.TestingServer;\n\nimport java.io.IOException;\nimport java.util.concurrent.Executors;\nimport java.util.concurrent.ScheduledExecutorService;\n\n/**\n * Zookeeper stream store configuration.\n */\npublic class ZKControllerServiceAsyncImplTest extends ControllerServiceImplTest {\n\n    private TestingServer zkServer;\n    private CuratorFramework zkClient;\n    private StoreClient storeClient;\n    private TaskMetadataStore taskMetadataStore;\n    private HostControllerStore hostStore;\n    private StreamMetadataTasks streamMetadataTasks;\n    private ScheduledExecutorService executorService;\n    private StreamTransactionMetadataTasks streamTransactionMetadataTasks;\n    private StreamMetadataStore streamStore;\n    private SegmentHelper segmentHelper;\n    private TimeoutService timeoutService;\n\n    @Override\n    public void setupStore() throws Exception {\n        zkServer = new TestingServer();\n        zkServer.start();\n        zkClient = CuratorFrameworkFactory.newClient(zkServer.getConnectString(),\n                new ExponentialBackoffRetry(200, 10, 5000));\n        zkClient.start();\n\n        storeClient = StoreClientFactory.createZKStoreClient(zkClient);\n        executorService = Executors.newScheduledThreadPool(20,\n                new ThreadFactoryBuilder().setNameFormat(\"testpool-%d\").build());\n        taskMetadataStore = TaskStoreFactory.createStore(storeClient, executorService);\n        hostStore = HostStoreFactory.createInMemoryStore(HostMonitorConfigImpl.dummyConfig());\n        streamStore = StreamStoreFactory.createZKStore(zkClient, executorService);\n        segmentHelper = SegmentHelperMock.getSegmentHelperMock();\n\n        ConnectionFactoryImpl connectionFactory = new ConnectionFactoryImpl(false);\n        streamMetadataTasks = new StreamMetadataTasks(streamStore, hostStore, taskMetadataStore, segmentHelper,\n                executorService, \"host\", connectionFactory);\n\n        streamTransactionMetadataTasks = new StreamTransactionMetadataTasks(\n                streamStore, hostStore, taskMetadataStore, segmentHelper, executorService, \"host\", connectionFactory);\n        timeoutService = new TimerWheelTimeoutService(streamTransactionMetadataTasks,\n                TimeoutServiceConfig.defaultConfig());\n\n        controllerService = new ControllerServiceImpl(\n                new ControllerService(streamStore, hostStore, streamMetadataTasks, streamTransactionMetadataTasks,\n                                      timeoutService, new SegmentHelper(), executorService));\n    }\n\n    @Override\n    public void cleanupStore() throws IOException {\n        executorService.shutdown();\n        zkClient.close();\n        zkServer.close();\n    }\n}\n"
    },
    {
      "path": "controller/server/src/test/java/com/emc/pravega/controller/task/Stream/StreamMetadataTasksTest.java",
      "content": "/**\n * Copyright (c) 2017 Dell Inc., or its subsidiaries.\n */\npackage com.emc.pravega.controller.task.Stream;\n\nimport com.emc.pravega.controller.server.ControllerService;\nimport com.emc.pravega.controller.mocks.SegmentHelperMock;\nimport com.emc.pravega.controller.server.SegmentHelper;\nimport com.emc.pravega.controller.store.host.HostControllerStore;\nimport com.emc.pravega.controller.store.host.HostStoreFactory;\nimport com.emc.pravega.controller.store.host.impl.HostMonitorConfigImpl;\nimport com.emc.pravega.controller.store.stream.StreamMetadataStore;\nimport com.emc.pravega.controller.store.stream.StreamStoreFactory;\nimport com.emc.pravega.controller.store.task.TaskMetadataStore;\nimport com.emc.pravega.controller.store.task.TaskStoreFactory;\nimport com.emc.pravega.controller.stream.api.grpc.v1.Controller.ScaleResponse;\nimport com.emc.pravega.controller.stream.api.grpc.v1.Controller.ScaleResponse.ScaleStreamStatus;\nimport com.emc.pravega.controller.stream.api.grpc.v1.Controller.UpdateStreamStatus;\nimport com.emc.pravega.controller.timeout.TimeoutService;\nimport com.emc.pravega.controller.timeout.TimeoutServiceConfig;\nimport com.emc.pravega.controller.timeout.TimerWheelTimeoutService;\nimport com.emc.pravega.stream.ScalingPolicy;\nimport com.emc.pravega.stream.StreamConfiguration;\nimport com.emc.pravega.stream.impl.netty.ConnectionFactoryImpl;\nimport org.apache.curator.framework.CuratorFramework;\nimport org.apache.curator.framework.CuratorFrameworkFactory;\nimport org.apache.curator.retry.ExponentialBackoffRetry;\nimport org.apache.curator.test.TestingServer;\nimport org.junit.After;\nimport org.junit.Before;\nimport org.junit.Test;\n\nimport java.io.IOException;\nimport java.util.AbstractMap;\nimport java.util.Arrays;\nimport java.util.Collections;\nimport java.util.concurrent.CompletableFuture;\nimport java.util.concurrent.Executors;\nimport java.util.concurrent.ScheduledExecutorService;\n\nimport static org.junit.Assert.assertEquals;\nimport static org.junit.Assert.assertNotEquals;\nimport static org.junit.Assert.assertTrue;\nimport static org.mockito.ArgumentMatchers.any;\nimport static org.mockito.ArgumentMatchers.anyString;\nimport static org.mockito.Mockito.doReturn;\nimport static org.mockito.Mockito.spy;\n\n\npublic class StreamMetadataTasksTest {\n\n    private static final String SCOPE = \"scope\";\n    private final String stream1 = \"stream1\";\n    private final ScheduledExecutorService executor = Executors.newScheduledThreadPool(10);\n\n    private ControllerService consumer;\n\n    private TestingServer zkServer;\n\n    private StreamMetadataStore streamStorePartialMock;\n    private StreamMetadataTasks streamMetadataTasks;\n\n    @Before\n    public void initialize() throws Exception {\n        zkServer = new TestingServer();\n        zkServer.start();\n        CuratorFramework zkClient = CuratorFrameworkFactory.newClient(zkServer.getConnectString(),\n                new ExponentialBackoffRetry(200, 10, 5000));\n        zkClient.start();\n\n        StreamMetadataStore streamStore = StreamStoreFactory.createInMemoryStore(executor);\n        streamStorePartialMock = spy(streamStore); //create a partial mock.\n        doReturn(CompletableFuture.completedFuture(false)).when(streamStorePartialMock).isTransactionOngoing(\n                anyString(), anyString(), any(), any()); //mock only isTransactionOngoing call.\n\n        TaskMetadataStore taskMetadataStore = TaskStoreFactory.createZKStore(zkClient, executor);\n        HostControllerStore hostStore = HostStoreFactory.createInMemoryStore(HostMonitorConfigImpl.dummyConfig());\n\n        SegmentHelper segmentHelperMock = SegmentHelperMock.getSegmentHelperMock();\n        ConnectionFactoryImpl connectionFactory = new ConnectionFactoryImpl(false);\n        streamMetadataTasks = new StreamMetadataTasks(streamStorePartialMock, hostStore,\n                taskMetadataStore, segmentHelperMock,\n                executor, \"host\", connectionFactory);\n\n        StreamTransactionMetadataTasks streamTransactionMetadataTasks = new StreamTransactionMetadataTasks(\n                streamStorePartialMock, hostStore, taskMetadataStore, segmentHelperMock, executor, \"host\", connectionFactory);\n        TimeoutService timeoutService = new TimerWheelTimeoutService(streamTransactionMetadataTasks,\n                TimeoutServiceConfig.defaultConfig());\n\n        consumer = new ControllerService(streamStorePartialMock, hostStore, streamMetadataTasks,\n                streamTransactionMetadataTasks, timeoutService, segmentHelperMock, executor);\n\n        final ScalingPolicy policy1 = ScalingPolicy.fixed(2);\n        final StreamConfiguration configuration1 = StreamConfiguration.builder().scope(SCOPE).streamName(stream1).scalingPolicy(policy1).build();\n        streamStorePartialMock.createScope(SCOPE);\n\n        streamStorePartialMock.createStream(SCOPE, stream1, configuration1, System.currentTimeMillis(), null, executor);\n\n        AbstractMap.SimpleEntry<Double, Double> segment1 = new AbstractMap.SimpleEntry<>(0.5, 0.75);\n        AbstractMap.SimpleEntry<Double, Double> segment2 = new AbstractMap.SimpleEntry<>(0.75, 1.0);\n        streamStorePartialMock.scale(SCOPE, stream1, Collections.singletonList(1), Arrays.asList(segment1, segment2), 20, null, executor);\n    }\n\n    @After\n    public void tearDown() throws IOException {\n        zkServer.close();\n    }\n\n    @Test\n    public void sealStreamTest() throws Exception {\n        assertNotEquals(0, consumer.getCurrentSegments(SCOPE, stream1).get().size());\n\n        //seal a stream.\n        UpdateStreamStatus.Status sealOperationResult = streamMetadataTasks.sealStreamBody(SCOPE, stream1, null).get();\n        assertEquals(UpdateStreamStatus.Status.SUCCESS, sealOperationResult);\n\n        //a sealed stream should have zero active/current segments\n        assertEquals(0, consumer.getCurrentSegments(SCOPE, stream1).get().size());\n        assertTrue(streamStorePartialMock.isSealed(SCOPE, stream1, null, executor).get());\n\n        //reseal a sealed stream.\n        assertEquals(UpdateStreamStatus.Status.SUCCESS, streamMetadataTasks.sealStreamBody(SCOPE, stream1, null).get());\n\n        //scale operation on the sealed stream.\n        AbstractMap.SimpleEntry<Double, Double> segment3 = new AbstractMap.SimpleEntry<>(0.0, 0.2);\n        AbstractMap.SimpleEntry<Double, Double> segment4 = new AbstractMap.SimpleEntry<>(0.3, 0.4);\n        AbstractMap.SimpleEntry<Double, Double> segment5 = new AbstractMap.SimpleEntry<>(0.4, 0.5);\n\n        ScaleResponse scaleOpResult = streamMetadataTasks.scaleBody(SCOPE, stream1, Collections\n                        .singletonList(0),\n                Arrays.asList(segment3, segment4, segment5), 30, null).get();\n\n        //scaling operation fails once a stream is sealed.\n        assertEquals(ScaleStreamStatus.PRECONDITION_FAILED, scaleOpResult.getStatus());\n    }\n}\n"
    },
    {
      "path": "integrationtests/src/main/java/com/emc/pravega/demo/ControllerWrapper.java",
      "content": "/**\n * Copyright (c) 2017 Dell Inc., or its subsidiaries.\n */\npackage com.emc.pravega.demo;\n\nimport com.emc.pravega.controller.eventProcessor.CheckpointConfig;\nimport com.emc.pravega.controller.server.ControllerServiceConfig;\nimport com.emc.pravega.controller.server.ControllerServiceStarter;\nimport com.emc.pravega.controller.server.ControllerService;\nimport com.emc.pravega.controller.server.eventProcessor.ControllerEventProcessorConfig;\nimport com.emc.pravega.controller.server.eventProcessor.impl.ControllerEventProcessorConfigImpl;\nimport com.emc.pravega.controller.server.impl.ControllerServiceConfigImpl;\nimport com.emc.pravega.controller.server.rest.RESTServerConfig;\nimport com.emc.pravega.controller.server.rpc.grpc.GRPCServerConfig;\nimport com.emc.pravega.controller.server.rpc.grpc.impl.GRPCServerConfigImpl;\nimport com.emc.pravega.controller.store.client.StoreClientConfig;\nimport com.emc.pravega.controller.store.client.ZKClientConfig;\nimport com.emc.pravega.controller.store.client.impl.StoreClientConfigImpl;\nimport com.emc.pravega.controller.store.client.impl.ZKClientConfigImpl;\nimport com.emc.pravega.controller.store.host.HostMonitorConfig;\nimport com.emc.pravega.controller.store.host.impl.HostMonitorConfigImpl;\nimport com.emc.pravega.controller.timeout.TimeoutServiceConfig;\nimport com.emc.pravega.controller.util.Config;\nimport com.emc.pravega.stream.ScalingPolicy;\nimport com.emc.pravega.stream.impl.Controller;\nimport lombok.extern.slf4j.Slf4j;\n\nimport java.util.Optional;\nimport java.util.UUID;\nimport java.util.concurrent.TimeUnit;\n\n\n@Slf4j\npublic class ControllerWrapper implements AutoCloseable {\n\n    private final ControllerServiceStarter controllerServiceStarter;\n\n    public ControllerWrapper(final String connectionString, final int servicePort) throws Exception {\n        this(connectionString, false, false, Config.RPC_SERVER_PORT, Config.SERVICE_HOST, servicePort,\n                Config.HOST_STORE_CONTAINER_COUNT);\n    }\n\n    public ControllerWrapper(final String connectionString, final int servicePort,\n            final boolean disableEventProcessor) throws Exception {\n        this(connectionString, disableEventProcessor, false, Config.RPC_SERVER_PORT, Config.SERVICE_HOST, servicePort,\n             Config.HOST_STORE_CONTAINER_COUNT);\n    }\n\n    public ControllerWrapper(final String connectionString, final boolean disableEventProcessor,\n                             final boolean disableRequestHandler,\n                             final int controllerPort, final String serviceHost, final int servicePort,\n                             final int containerCount) throws Exception {\n\n        ZKClientConfig zkClientConfig = ZKClientConfigImpl.builder().connectionString(connectionString)\n                .initialSleepInterval(2000)\n                .maxRetries(1)\n                .namespace(\"pravega/\" + UUID.randomUUID())\n                .build();\n        StoreClientConfig storeClientConfig = StoreClientConfigImpl.withZKClient(zkClientConfig);\n\n        HostMonitorConfig hostMonitorConfig = HostMonitorConfigImpl.builder()\n                .hostMonitorEnabled(false)\n                .hostMonitorMinRebalanceInterval(Config.CLUSTER_MIN_REBALANCE_INTERVAL)\n                .containerCount(Config.HOST_STORE_CONTAINER_COUNT)\n                .hostContainerMap(HostMonitorConfigImpl.getHostContainerMap(serviceHost, servicePort, containerCount))\n                .build();\n\n        TimeoutServiceConfig timeoutServiceConfig = TimeoutServiceConfig.builder()\n                .maxLeaseValue(Config.MAX_LEASE_VALUE)\n                .maxScaleGracePeriod(Config.MAX_SCALE_GRACE_PERIOD)\n                .build();\n\n        Optional<ControllerEventProcessorConfig> eventProcessorConfig;\n        if (!disableEventProcessor) {\n            eventProcessorConfig = Optional.of(ControllerEventProcessorConfigImpl.builder()\n                    .scopeName(\"system\")\n                    .commitStreamName(\"commitStream\")\n                    .abortStreamName(\"abortStream\")\n                    .commitStreamScalingPolicy(ScalingPolicy.fixed(2))\n                    .abortStreamScalingPolicy(ScalingPolicy.fixed(2))\n                    .commitReaderGroupName(\"commitStreamReaders\")\n                    .commitReaderGroupSize(1)\n                    .abortReaderGrouopName(\"abortStreamReaders\")\n                    .abortReaderGroupSize(1)\n                    .commitCheckpointConfig(CheckpointConfig.periodic(10, 10))\n                    .abortCheckpointConfig(CheckpointConfig.periodic(10, 10))\n                    .build());\n        } else {\n            eventProcessorConfig = Optional.empty();\n        }\n\n        GRPCServerConfig grpcServerConfig = GRPCServerConfigImpl.builder().port(controllerPort).build();\n\n        ControllerServiceConfig serviceConfig = ControllerServiceConfigImpl.builder()\n                .serviceThreadPoolSize(3)\n                .taskThreadPoolSize(3)\n                .storeThreadPoolSize(3)\n                .eventProcThreadPoolSize(3)\n                .requestHandlerThreadPoolSize(3)\n                .storeClientConfig(storeClientConfig)\n                .hostMonitorConfig(hostMonitorConfig)\n                .timeoutServiceConfig(timeoutServiceConfig)\n                .eventProcessorConfig(eventProcessorConfig)\n                .requestHandlersEnabled(!disableRequestHandler)\n                .grpcServerConfig(Optional.of(grpcServerConfig))\n                .restServerConfig(Optional.<RESTServerConfig>empty())\n                .build();\n\n        controllerServiceStarter = new ControllerServiceStarter(serviceConfig);\n        controllerServiceStarter.startAsync();\n    }\n\n    public boolean awaitTasksModuleInitialization(long timeout, TimeUnit timeUnit) throws InterruptedException {\n        return this.controllerServiceStarter.awaitTasksModuleInitialization(timeout, timeUnit);\n    }\n\n    public ControllerService getControllerService() throws InterruptedException {\n        return this.controllerServiceStarter.getControllerService();\n    }\n\n    public Controller getController() throws InterruptedException {\n        return this.controllerServiceStarter.getController();\n    }\n\n    public void awaitRunning() {\n        this.controllerServiceStarter.awaitRunning();\n    }\n\n    public void awaitTerminated() {\n        this.controllerServiceStarter.awaitTerminated();\n    }\n\n    @Override\n    public void close() throws Exception {\n        this.controllerServiceStarter.stopAsync();\n    }\n}\n"
    },
    {
      "path": "integrationtests/src/main/java/com/emc/pravega/demo/EndToEndAutoScaleDownTest.java",
      "content": "/**\n * Copyright (c) 2017 Dell Inc., or its subsidiaries.\n */\npackage com.emc.pravega.demo;\n\nimport com.emc.pravega.ClientFactory;\nimport com.emc.pravega.controller.util.Config;\nimport com.emc.pravega.common.util.Retry;\nimport com.emc.pravega.service.contracts.StreamSegmentStore;\nimport com.emc.pravega.service.server.host.handler.PravegaConnectionListener;\nimport com.emc.pravega.service.server.host.stat.AutoScalerConfig;\nimport com.emc.pravega.service.server.host.stat.SegmentStatsFactory;\nimport com.emc.pravega.service.server.host.stat.SegmentStatsRecorder;\nimport com.emc.pravega.service.server.store.ServiceBuilder;\nimport com.emc.pravega.service.server.store.ServiceBuilderConfig;\nimport com.emc.pravega.stream.ScalingPolicy;\nimport com.emc.pravega.stream.Stream;\nimport com.emc.pravega.stream.StreamConfiguration;\nimport com.emc.pravega.stream.impl.ClientFactoryImpl;\nimport com.emc.pravega.stream.impl.Controller;\nimport com.emc.pravega.stream.impl.StreamImpl;\nimport com.emc.pravega.stream.impl.netty.ConnectionFactoryImpl;\nimport lombok.Cleanup;\nimport lombok.extern.slf4j.Slf4j;\nimport org.apache.curator.test.TestingServer;\n\nimport java.util.Collections;\nimport java.util.HashMap;\nimport java.util.Map;\nimport java.util.concurrent.Executors;\n\n@Slf4j\npublic class EndToEndAutoScaleDownTest {\n    static final StreamConfiguration CONFIG =\n            StreamConfiguration.builder().scope(\"test\").streamName(\"test\").scalingPolicy(\n                    ScalingPolicy.byEventRate(10, 2, 1)).build();\n\n    public static void main(String[] args) throws Exception {\n        try {\n            @Cleanup\n            TestingServer zkTestServer = new TestingServer();\n            int port = Config.SERVICE_PORT;\n            ControllerWrapper controllerWrapper = new ControllerWrapper(zkTestServer.getConnectString(), port, true);\n            Controller controller = controllerWrapper.getController();\n\n            controllerWrapper.getControllerService().createScope(\"pravega\").get();\n            ClientFactory internalCF = new ClientFactoryImpl(\"pravega\", controller, new ConnectionFactoryImpl(false));\n\n            ServiceBuilder serviceBuilder = ServiceBuilder.newInMemoryBuilder(ServiceBuilderConfig.getDefaultConfig());\n            serviceBuilder.initialize().get();\n            StreamSegmentStore store = serviceBuilder.createStreamSegmentService();\n\n            SegmentStatsRecorder statsRecorder = new SegmentStatsFactory().createSegmentStatsRecorder(store,\n                    internalCF,\n                    AutoScalerConfig.builder().with(AutoScalerConfig.MUTE_IN_SECONDS, 0)\n                            .with(AutoScalerConfig.COOLDOWN_IN_SECONDS, 0)\n                            .with(AutoScalerConfig.CACHE_CLEANUP_IN_SECONDS, 5)\n                            .with(AutoScalerConfig.CACHE_EXPIRY_IN_SECONDS, 30).build());\n\n            @Cleanup\n            PravegaConnectionListener server = new PravegaConnectionListener(false, 12345, store, statsRecorder);\n            server.startListening();\n            controllerWrapper.getControllerService().createScope(\"test\").get();\n\n            controller.createStream(CONFIG).get();\n\n            Stream stream = new StreamImpl(\"test\", \"test\");\n            Map<Double, Double> map = new HashMap<>();\n            map.put(0.0, 0.33);\n            map.put(0.33, 0.66);\n            map.put(0.66, 1.0);\n            controller.scaleStream(stream, Collections.singletonList(0), map).get();\n\n            Retry.withExpBackoff(10, 10, 100, 10000)\n                    .retryingOn(NotDoneException.class)\n                    .throwingOn(RuntimeException.class)\n                    .runAsync(() -> controller.getCurrentSegments(\"test\", \"test\")\n                            .thenAccept(streamSegments -> {\n                                if (streamSegments.getSegments().size() < 3) {\n                                    System.err.println(\"Success\");\n                                    log.info(\"Success\");\n                                    System.exit(0);\n                                } else {\n                                    throw new NotDoneException();\n                                }\n                            }), Executors.newSingleThreadScheduledExecutor())\n                    .exceptionally(e -> {\n                        System.err.println(\"Failure\");\n                        log.error(\"Failure\");\n                        System.exit(1);\n                        return null;\n                    }).get();\n\n        } catch (Throwable e) {\n            System.err.print(\"Test failed with exception: \" + e.getMessage());\n            System.exit(-1);\n        }\n\n        System.exit(0);\n    }\n}\n"
    },
    {
      "path": "integrationtests/src/main/java/com/emc/pravega/demo/EndToEndAutoScaleUpTest.java",
      "content": "/**\n * Copyright (c) 2017 Dell Inc., or its subsidiaries.\n */\npackage com.emc.pravega.demo;\n\nimport com.emc.pravega.ClientFactory;\nimport com.emc.pravega.controller.util.Config;\nimport com.emc.pravega.common.util.Retry;\nimport com.emc.pravega.service.contracts.StreamSegmentStore;\nimport com.emc.pravega.service.server.host.handler.PravegaConnectionListener;\nimport com.emc.pravega.service.server.host.stat.AutoScalerConfig;\nimport com.emc.pravega.service.server.host.stat.SegmentStatsFactory;\nimport com.emc.pravega.service.server.host.stat.SegmentStatsRecorder;\nimport com.emc.pravega.service.server.store.ServiceBuilder;\nimport com.emc.pravega.service.server.store.ServiceBuilderConfig;\nimport com.emc.pravega.stream.EventStreamWriter;\nimport com.emc.pravega.stream.EventWriterConfig;\nimport com.emc.pravega.stream.ScalingPolicy;\nimport com.emc.pravega.stream.StreamConfiguration;\nimport com.emc.pravega.stream.impl.ClientFactoryImpl;\nimport com.emc.pravega.stream.impl.Controller;\nimport com.emc.pravega.stream.impl.JavaSerializer;\nimport com.emc.pravega.stream.impl.netty.ConnectionFactoryImpl;\nimport com.emc.pravega.stream.mock.MockClientFactory;\nimport lombok.Cleanup;\nimport lombok.extern.slf4j.Slf4j;\nimport org.apache.curator.test.TestingServer;\n\nimport java.time.Duration;\nimport java.util.Arrays;\nimport java.util.concurrent.CompletableFuture;\nimport java.util.concurrent.Executors;\n\n@Slf4j\npublic class EndToEndAutoScaleUpTest {\n    static final StreamConfiguration CONFIG =\n            StreamConfiguration.builder().scope(\"test\").streamName(\"test\").scalingPolicy(\n                    ScalingPolicy.byEventRate(10, 2, 3)).build();\n\n    public static void main(String[] args) throws Exception {\n        try {\n            @Cleanup\n            TestingServer zkTestServer = new TestingServer();\n            int port = Config.SERVICE_PORT;\n            ControllerWrapper controllerWrapper = new ControllerWrapper(zkTestServer.getConnectString(), port, true);\n            Controller controller = controllerWrapper.getController();\n            controllerWrapper.getControllerService().createScope(\"pravega\").get();\n\n            ClientFactory internalCF = new ClientFactoryImpl(\"pravega\", controller, new ConnectionFactoryImpl(false));\n\n            ServiceBuilder serviceBuilder = ServiceBuilder.newInMemoryBuilder(ServiceBuilderConfig.getDefaultConfig());\n            serviceBuilder.initialize().get();\n            StreamSegmentStore store = serviceBuilder.createStreamSegmentService();\n            SegmentStatsRecorder statsRecorder = new SegmentStatsFactory().createSegmentStatsRecorder(store,\n                    internalCF,\n                    AutoScalerConfig.builder().with(AutoScalerConfig.MUTE_IN_SECONDS, 0)\n                            .with(AutoScalerConfig.COOLDOWN_IN_SECONDS, 0).build());\n\n            @Cleanup\n            PravegaConnectionListener server = new PravegaConnectionListener(false, 12345, store, statsRecorder);\n            server.startListening();\n\n            controllerWrapper.getControllerService().createScope(\"test\").get();\n\n            controller.createStream(CONFIG).get();\n            MockClientFactory clientFactory = new MockClientFactory(\"test\", controller);\n\n            // Mocking pravega service by putting scale up and scale down requests for the stream\n            EventStreamWriter<String> test = clientFactory.createEventWriter(\n                    \"test\", new JavaSerializer<>(), EventWriterConfig.builder().build());\n\n            // keep writing. Scale should happen\n            long start = System.currentTimeMillis();\n            char[] chars = new char[1];\n            Arrays.fill(chars, 'a');\n\n            String str = new String(chars);\n\n            CompletableFuture.runAsync(() -> {\n                while (System.currentTimeMillis() - start < Duration.ofMinutes(3).toMillis()) {\n                    try {\n                        test.writeEvent(\"0\", str).get();\n                    } catch (Throwable e) {\n                        System.err.println(\"test exception writing events \" + e.getMessage());\n                        break;\n                    }\n                }\n            });\n\n            Retry.withExpBackoff(10, 10, 100, 10000)\n                    .retryingOn(NotDoneException.class)\n                    .throwingOn(RuntimeException.class)\n                    .runAsync(() -> controller.getCurrentSegments(\"test\", \"test\")\n                            .thenAccept(streamSegments -> {\n                                if (streamSegments.getSegments().size() > 3) {\n                                    System.err.println(\"Success\");\n                                    log.info(\"Success\");\n                                    System.exit(0);\n                                } else {\n                                    throw new NotDoneException();\n                                }\n                            }), Executors.newSingleThreadScheduledExecutor())\n                    .exceptionally(e -> {\n                        System.err.println(\"Failure\");\n                        log.error(\"Failure\");\n                        System.exit(1);\n                        return null;\n                    }).get();\n\n        } catch (Throwable e) {\n            System.err.print(\"Test failed with exception: \" + e.getMessage());\n            System.exit(-1);\n        }\n\n        System.exit(0);\n    }\n}\n"
    },
    {
      "path": "integrationtests/src/main/java/com/emc/pravega/demo/EndToEndAutoScaleUpWithTxnTest.java",
      "content": "/**\n * Copyright (c) 2017 Dell Inc., or its subsidiaries.\n */\npackage com.emc.pravega.demo;\n\nimport com.emc.pravega.ClientFactory;\nimport com.emc.pravega.controller.util.Config;\nimport com.emc.pravega.common.util.Retry;\nimport com.emc.pravega.service.contracts.StreamSegmentStore;\nimport com.emc.pravega.service.server.host.handler.PravegaConnectionListener;\nimport com.emc.pravega.service.server.host.stat.AutoScalerConfig;\nimport com.emc.pravega.service.server.host.stat.SegmentStatsFactory;\nimport com.emc.pravega.service.server.host.stat.SegmentStatsRecorder;\nimport com.emc.pravega.service.server.store.ServiceBuilder;\nimport com.emc.pravega.service.server.store.ServiceBuilderConfig;\nimport com.emc.pravega.stream.EventStreamWriter;\nimport com.emc.pravega.stream.EventWriterConfig;\nimport com.emc.pravega.stream.ScalingPolicy;\nimport com.emc.pravega.stream.StreamConfiguration;\nimport com.emc.pravega.stream.Transaction;\nimport com.emc.pravega.stream.impl.ClientFactoryImpl;\nimport com.emc.pravega.stream.impl.Controller;\nimport com.emc.pravega.stream.impl.JavaSerializer;\nimport com.emc.pravega.stream.impl.netty.ConnectionFactoryImpl;\nimport com.emc.pravega.stream.mock.MockClientFactory;\nimport lombok.Cleanup;\nimport lombok.extern.slf4j.Slf4j;\nimport org.apache.curator.test.TestingServer;\n\nimport java.util.concurrent.CompletableFuture;\nimport java.util.concurrent.Executors;\nimport java.util.concurrent.atomic.AtomicBoolean;\n\n@Slf4j\npublic class EndToEndAutoScaleUpWithTxnTest {\n    static final StreamConfiguration CONFIG =\n            StreamConfiguration.builder().scope(\"test\").streamName(\"test\").scalingPolicy(\n                    ScalingPolicy.byEventRate(10, 2, 3)).build();\n\n    public static void main(String[] args) throws Exception {\n        try {\n            @Cleanup\n            TestingServer zkTestServer = new TestingServer();\n            int port = Config.SERVICE_PORT;\n            @Cleanup\n            ControllerWrapper controllerWrapper = new ControllerWrapper(zkTestServer.getConnectString(), port);\n            Controller controller = controllerWrapper.getController();\n            controllerWrapper.getControllerService().createScope(\"pravega\").get();\n\n            @Cleanup\n            ClientFactory internalCF = new ClientFactoryImpl(\"pravega\", controller, new ConnectionFactoryImpl(false));\n\n            ServiceBuilder serviceBuilder = ServiceBuilder.newInMemoryBuilder(ServiceBuilderConfig.getDefaultConfig());\n            serviceBuilder.initialize().get();\n            StreamSegmentStore store = serviceBuilder.createStreamSegmentService();\n            SegmentStatsRecorder statsRecorder = new SegmentStatsFactory().createSegmentStatsRecorder(store,\n                    internalCF,\n                    AutoScalerConfig.builder().with(AutoScalerConfig.MUTE_IN_SECONDS, 0)\n                            .with(AutoScalerConfig.COOLDOWN_IN_SECONDS, 0).build());\n\n            @Cleanup\n            PravegaConnectionListener server = new PravegaConnectionListener(false, 12345, store, statsRecorder);\n            server.startListening();\n\n            controllerWrapper.getControllerService().createScope(\"test\").get();\n\n            controller.createStream(CONFIG).get();\n            MockClientFactory clientFactory = new MockClientFactory(\"test\", controller);\n\n            // Mocking pravega service by putting scale up and scale down requests for the stream\n            EventStreamWriter<String> test = clientFactory.createEventWriter(\n                    \"test\", new JavaSerializer<>(), EventWriterConfig.builder().build());\n\n            final AtomicBoolean done = new AtomicBoolean(false);\n\n            startWriter(test, done);\n\n            Retry.withExpBackoff(10, 10, 100, 10000)\n                    .retryingOn(NotDoneException.class)\n                    .throwingOn(RuntimeException.class)\n                    .runAsync(() -> controller.getCurrentSegments(\"test\", \"test\")\n                            .thenAccept(streamSegments -> {\n                                if (streamSegments.getSegments().size() > 3) {\n                                    System.err.println(\"Success\");\n                                    log.info(\"Success\");\n                                    System.exit(0);\n                                } else {\n                                    throw new NotDoneException();\n                                }\n                            }), Executors.newSingleThreadScheduledExecutor())\n                    .exceptionally(e -> {\n                        System.err.println(\"Failure\");\n                        log.error(\"Failure\");\n                        System.exit(1);\n                        return null;\n                    }).get();\n\n        } catch (Throwable e) {\n            System.err.print(\"Test failed with exception: \" + e.getMessage());\n            log.error(\"Test failed with exception: {}\", e);\n            System.exit(-1);\n        }\n\n        System.exit(0);\n    }\n\n    private static void startWriter(EventStreamWriter<String> test, AtomicBoolean done) {\n        CompletableFuture.runAsync(() -> {\n            while (!done.get()) {\n                try {\n                    Transaction<String> transaction = test.beginTxn(5000, 3600000, 60000);\n\n                    for (int i = 0; i < 1000; i++) {\n                        transaction.writeEvent(\"0\", \"txntest\");\n                    }\n                    Thread.sleep(900);\n                    transaction.commit();\n                } catch (Throwable e) {\n                    System.err.println(\"test exception writing events \" + e.getMessage());\n                    log.error(\"test exception writing events {}\", e);\n                }\n            }\n        });\n    }\n}\n"
    },
    {
      "path": "integrationtests/src/main/java/com/emc/pravega/demo/EndToEndTransactionTest.java",
      "content": "/**\n * Copyright (c) 2017 Dell Inc., or its subsidiaries.\n */\npackage com.emc.pravega.demo;\n\nimport com.emc.pravega.controller.util.Config;\nimport com.emc.pravega.service.contracts.StreamSegmentStore;\nimport com.emc.pravega.service.server.host.handler.PravegaConnectionListener;\nimport com.emc.pravega.service.server.store.ServiceBuilder;\nimport com.emc.pravega.service.server.store.ServiceBuilderConfig;\nimport com.emc.pravega.stream.EventStreamWriter;\nimport com.emc.pravega.stream.EventWriterConfig;\nimport com.emc.pravega.stream.PingFailedException;\nimport com.emc.pravega.stream.ScalingPolicy;\nimport com.emc.pravega.stream.StreamConfiguration;\nimport com.emc.pravega.stream.Transaction;\nimport com.emc.pravega.stream.impl.Controller;\nimport com.emc.pravega.stream.impl.JavaSerializer;\nimport com.emc.pravega.stream.mock.MockClientFactory;\nimport java.util.concurrent.CompletableFuture;\nimport lombok.Cleanup;\nimport lombok.extern.slf4j.Slf4j;\nimport org.apache.curator.test.TestingServer;\nimport org.junit.Assert;\nimport org.junit.Test;\n\nimport static org.junit.Assert.assertTrue;\n\n@Slf4j\npublic class EndToEndTransactionTest {\n\n    final static long MAX_LEASE_VALUE = 30000;\n    final static long MAX_SCALE_GRACE_PERIOD = 60000;\n\n    @Test\n    public static void main(String[] args) throws Exception {\n        @Cleanup\n        TestingServer zkTestServer = new TestingServer();\n\n        ServiceBuilder serviceBuilder = ServiceBuilder.newInMemoryBuilder(ServiceBuilderConfig.getDefaultConfig());\n        serviceBuilder.initialize().get();\n        StreamSegmentStore store = serviceBuilder.createStreamSegmentService();\n        int port = Config.SERVICE_PORT;\n        @Cleanup\n        PravegaConnectionListener server = new PravegaConnectionListener(false, port, store);\n        server.startListening();\n\n        Thread.sleep(1000);\n        ControllerWrapper controllerWrapper = new ControllerWrapper(zkTestServer.getConnectString(), port);\n        Controller controller = controllerWrapper.getController();\n\n        final String testScope = \"testScope\";\n        final String testStream = \"testStream\";\n\n        if (!controller.createScope(testScope).get()) {\n            log.error(\"FAILURE: Error creating test scope\");\n            return;\n        }\n\n        ScalingPolicy policy = ScalingPolicy.fixed(5);\n        StreamConfiguration streamConfig =\n                StreamConfiguration.builder()\n                        .scope(testScope)\n                        .streamName(testStream)\n                        .scalingPolicy(policy)\n                        .build();\n\n        if (!controller.createStream(streamConfig).get()) {\n            log.error(\"FAILURE: Error creating test stream\");\n            return;\n        }\n\n        final long lease = 4000;\n        final long maxExecutionTime = 10000;\n        final long scaleGracePeriod = 30000;\n\n        MockClientFactory clientFactory = new MockClientFactory(testScope, controller);\n\n        @Cleanup\n        EventStreamWriter<String> producer = clientFactory.createEventWriter(\n                testStream,\n                new JavaSerializer<>(),\n                EventWriterConfig.builder().build());\n\n        // region Successful commit tests\n        Transaction<String> transaction = producer.beginTxn(5000, 3600000, 60000);\n\n        for (int i = 0; i < 1; i++) {\n            String event = \"\\n Transactional Publish \\n\";\n            log.info(\"Producing event: \" + event);\n            transaction.writeEvent(\"\", event);\n            transaction.flush();\n            Thread.sleep(500);\n        }\n\n        CompletableFuture<Object> commit = CompletableFuture.supplyAsync(() -> {\n            try {\n                transaction.commit();\n            } catch (Exception e) {\n                log.warn(\"Error committing transaction\", e);\n            }\n            return null;\n        });\n\n        commit.join();\n\n        Transaction.Status txnStatus = transaction.checkStatus();\n        assertTrue(txnStatus == Transaction.Status.COMMITTING || txnStatus == Transaction.Status.COMMITTED);\n        log.info(\"SUCCESS: successful in committing transaction. Transaction status=\" + txnStatus);\n\n        Thread.sleep(2000);\n\n        txnStatus = transaction.checkStatus();\n        assertTrue(txnStatus == Transaction.Status.COMMITTED);\n        log.info(\"SUCCESS: successfully committed transaction. Transaction status=\" + txnStatus);\n\n        // endregion\n\n        // region Successful abort tests\n\n        Transaction<String> transaction2 = producer.beginTxn(5000, 3600000, 60000);\n        for (int i = 0; i < 1; i++) {\n            String event = \"\\n Transactional Publish \\n\";\n            log.info(\"Producing event: \" + event);\n            transaction2.writeEvent(\"\", event);\n            transaction2.flush();\n            Thread.sleep(500);\n        }\n\n        CompletableFuture<Object> drop = CompletableFuture.supplyAsync(() -> {\n            try {\n                transaction2.abort();\n            } catch (Exception e) {\n                log.warn(\"Error aborting transaction\", e);\n            }\n            return null;\n        });\n\n        drop.join();\n\n        Transaction.Status txn2Status = transaction2.checkStatus();\n        assertTrue(txn2Status == Transaction.Status.ABORTING || txn2Status == Transaction.Status.ABORTED);\n        log.info(\"SUCCESS: successful in dropping transaction. Transaction status=\" + txn2Status);\n\n        Thread.sleep(2000);\n\n        txn2Status = transaction2.checkStatus();\n        assertTrue(txn2Status == Transaction.Status.ABORTED);\n        log.info(\"SUCCESS: successfully aborted transaction. Transaction status=\" + txn2Status);\n\n        // endregion\n\n        // region Successful timeout tests\n        Transaction<String> tx1 = producer.beginTxn(lease, maxExecutionTime, scaleGracePeriod);\n\n        Thread.sleep((long) (1.3 * lease));\n\n        Transaction.Status txStatus = tx1.checkStatus();\n        Assert.assertTrue(Transaction.Status.ABORTING == txStatus || Transaction.Status.ABORTED == txStatus);\n        log.info(\"SUCCESS: successfully aborted transaction after timeout. Transaction status=\" + txStatus);\n\n        // endregion\n\n        // region Successful ping tests\n\n        Transaction<String> tx2 = producer.beginTxn(lease, maxExecutionTime, scaleGracePeriod);\n\n        Thread.sleep((long) (0.75 * lease));\n\n        Assert.assertEquals(Transaction.Status.OPEN, tx2.checkStatus());\n\n        try {\n            tx2.ping(lease);\n            Assert.assertTrue(true);\n        } catch (PingFailedException pfe) {\n            Assert.assertTrue(false);\n        }\n        log.info(\"SUCCESS: successfully pinged transaction.\");\n\n        Thread.sleep((long) (0.5 * lease));\n\n        Assert.assertEquals(Transaction.Status.OPEN, tx2.checkStatus());\n\n        Thread.sleep((long) (0.8 * lease));\n\n        txStatus = tx2.checkStatus();\n        Assert.assertTrue(Transaction.Status.ABORTING == txStatus || Transaction.Status.ABORTED == txStatus);\n        log.info(\"SUCCESS: successfully aborted transaction after pinging. Transaction status=\" + txStatus);\n\n        // endregion\n\n        // region Ping failure due to MaxExecutionTime exceeded\n\n        Transaction<String> tx3 = producer.beginTxn(lease, maxExecutionTime, scaleGracePeriod);\n\n        Thread.sleep((long) (0.75 * lease));\n\n        Assert.assertEquals(Transaction.Status.OPEN, tx3.checkStatus());\n\n        try {\n            //Assert.assertEquals(PingStatus.OK, pingStatus);\n            tx3.ping(lease);\n            Assert.assertTrue(true);\n        } catch (PingFailedException pfe) {\n            Assert.assertTrue(false);\n        }\n\n        Thread.sleep((long) (0.75 * lease));\n\n        Assert.assertEquals(Transaction.Status.OPEN, tx3.checkStatus());\n\n        try {\n            // PingFailedException is expected to be thrown.\n            tx3.ping(lease + 1);\n            Assert.assertTrue(false);\n        } catch (PingFailedException pfe) {\n            Assert.assertTrue(true);\n            log.info(\"SUCCESS: successfully received error after max expiry time\");\n        }\n\n        Thread.sleep((long) (0.5 * lease));\n\n        txStatus = tx3.checkStatus();\n        Assert.assertTrue(Transaction.Status.ABORTING == txStatus || Transaction.Status.ABORTED == txStatus);\n        log.info(\"SUCCESS: successfully aborted transaction after 1 successful ping and 1 unsuccessful\" +\n                \"ping. Transaction status=\" + txStatus);\n\n        // endregion\n\n        // region Ping failure due to very high lease value\n\n        Transaction<String> tx4 = producer.beginTxn(lease, maxExecutionTime, scaleGracePeriod);\n\n        try {\n            tx4.ping(scaleGracePeriod + 1);\n            Assert.assertTrue(false);\n        } catch (PingFailedException pfe) {\n            Assert.assertTrue(true);\n        }\n\n        try {\n            tx4.ping(maxExecutionTime + 1);\n            Assert.assertTrue(false);\n        } catch (PingFailedException pfe) {\n            Assert.assertTrue(true);\n        }\n\n        try {\n            tx4.ping(MAX_LEASE_VALUE + 1);\n            Assert.assertTrue(false);\n        } catch (PingFailedException pfe) {\n            Assert.assertTrue(true);\n        }\n\n        try {\n            tx4.ping(MAX_SCALE_GRACE_PERIOD + 1);\n            Assert.assertTrue(false);\n        } catch (PingFailedException pfe) {\n            Assert.assertTrue(true);\n        }\n\n        // endregion\n\n        // region Ping failure due to controller going into disconnection state\n\n        // Fill in these tests once we have controller.stop() implemented.\n\n        System.exit(0);\n    }\n}\n"
    },
    {
      "path": "integrationtests/src/main/java/com/emc/pravega/demo/ScaleTest.java",
      "content": "/**\n * Copyright (c) 2017 Dell Inc., or its subsidiaries.\n */\npackage com.emc.pravega.demo;\n\nimport com.emc.pravega.common.concurrent.FutureHelpers;\nimport com.emc.pravega.controller.util.Config;\nimport com.emc.pravega.service.contracts.StreamSegmentStore;\nimport com.emc.pravega.service.server.host.handler.PravegaConnectionListener;\nimport com.emc.pravega.service.server.store.ServiceBuilder;\nimport com.emc.pravega.service.server.store.ServiceBuilderConfig;\nimport com.emc.pravega.stream.ScalingPolicy;\nimport com.emc.pravega.stream.Stream;\nimport com.emc.pravega.stream.StreamConfiguration;\nimport com.emc.pravega.stream.Transaction;\nimport com.emc.pravega.stream.impl.Controller;\nimport com.emc.pravega.stream.impl.StreamImpl;\nimport java.util.Arrays;\nimport java.util.Collections;\nimport java.util.HashMap;\nimport java.util.Map;\nimport java.util.UUID;\nimport java.util.concurrent.CompletableFuture;\nimport lombok.Cleanup;\nimport lombok.extern.slf4j.Slf4j;\nimport org.apache.curator.test.TestingServer;\n\n/**\n * End to end scale tests.\n */\n@Slf4j\npublic class ScaleTest {\n    @SuppressWarnings(\"checkstyle:ReturnCount\")\n    public static void main(String[] args) throws Exception {\n        TestingServer zkTestServer = new TestingServer();\n\n        ServiceBuilder serviceBuilder = ServiceBuilder.newInMemoryBuilder(ServiceBuilderConfig.getDefaultConfig());\n        serviceBuilder.initialize().get();\n        StreamSegmentStore store = serviceBuilder.createStreamSegmentService();\n        int port = Config.SERVICE_PORT;\n        @Cleanup\n        PravegaConnectionListener server = new PravegaConnectionListener(false, port, store);\n        server.startListening();\n\n        // Create controller object for testing against a separate controller report.\n        ControllerWrapper controllerWrapper = new ControllerWrapper(zkTestServer.getConnectString(), port);\n        Controller controller = controllerWrapper.getController();\n\n        final String scope = \"scope\";\n        controllerWrapper.getControllerService().createScope(scope).get();\n\n        final String streamName = \"stream1\";\n        final StreamConfiguration config =\n                StreamConfiguration.builder().scope(scope).streamName(streamName).scalingPolicy(\n                        ScalingPolicy.fixed(1)).build();\n\n        Stream stream = new StreamImpl(scope, streamName);\n\n        log.info(\"Creating stream {}/{}\", scope, streamName);\n        if (!controller.createStream(config).get()) {\n            log.error(\"Stream already existed, exiting\");\n            return;\n        }\n\n        // Test 1: scale stream: split one segment into two\n        log.info(\"Scaling stream {}/{}, splitting one segment into two\", scope, streamName);\n        Map<Double, Double> map = new HashMap<>();\n        map.put(0.0, 0.5);\n        map.put(0.5, 1.0);\n\n        if (!controller.scaleStream(stream, Collections.singletonList(0), map).get()) {\n            log.error(\"Scale stream: splitting segment into two failed, exiting\");\n            return;\n        }\n\n        // Test 2: scale stream: merge two segments into one\n        log.info(\"Scaling stream {}/{}, merging two segments into one\", scope, streamName);\n        CompletableFuture<Boolean> scaleResponseFuture = controller.scaleStream(stream, Arrays.asList(1, 2), Collections.singletonMap(0.0, 1.0));\n        \n        if (!scaleResponseFuture.get()) {\n            log.error(\"Scale stream: merging two segments into one failed, exiting\");\n            return;\n        }\n\n        // Test 3: create a transaction, and try scale operation, it should fail with precondition check failure\n        CompletableFuture<UUID> txIdFuture = controller.createTransaction(stream, 5000, 3600000, 60000);\n        UUID txId = txIdFuture.get();\n        if (txId == null) {\n            log.error(\"Create transaction failed, exiting\");\n            return;\n        }\n\n        log.info(\"Scaling stream {}/{}, splitting one segment into two, while transaction is ongoing\",\n                scope, streamName);\n        scaleResponseFuture = controller.scaleStream(stream, Collections.singletonList(3), map);\n        CompletableFuture<Boolean> future = scaleResponseFuture.whenComplete((r, e) -> {\n            if (e != null) {\n                log.error(\"Failed: scale with ongoing transaction.\", e);\n            } else if (FutureHelpers.getAndHandleExceptions(\n                    controller.checkTransactionStatus(stream, txId), RuntimeException::new) != Transaction.Status.OPEN) {\n                log.info(\"Success: scale with ongoing transaction.\");\n            } else {\n                log.error(\"Failed: scale with ongoing transaction.\");\n            }\n        });\n\n        CompletableFuture<Void> statusFuture = controller.abortTransaction(stream, txId);\n        statusFuture.get();\n        future.get();\n\n        log.info(\"All scaling test PASSED\");\n\n        System.exit(0);\n    }\n}\n"
    },
    {
      "path": "integrationtests/src/main/java/com/emc/pravega/integrationtests/utils/SetupUtils.java",
      "content": "/**\n * Copyright (c) 2017 Dell Inc., or its subsidiaries.\n */\npackage com.emc.pravega.integrationtests.utils;\n\nimport com.emc.pravega.ClientFactory;\nimport com.emc.pravega.ReaderGroupManager;\nimport com.emc.pravega.StreamManager;\nimport com.emc.pravega.connectors.IntegerSerializer;\nimport com.emc.pravega.controller.util.Config;\nimport com.emc.pravega.demo.ControllerWrapper;\nimport com.emc.pravega.service.contracts.StreamSegmentStore;\nimport com.emc.pravega.service.server.host.handler.PravegaConnectionListener;\nimport com.emc.pravega.service.server.store.ServiceBuilder;\nimport com.emc.pravega.service.server.store.ServiceBuilderConfig;\nimport com.emc.pravega.stream.EventStreamReader;\nimport com.emc.pravega.stream.EventStreamWriter;\nimport com.emc.pravega.stream.EventWriterConfig;\nimport com.emc.pravega.stream.ReaderConfig;\nimport com.emc.pravega.stream.ReaderGroupConfig;\nimport com.emc.pravega.stream.ScalingPolicy;\nimport com.emc.pravega.stream.StreamConfiguration;\nimport com.emc.pravega.testcommon.TestUtils;\nimport com.google.common.base.Preconditions;\nimport lombok.Cleanup;\nimport lombok.Getter;\nimport lombok.extern.slf4j.Slf4j;\nimport org.apache.curator.test.TestingServer;\n\nimport javax.annotation.concurrent.NotThreadSafe;\nimport java.net.URI;\nimport java.util.Collections;\nimport java.util.UUID;\nimport java.util.concurrent.atomic.AtomicBoolean;\n\n/**\n * Utility functions for creating the test setup.\n */\n@Slf4j\n@NotThreadSafe\npublic final class SetupUtils {\n    // The controller endpoint.\n    @Getter\n    private URI controllerUri = null;\n\n    // The different services.\n    private ControllerWrapper controllerWrapper = null;\n    private PravegaConnectionListener server = null;\n    private TestingServer zkTestServer = null;\n\n    // Manage the state of the class.\n    private final AtomicBoolean started = new AtomicBoolean(false);\n\n    // The test Scope name.\n    @Getter\n    private final String scope = \"scope\";\n\n    /**\n     * Start all pravega related services required for the test deployment.\n     *\n     * @throws Exception on any errors.\n     */\n    public void startAllServices() throws Exception {\n        if (!this.started.compareAndSet(false, true)) {\n            log.warn(\"Services already started, not attempting to start again\");\n            return;\n        }\n\n        // Start Pravega Service.\n        ServiceBuilder serviceBuilder = ServiceBuilder.newInMemoryBuilder(ServiceBuilderConfig.getDefaultConfig());\n        serviceBuilder.initialize().get();\n        StreamSegmentStore store = serviceBuilder.createStreamSegmentService();\n        int servicePort = TestUtils.randomPort();\n        this.server = new PravegaConnectionListener(false, servicePort, store);\n        this.server.startListening();\n        log.info(\"Started Pravega Service\");\n\n        // Start zookeeper.\n        this.zkTestServer = new TestingServer();\n        this.zkTestServer.start();\n\n        // Start Controller.\n        int controllerPort = TestUtils.randomPort();\n        this.controllerWrapper = new ControllerWrapper(\n                this.zkTestServer.getConnectString(), true, true, controllerPort, \"localhost\", servicePort,\n                Config.HOST_STORE_CONTAINER_COUNT);\n        this.controllerWrapper.getController().createScope(this.scope).get();\n        this.controllerUri = URI.create(\"tcp://localhost:\" + String.valueOf(controllerPort));\n        log.info(\"Initialized Pravega Controller\");\n    }\n\n    /**\n     * Stop the pravega cluster and release all resources.\n     *\n     * @throws Exception on any errors.\n     */\n    public void stopAllServices() throws Exception {\n        if (!this.started.compareAndSet(true, false)) {\n            log.warn(\"Services not yet started or already stopped, not attempting to stop\");\n            return;\n        }\n\n        this.controllerWrapper.close();\n        this.server.close();\n        this.zkTestServer.close();\n    }\n\n    /**\n     * Create the test stream.\n     *\n     * @param streamName     Name of the test stream.\n     * @param numSegments    Number of segments to be created for this stream.\n     *\n     * @throws Exception on any errors.\n     */\n    public void createTestStream(final String streamName, final int numSegments)\n            throws Exception {\n        Preconditions.checkState(this.started.get(), \"Services not yet started\");\n        Preconditions.checkNotNull(streamName);\n        Preconditions.checkArgument(numSegments > 0);\n\n        @Cleanup\n        StreamManager streamManager = StreamManager.create(this.controllerUri);\n        streamManager.createScope(this.scope);\n        streamManager.createStream(this.scope, streamName,\n                StreamConfiguration.builder()\n                        .scope(this.scope)\n                        .streamName(streamName)\n                        .scalingPolicy(ScalingPolicy.fixed(numSegments))\n                        .build());\n        log.info(\"Created stream: \" + streamName);\n    }\n\n    /**\n     * Create a stream writer for writing Integer events.\n     *\n     * @param streamName    Name of the test stream.\n     *\n     * @return Stream writer instance.\n     */\n    public EventStreamWriter<Integer> getIntegerWriter(final String streamName) {\n        Preconditions.checkState(this.started.get(), \"Services not yet started\");\n        Preconditions.checkNotNull(streamName);\n\n        ClientFactory clientFactory = ClientFactory.withScope(this.scope, this.controllerUri);\n        return clientFactory.createEventWriter(\n                streamName,\n                new IntegerSerializer(),\n                EventWriterConfig.builder().build());\n    }\n\n    /**\n     * Create a stream reader for reading Integer events.\n     *\n     * @param streamName    Name of the test stream.\n     *\n     * @return Stream reader instance.\n     */\n    public EventStreamReader<Integer> getIntegerReader(final String streamName) {\n        Preconditions.checkState(this.started.get(), \"Services not yet started\");\n        Preconditions.checkNotNull(streamName);\n\n        ReaderGroupManager readerGroupManager = ReaderGroupManager.withScope(this.scope, this.controllerUri);\n        final String readerGroup = \"testReaderGroup\" + this.scope + streamName;\n        readerGroupManager.createReaderGroup(\n                readerGroup,\n                ReaderGroupConfig.builder().startingTime(0).build(),\n                Collections.singleton(streamName));\n\n        ClientFactory clientFactory = ClientFactory.withScope(this.scope, this.controllerUri);\n        final String readerGroupId = UUID.randomUUID().toString();\n        return clientFactory.createReader(\n                readerGroupId,\n                readerGroup,\n                new IntegerSerializer(),\n                ReaderConfig.builder().build());\n    }\n}\n"
    },
    {
      "path": "integrationtests/src/test/java/com/emc/controller/pravega/server/ControllerServiceTest.java",
      "content": "/**\n *\n *  Copyright (c) 2017 Dell Inc., or its subsidiaries.\n *\n */\npackage com.emc.controller.pravega.server;\n\nimport com.emc.pravega.common.concurrent.FutureHelpers;\nimport com.emc.pravega.controller.store.stream.DataNotFoundException;\nimport com.emc.pravega.demo.ControllerWrapper;\nimport com.emc.pravega.service.contracts.StreamSegmentStore;\nimport com.emc.pravega.service.server.host.handler.PravegaConnectionListener;\nimport com.emc.pravega.service.server.store.ServiceBuilder;\nimport com.emc.pravega.service.server.store.ServiceBuilderConfig;\nimport com.emc.pravega.stream.ScalingPolicy;\nimport com.emc.pravega.stream.Segment;\nimport com.emc.pravega.stream.Stream;\nimport com.emc.pravega.stream.StreamConfiguration;\nimport com.emc.pravega.stream.impl.Controller;\nimport com.emc.pravega.stream.impl.StreamImpl;\nimport com.emc.pravega.stream.impl.StreamSegments;\nimport com.emc.pravega.testcommon.TestUtils;\nimport java.util.Map;\nimport java.util.concurrent.CompletableFuture;\nimport java.util.concurrent.CompletionException;\nimport java.util.concurrent.ExecutionException;\nimport org.apache.curator.test.TestingServer;\nimport org.junit.After;\nimport org.junit.Before;\nimport org.junit.Test;\n\nimport static org.junit.Assert.assertFalse;\nimport static org.junit.Assert.assertTrue;\n\npublic class ControllerServiceTest {\n\n    private final int controllerPort = TestUtils.randomPort();\n    private final String serviceHost = \"localhost\";\n    private final int servicePort = TestUtils.randomPort();\n    private final int containerCount = 4;\n    private TestingServer zkTestServer;\n    private PravegaConnectionListener server;\n    private ControllerWrapper controllerWrapper;\n    private ServiceBuilder serviceBuilder;\n    \n    @Before\n    public void setUp() throws Exception {\n        zkTestServer = new TestingServer();\n        \n        serviceBuilder = ServiceBuilder.newInMemoryBuilder(ServiceBuilderConfig.getDefaultConfig());\n        serviceBuilder.initialize().get();\n        StreamSegmentStore store = serviceBuilder.createStreamSegmentService();\n        \n        server = new PravegaConnectionListener(false, servicePort, store);\n        server.startListening();\n        \n        controllerWrapper = new ControllerWrapper(zkTestServer.getConnectString(), false, true,\n                                                                    controllerPort, serviceHost, servicePort, containerCount);\n    }\n    \n    @After\n    public void tearDown() throws Exception {\n        controllerWrapper.close();\n        server.close();\n        serviceBuilder.close();\n        zkTestServer.close();\n    }\n    \n    \n    @Test(timeout = 40000)\n    public void streamMetadataTest() throws Exception {\n        final String scope = \"testScope\";\n        final String stream = \"testStream\";\n\n        StreamConfiguration streamConfiguration = StreamConfiguration.builder()\n                .scope(scope)\n                .streamName(stream)\n                .scalingPolicy(ScalingPolicy.fixed(1))\n                .build();\n        Controller controller = controllerWrapper.getController();\n        // Create test scope. This operation should succeed.\n        assertTrue(controller.createScope(scope).join());\n\n        // Delete the test scope. This operation should also succeed.\n        assertTrue(controller.deleteScope(scope).join());\n\n        // Try creating a stream. It should fail, since the scope does not exist.\n        assertFalse(FutureHelpers.await(controller.createStream(streamConfiguration)));\n\n        // Again create the scope.\n        assertTrue(controller.createScope(scope).join());\n\n        // Try creating the stream again. It should succeed now, since the scope exists.\n        assertTrue(controller.createStream(streamConfiguration).join());\n\n        // Delete test scope. This operation should fail, since it is not empty.\n        assertFalse(FutureHelpers.await(controller.deleteScope(scope)));\n\n        // Delete a non-existent scope.\n        assertFalse(controller.deleteScope(\"non_existent_scope\").get());\n\n        // Create a scope with invalid characters. It should fail.\n        assertFalse(FutureHelpers.await(controller.createScope(\"abc/def\")));\n\n        // Try creating already existing scope. \n        assertFalse(controller.createScope(scope).join());\n\n        // Try creating stream with invalid characters. It should fail.\n        assertFalse(FutureHelpers.await(controller.createStream(StreamConfiguration.builder()\n                                                                                   .scope(scope)\n                                                                                   .streamName(\"abc/def\")\n                                                                                   .scalingPolicy(ScalingPolicy.fixed(1))\n                                                                                   .build())));\n\n        // Try creating already existing stream.\n        assertFalse(controller.createStream(streamConfiguration).join());\n    }\n    \n    \n    @Test(timeout = 40000)\n    public void testControllerService() throws Exception {\n        final String scope1 = \"scope1\";\n        final String scope2 = \"scope2\";\n        controllerWrapper.getControllerService().createScope(\"scope1\").get();\n        controllerWrapper.getControllerService().createScope(\"scope2\").get();\n        Controller controller = controllerWrapper.getController();\n\n        final String streamName1 = \"stream1\";\n        final String streamName2 = \"stream2\";\n        final ScalingPolicy scalingPolicy = ScalingPolicy.fixed(2);\n        final StreamConfiguration config1 = StreamConfiguration.builder()\n                .scope(scope1)\n                .streamName(streamName1)\n                .scalingPolicy(scalingPolicy)\n                .build();\n        final StreamConfiguration config2 = StreamConfiguration.builder()\n                .scope(scope2)\n                .streamName(streamName1)\n                .scalingPolicy(scalingPolicy)\n                .build();\n        final StreamConfiguration config3 = StreamConfiguration.builder()\n                .scope(scope1)\n                .streamName(streamName2)\n                .scalingPolicy(ScalingPolicy.fixed(3))\n                .build();\n\n        createAStream(controller, config1);\n        //Same name in different scope\n        createAStream(controller, config2);\n        //Different name in same scope\n        createAStream(controller, config3);\n        \n        final String scopeSeal = \"scopeSeal\";\n        final String streamNameSeal = \"streamSeal\";\n        sealAStream(controllerWrapper, controller, scalingPolicy, scopeSeal, streamNameSeal);\n        \n        sealASealedStream(controller, scopeSeal, streamNameSeal);\n \n        sealNonExistantStream(controller, scopeSeal);\n\n        streamDuplicationNotAllowed(controller, config1);\n       \n        //update stream config section\n\n        updateStreamName(controller, scope1, scalingPolicy);\n\n        updateScalingPolicy(controller, scope1, streamName1);\n\n        updateTargetRate(controller, scope1, streamName1);\n\n        updateScaleFactor(controller, scope1, streamName1);\n\n        updataMinSegmentes(controller, scope1, streamName1);\n\n        alterConfigOfNonExistantStream(controller);\n\n        //get currently active segments\n\n        getActiveSegments(controller, scope1, streamName1);\n\n        getActiveSegmentsForNonExistentStream(controller);\n\n        //get positions at a given time stamp\n\n        getSegmentsAtTime(controller, scope1, streamName1);\n        getSegmentsAtTime(controller, scope1, streamName2);\n\n        getSegmentsForNonExistentStream(controller);\n        \n        getSegmentsBeforeCreation(controller, scope1, streamName1);\n\n        getSegmentsAfterCreation(controller, scope1, streamName1);\n    }\n\n    private static void getSegmentsAfterCreation(Controller controller, final String scope,\n                                                 final String streamName) throws InterruptedException,\n                                                                          ExecutionException {\n        CompletableFuture<Map<Segment, Long>> segments = controller.getSegmentsAtTime(new StreamImpl(scope, streamName), System.currentTimeMillis() + 3600);\n        assertFalse(\"FAILURE: Fetching positions at given time in furture after stream creation failed\", segments.get().isEmpty());\n    }\n\n    private static void getSegmentsBeforeCreation(Controller controller, final String scope,\n                                                  final String streamName) throws InterruptedException,\n                                                                           ExecutionException {\n        CompletableFuture<Map<Segment, Long>> segments = controller.getSegmentsAtTime(new StreamImpl(scope, streamName), System.currentTimeMillis() - 36000);\n        assertFalse(\"FAILURE: Fetching positions at given time before stream creation failed\", segments.get().size() != controller.getCurrentSegments(scope, streamName).get().getSegments().size());\n       \n    }\n\n    private static void getSegmentsForNonExistentStream(Controller controller) throws InterruptedException {\n        Stream stream = new StreamImpl(\"scope\", \"streamName\");\n        try {\n            CompletableFuture<Map<Segment, Long>> segments = controller.getSegmentsAtTime(stream, System.currentTimeMillis());\n            assertTrue(\"FAILURE: Fetching positions for non existent stream\", segments.get().isEmpty());\n            \n            System.err.println(\"SUCCESS: Positions cannot be fetched for non existent stream\");\n        } catch (ExecutionException | CompletionException e) {\n            assertTrue(\"FAILURE: Fetching positions for non existent stream\", e.getCause() instanceof DataNotFoundException);\n            System.err.println(\"SUCCESS: Positions cannot be fetched for non existent stream\");\n        }\n    }\n\n    private static void getSegmentsAtTime(Controller controller, final String scope,\n                                            final String streamName) throws InterruptedException, ExecutionException {\n        CompletableFuture<Map<Segment, Long>> segments = controller.getSegmentsAtTime(new StreamImpl(scope, streamName), System.currentTimeMillis());\n        assertFalse(\"FAILURE: Fetching positions at given time stamp failed\", segments.get().isEmpty()); \n    }\n\n    private static void getActiveSegmentsForNonExistentStream(Controller controller) throws InterruptedException {\n        try {\n            CompletableFuture<StreamSegments> getActiveSegments = controller.getCurrentSegments(\"scope\", \"streamName\");\n            assertTrue(\"FAILURE: Fetching active segments for non existent stream\", getActiveSegments.get().getSegments().isEmpty());\n        } catch (ExecutionException | CompletionException e) {\n            assertTrue(\"FAILURE: Fetching active segments for non existent stream\", e.getCause() instanceof DataNotFoundException);\n        }\n    }\n\n    private static void getActiveSegments(Controller controller, final String scope,\n                                          final String streamName) throws InterruptedException, ExecutionException {\n        CompletableFuture<StreamSegments> getActiveSegments = controller.getCurrentSegments(scope, streamName);\n        assertFalse(\"FAILURE: Fetching active segments failed\", getActiveSegments.get().getSegments().isEmpty());\n        \n    }\n\n\n    private static void alterConfigOfNonExistantStream(Controller controller) {\n        assertFalse(FutureHelpers.await(controller.alterStream(StreamConfiguration.builder()\n                                                               .scope(\"scope\")\n                                                               .streamName(\"streamName\")\n                                                               .scalingPolicy(ScalingPolicy.byEventRate(200, 2, 3))\n                                                               .build())));\n    }\n\n    private static void updataMinSegmentes(Controller controller, final String scope,\n                                           final String streamName) throws InterruptedException, ExecutionException {\n        assertTrue(controller.alterStream(StreamConfiguration.builder()\n                                          .scope(scope)\n                                          .streamName(streamName)\n                                          .scalingPolicy(ScalingPolicy.byEventRate(200, 2, 3))\n                                          .build()).get());\n    }\n\n    private static void updateScaleFactor(Controller controller, final String scope,\n                                          final String streamName) throws InterruptedException, ExecutionException {\n        assertTrue(controller.alterStream(StreamConfiguration.builder()\n                                          .scope(scope)\n                                          .streamName(streamName)\n                                          .scalingPolicy(ScalingPolicy.byEventRate(100, 3, 2))\n                                          .build()).get());\n    }\n\n    private static void updateTargetRate(Controller controller, final String scope,\n                                         final String streamName) throws InterruptedException, ExecutionException {\n        assertTrue(controller.alterStream(StreamConfiguration.builder()\n                                          .scope(scope)\n                                          .streamName(streamName)\n                                          .scalingPolicy(ScalingPolicy.byEventRate(200, 2, 2))\n                                          .build()).get());\n    }\n\n    private static void updateScalingPolicy(Controller controller, final String scope,\n                                            final String streamName) throws InterruptedException, ExecutionException {\n        assertTrue(controller.alterStream(StreamConfiguration.builder()\n                                          .scope(scope)\n                                          .streamName(streamName)\n                                          .scalingPolicy(ScalingPolicy.byEventRate(100, 2, 2))\n                                          .build()).get());\n    }\n\n    private static void updateStreamName(Controller controller, final String scope,\n                                         final ScalingPolicy scalingPolicy) {\n        assertFalse(FutureHelpers.await(controller.alterStream(StreamConfiguration.builder()\n                                                               .scope(scope)\n                                                               .streamName(\"stream4\")\n                                                               .scalingPolicy(scalingPolicy)\n                                                               .build())));\n    }\n\n    private static void sealAStream(ControllerWrapper controllerWrapper, Controller controller,\n                                   final ScalingPolicy scalingPolicy, final String scopeSeal,\n                                   final String streamNameSeal) throws InterruptedException, ExecutionException {\n        controllerWrapper.getControllerService().createScope(\"scopeSeal\").get();\n\n        final StreamConfiguration configSeal = StreamConfiguration.builder()\n                .scope(scopeSeal)\n                .streamName(streamNameSeal)\n                .scalingPolicy(scalingPolicy)\n                .build();\n        assertTrue(controller.createStream(configSeal).get());\n\n        @SuppressWarnings(\"unused\")\n        StreamSegments result = controller.getCurrentSegments(scopeSeal, streamNameSeal).get();\n        assertTrue(controller.sealStream(scopeSeal, streamNameSeal).get());\n\n        StreamSegments currentSegs = controller.getCurrentSegments(scopeSeal, streamNameSeal).get();\n        assertTrue(\"FAILURE: No active segments should be present in a sealed stream\", currentSegs.getSegments().isEmpty());\n        \n    }\n\n    private static void createAStream(Controller controller, final StreamConfiguration config) throws InterruptedException,\n                                                                         ExecutionException {\n        assertTrue(controller.createStream(config).get());\n    }\n\n    private static void sealNonExistantStream(Controller controller,\n                                              final String scopeSeal) {\n        assertFalse(FutureHelpers.await(controller.sealStream(scopeSeal, \"nonExistentStream\")));\n    }\n\n    private static void streamDuplicationNotAllowed(Controller controller, final StreamConfiguration config) throws InterruptedException,\n                                                                                       ExecutionException {\n        assertFalse(controller.createStream(config).get());\n    }\n\n    private static void sealASealedStream(Controller controller, final String scopeSeal,\n                                          final String streamNameSeal) throws InterruptedException, ExecutionException {\n        assertTrue(controller.sealStream(scopeSeal, streamNameSeal).get());\n\n        StreamSegments currentSegs = controller.getCurrentSegments(scopeSeal, streamNameSeal).get();\n        assertTrue(\"FAILURE: No active segments should be present in a sealed stream\", currentSegs.getSegments().isEmpty());\n        \n    }\n\n}\n"
    },
    {
      "path": "integrationtests/src/test/java/com/emc/controller/pravega/server/EventProcessorTest.java",
      "content": "/**\n * Copyright (c) 2017 Dell Inc., or its subsidiaries.\n */\npackage com.emc.controller.pravega.server;\n\nimport com.emc.pravega.ClientFactory;\nimport com.emc.pravega.controller.eventProcessor.CheckpointConfig;\nimport com.emc.pravega.controller.eventProcessor.ControllerEvent;\nimport com.emc.pravega.controller.eventProcessor.EventProcessorConfig;\nimport com.emc.pravega.controller.eventProcessor.EventProcessorGroup;\nimport com.emc.pravega.controller.eventProcessor.EventProcessorGroupConfig;\nimport com.emc.pravega.controller.eventProcessor.EventProcessorSystem;\nimport com.emc.pravega.controller.eventProcessor.ExceptionHandler;\nimport com.emc.pravega.controller.eventProcessor.impl.EventProcessor;\nimport com.emc.pravega.controller.eventProcessor.impl.EventProcessorGroupConfigImpl;\nimport com.emc.pravega.controller.eventProcessor.impl.EventProcessorSystemImpl;\nimport com.emc.pravega.controller.store.checkpoint.CheckpointStoreFactory;\nimport com.emc.pravega.demo.ControllerWrapper;\nimport com.emc.pravega.service.contracts.StreamSegmentStore;\nimport com.emc.pravega.service.server.host.handler.PravegaConnectionListener;\nimport com.emc.pravega.service.server.store.ServiceBuilder;\nimport com.emc.pravega.service.server.store.ServiceBuilderConfig;\nimport com.emc.pravega.stream.EventStreamWriter;\nimport com.emc.pravega.stream.EventWriterConfig;\nimport com.emc.pravega.stream.ScalingPolicy;\nimport com.emc.pravega.stream.StreamConfiguration;\nimport com.emc.pravega.stream.impl.ClientFactoryImpl;\nimport com.emc.pravega.stream.impl.Controller;\nimport com.emc.pravega.stream.impl.JavaSerializer;\nimport com.emc.pravega.stream.impl.netty.ConnectionFactoryImpl;\nimport com.emc.pravega.testcommon.TestUtils;\nimport com.google.common.base.Preconditions;\nimport java.io.Serializable;\nimport java.util.concurrent.CompletableFuture;\nimport lombok.AllArgsConstructor;\nimport lombok.Cleanup;\nimport lombok.Data;\nimport lombok.extern.slf4j.Slf4j;\nimport org.apache.curator.test.TestingServer;\nimport org.junit.Assert;\nimport org.junit.Test;\n\n/**\n * End-to-end tests for event processor.\n */\n@Slf4j\npublic class EventProcessorTest {\n\n    public static class TestEventProcessor extends EventProcessor<TestEvent> {\n        long sum;\n        CompletableFuture<Long> result;\n        final boolean throwErrors;\n\n        public TestEventProcessor(Boolean throwErrors, CompletableFuture<Long> result) {\n            Preconditions.checkNotNull(throwErrors);\n            Preconditions.checkNotNull(result);\n            sum = 0;\n            this.result = result;\n            this.throwErrors = throwErrors;\n        }\n\n        @Override\n        protected void process(TestEvent event) {\n            if (event.getNumber() < 0) {\n                result.complete(sum);\n                throw new RuntimeException();\n            } else {\n                int val = event.getNumber();\n                sum += val;\n                if (throwErrors && val % 2 == 0) {\n                    throw new IllegalArgumentException();\n                }\n            }\n        }\n    }\n\n    @Data\n    @AllArgsConstructor\n    public static class TestEvent implements ControllerEvent, Serializable {\n        private static final long serialVersionUID = 1L;\n        int number;\n    }\n\n    public static void main(String[] args) throws Exception {\n        new EventProcessorTest().testEventProcessor();\n        System.exit(0);\n    }\n\n    @Test(timeout = 60000)\n    public void testEventProcessor() throws Exception {\n        @Cleanup\n        TestingServer zkTestServer = new TestingServer();\n\n        ServiceBuilder serviceBuilder = ServiceBuilder.newInMemoryBuilder(ServiceBuilderConfig.getDefaultConfig());\n        serviceBuilder.initialize().get();\n        StreamSegmentStore store = serviceBuilder.createStreamSegmentService();\n        int port = TestUtils.randomPort();\n        @Cleanup\n        PravegaConnectionListener server = new PravegaConnectionListener(false, port, store);\n        server.startListening();\n        @Cleanup\n        ControllerWrapper controllerWrapper = new ControllerWrapper(zkTestServer.getConnectString(), port);\n        Controller controller = controllerWrapper.getController();\n\n        // Create controller object for testing against a separate controller process.\n        // ControllerImpl controller = new ControllerImpl(\"localhost\", 9090);\n\n        final String host = \"host\";\n        final String scope = \"controllerScope\";\n        final String streamName = \"stream1\";\n        final String readerGroup = \"readerGroup\";\n\n        final CompletableFuture<Boolean> createScopeStatus = controller.createScope(scope);\n\n        if (!createScopeStatus.join()) {\n            throw new RuntimeException(\"Scope already existed\");\n        }\n\n        final StreamConfiguration config = StreamConfiguration.builder()\n                .scope(scope)\n                .streamName(streamName)\n                .scalingPolicy(ScalingPolicy.fixed(1))\n                .build();\n\n        System.err.println(String.format(\"Creating stream (%s, %s)\", scope, streamName));\n        CompletableFuture<Boolean> createStatus = controller.createStream(config);\n        if (!createStatus.get()) {\n            System.err.println(\"Stream alrady existed, exiting\");\n            return;\n        }\n\n        ConnectionFactoryImpl connectionFactory = new ConnectionFactoryImpl(false);\n        @Cleanup\n        ClientFactory clientFactory = new ClientFactoryImpl(scope, controller, connectionFactory);\n\n        @Cleanup\n        EventStreamWriter<TestEvent> producer = clientFactory.createEventWriter(streamName,\n                new JavaSerializer<>(), EventWriterConfig.builder().build());\n\n        int[] input = {1, 2, 3, 4, 5, 6, 7, 8, 9, 10};\n        int expectedSum = input.length * (input.length + 1) / 2;\n\n        for (int i = 0; i < input.length; i++) {\n            producer.writeEvent(\"key\", new TestEvent(input[i]));\n        }\n        producer.writeEvent(\"key\", new TestEvent(-1));\n        producer.flush();\n\n        EventProcessorSystem system = new EventProcessorSystemImpl(\"Controller\", host, scope, controller, connectionFactory);\n\n        CheckpointConfig.CheckpointPeriod period =\n                CheckpointConfig.CheckpointPeriod.builder()\n                        .numEvents(1)\n                        .numSeconds(1)\n                        .build();\n\n        CheckpointConfig checkpointConfig =\n                CheckpointConfig.builder()\n                        .type(CheckpointConfig.Type.Periodic)\n                        .checkpointPeriod(period)\n                        .build();\n\n        EventProcessorGroupConfig eventProcessorGroupConfig =\n                EventProcessorGroupConfigImpl.builder()\n                        .eventProcessorCount(1)\n                        .readerGroupName(readerGroup)\n                        .streamName(streamName)\n                        .checkpointConfig(checkpointConfig)\n                        .build();\n        CompletableFuture<Long> result = new CompletableFuture<>();\n        // Test case 1. Actor does not throw any exception during normal operation.\n        EventProcessorConfig<TestEvent> eventProcessorConfig = EventProcessorConfig.<TestEvent>builder()\n                .supplier(() -> new TestEventProcessor(false, result))\n                .serializer(new JavaSerializer<>())\n                .decider((Throwable e) -> ExceptionHandler.Directive.Stop)\n                .config(eventProcessorGroupConfig)\n                .build();\n        @Cleanup\n        EventProcessorGroup<TestEvent> eventProcessorGroup =\n                system.createEventProcessorGroup(eventProcessorConfig, CheckpointStoreFactory.createInMemoryStore());\n\n        Long value = result.join();\n        Assert.assertEquals(expectedSum, value.longValue());\n        log.info(\"SUCCESS: received expected sum = \" + expectedSum);\n    }\n}\n"
    },
    {
      "path": "integrationtests/src/test/java/com/emc/controller/pravega/server/StreamMetadataTest.java",
      "content": "/**\n * Copyright (c) 2017 Dell Inc., or its subsidiaries.\n */\npackage com.emc.controller.pravega.server;\n\nimport com.emc.pravega.demo.ControllerWrapper;\nimport com.emc.pravega.service.contracts.StreamSegmentStore;\nimport com.emc.pravega.service.server.host.handler.PravegaConnectionListener;\nimport com.emc.pravega.service.server.store.ServiceBuilder;\nimport com.emc.pravega.service.server.store.ServiceBuilderConfig;\nimport com.emc.pravega.stream.ScalingPolicy;\nimport com.emc.pravega.stream.Segment;\nimport com.emc.pravega.stream.Stream;\nimport com.emc.pravega.stream.StreamConfiguration;\nimport com.emc.pravega.stream.impl.Controller;\nimport com.emc.pravega.stream.impl.StreamImpl;\nimport com.emc.pravega.testcommon.TestUtils;\nimport java.util.Map;\nimport java.util.concurrent.CompletableFuture;\nimport lombok.Cleanup;\nimport org.apache.curator.test.TestingServer;\nimport org.junit.Test;\n\nimport static com.emc.pravega.testcommon.AssertExtensions.assertThrows;\nimport static org.junit.Assert.assertEquals;\nimport static org.junit.Assert.assertFalse;\nimport static org.junit.Assert.assertTrue;\n\npublic class StreamMetadataTest {\n\n    @Test(timeout = 60000)\n    public void testMedadataOperations() throws Exception {\n        @Cleanup\n        TestingServer zkTestServer = new TestingServer();\n\n        ServiceBuilder serviceBuilder = ServiceBuilder.newInMemoryBuilder(ServiceBuilderConfig.getDefaultConfig());\n        serviceBuilder.initialize().get();\n        StreamSegmentStore store = serviceBuilder.createStreamSegmentService();\n        int port = TestUtils.randomPort();\n        @Cleanup\n        PravegaConnectionListener server = new PravegaConnectionListener(false, port, store);\n        server.startListening();\n        @Cleanup\n        ControllerWrapper controllerWrapper = new ControllerWrapper(zkTestServer.getConnectString(), port);\n        Controller controller = controllerWrapper.getController();\n\n        final String scope1 = \"scope1\";\n        final String streamName1 = \"stream1\";\n        final String scopeSeal = \"scopeSeal\";\n        final String streamNameSeal = \"streamSeal\";\n        final String scope2 = \"scope2\";\n        final String streamName2 = \"stream2\";\n\n        controllerWrapper.getControllerService().createScope(scope1).get();\n        final ScalingPolicy scalingPolicy = ScalingPolicy.fixed(2);\n        final StreamConfiguration config1 = StreamConfiguration.builder()\n                                                               .scope(scope1)\n                                                               .streamName(streamName1)\n                                                               .scalingPolicy(scalingPolicy)\n                                                               .build();\n\n        // create stream and seal stream\n\n        // CS1:create a stream :given a streamName, scope and config\n        assertTrue(controller.createStream(config1).get());\n\n        // Seal a stream given a streamName and scope.\n        controllerWrapper.getControllerService().createScope(scopeSeal).get();\n\n        final StreamConfiguration configSeal = StreamConfiguration.builder()\n                                                                  .scope(scopeSeal)\n                                                                  .streamName(streamNameSeal)\n                                                                  .scalingPolicy(scalingPolicy)\n                                                                  .build();\n\n        assertTrue(controller.createStream(configSeal).get());\n        controller.getCurrentSegments(scopeSeal, streamNameSeal).get();\n\n        assertTrue(controller.sealStream(scopeSeal, streamNameSeal).get());\n\n        assertTrue(\"FAILURE: No active segments should be present in a sealed stream\",\n                   controller.getCurrentSegments(scopeSeal, streamNameSeal).get().getSegments().isEmpty());\n\n        // Seal an already sealed stream.\n        assertTrue(controller.sealStream(scopeSeal, streamNameSeal).get());\n        assertTrue(\"FAILURE: No active segments should be present in a sealed stream\",\n                   controller.getCurrentSegments(scopeSeal, streamNameSeal).get().getSegments().isEmpty());\n\n        assertThrows(\"FAILURE: Seal operation on a non-existent stream returned \",\n                     controller.sealStream(scopeSeal, \"nonExistentStream\"),\n                     t -> true);\n\n        // CS2:stream duplication not allowed\n        assertFalse(controller.createStream(config1).get());\n\n        // CS3:create a stream with same stream name in different scopes\n        controllerWrapper.getControllerService().createScope(scope2).get();\n\n        final StreamConfiguration config2 = StreamConfiguration.builder()\n                                                               .scope(scope2)\n                                                               .streamName(streamName1)\n                                                               .scalingPolicy(scalingPolicy)\n                                                               .build();\n        assertTrue(controller.createStream(config2).get());\n\n        // CS4:create a stream with different stream name and config in same scope\n        final StreamConfiguration config3 = StreamConfiguration.builder()\n                                                               .scope(scope1)\n                                                               .streamName(streamName2)\n                                                               .scalingPolicy(ScalingPolicy.fixed(3))\n                                                               .build();\n\n        assertTrue(controller.createStream(config3).get());\n\n        // update stream config(alter Stream)\n\n        // AS3:update the type of scaling policy\n        final StreamConfiguration config6 = StreamConfiguration.builder()\n                                                               .scope(scope1)\n                                                               .streamName(streamName1)\n                                                               .scalingPolicy(ScalingPolicy.byDataRate(100, 2, 2))\n                                                               .build();\n        assertTrue(controller.alterStream(config6).get());\n\n        // AS4:update the target rate of scaling policy\n        final StreamConfiguration config7 = StreamConfiguration.builder()\n                                                               .scope(scope1)\n                                                               .streamName(streamName1)\n                                                               .scalingPolicy(ScalingPolicy.byDataRate(200, 2, 2))\n                                                               .build();\n        assertTrue(controller.alterStream(config7).get());\n\n        // AS5:update the scale factor of scaling policy\n        final StreamConfiguration config8 = StreamConfiguration.builder()\n                                                               .scope(scope1)\n                                                               .streamName(streamName1)\n                                                               .scalingPolicy(ScalingPolicy.byDataRate(200, 4, 2))\n                                                               .build();\n        assertTrue(controller.alterStream(config8).get());\n\n        // AS6:update the minNumsegments of scaling policy\n        final StreamConfiguration config9 = StreamConfiguration.builder()\n                                                               .scope(scope1)\n                                                               .streamName(streamName1)\n                                                               .scalingPolicy(ScalingPolicy.byDataRate(200, 4, 3))\n                                                               .build();\n        assertTrue(controller.alterStream(config9).get());\n\n        // AS7:alter configuration of non-existent stream.\n        final StreamConfiguration config = StreamConfiguration.builder()\n                                                              .scope(\"scope\")\n                                                              .streamName(\"streamName\")\n                                                              .scalingPolicy(ScalingPolicy.fixed(2))\n                                                              .build();\n        CompletableFuture<Boolean> updateStatus = controller.alterStream(config);\n        assertThrows(\"FAILURE: Altering the configuration of a non-existent stream\", updateStatus, t -> true);\n\n        // get currently active segments\n\n        // GCS1:get active segments of the stream\n        assertFalse(controller.getCurrentSegments(scope1, streamName1).get().getSegments().isEmpty());\n\n        // GCS2:Get active segments for a non-existent stream.\n\n        assertThrows(\"Active segments cannot be fetched for non existent stream\",\n                     controller.getCurrentSegments(\"scope\", \"streamName\"),\n                     t -> true);\n\n        // get positions at a given time stamp\n\n        // PS1:get positions at a given time stamp:given stream, time stamp, count\n        Stream stream1 = new StreamImpl(scope1, streamName1);\n        CompletableFuture<Map<Segment, Long>> segments = controller.getSegmentsAtTime(stream1,\n                                                                                      System.currentTimeMillis());\n        assertEquals(2, segments.get().size());\n\n        // PS2:get positions of a stream with different count\n        Stream stream2 = new StreamImpl(scope1, streamName2);\n        segments = controller.getSegmentsAtTime(stream2, System.currentTimeMillis());\n        assertEquals(3, segments.get().size());\n\n        // PS4:get positions at a given timestamp for non-existent stream.\n        Stream stream = new StreamImpl(\"scope\", \"streamName\");\n        assertThrows(\"Fetching segments at given time stamp for non existent stream \",\n                     controller.getSegmentsAtTime(stream, System.currentTimeMillis()),\n                     t -> true);\n\n        // PS5:Get position at time before stream creation\n        segments = controller.getSegmentsAtTime(stream1, System.currentTimeMillis() - 36000);\n        assertEquals(controller.getCurrentSegments(scope1, streamName1).get().getSegments().size(),\n                     segments.get().size());\n\n        // PS6:Get positions at a time in future after stream creation\n        segments = controller.getSegmentsAtTime(stream1, System.currentTimeMillis() + 3600);\n        assertTrue(!segments.get().isEmpty());\n\n    }\n}\n"
    },
    {
      "path": "integrationtests/src/test/java/com/emc/pravega/integrationtests/ControllerBootstrapTest.java",
      "content": "/**\n *\n *  Copyright (c) 2017 Dell Inc., or its subsidiaries.\n *\n */\npackage com.emc.pravega.integrationtests;\n\nimport com.emc.pravega.demo.ControllerWrapper;\nimport com.emc.pravega.service.contracts.StreamSegmentStore;\nimport com.emc.pravega.service.server.host.handler.PravegaConnectionListener;\nimport com.emc.pravega.service.server.store.ServiceBuilder;\nimport com.emc.pravega.service.server.store.ServiceBuilderConfig;\nimport com.emc.pravega.stream.ScalingPolicy;\nimport com.emc.pravega.stream.StreamConfiguration;\nimport com.emc.pravega.stream.impl.Controller;\nimport com.emc.pravega.stream.impl.StreamImpl;\nimport com.emc.pravega.testcommon.TestUtils;\nimport java.util.UUID;\nimport java.util.concurrent.CompletableFuture;\nimport java.util.concurrent.CompletionException;\nimport java.util.concurrent.TimeUnit;\nimport org.apache.curator.test.TestingServer;\nimport org.junit.After;\nimport org.junit.Assert;\nimport org.junit.Before;\nimport org.junit.Test;\n\n/**\n * Collection of tests to validate controller bootstrap sequence.\n */\npublic class ControllerBootstrapTest {\n\n    private static final String SCOPE = \"testScope\";\n    private static final String STREAM = \"testStream\";\n\n    private final int controllerPort = TestUtils.randomPort();\n    private final int servicePort = TestUtils.randomPort();\n    private TestingServer zkTestServer;\n    private ControllerWrapper controllerWrapper;\n    private PravegaConnectionListener server;\n\n    @Before\n    public void setup() {\n        final String serviceHost = \"localhost\";\n        final int containerCount = 4;\n\n        // 1. Start ZK\n        try {\n            zkTestServer = new TestingServer();\n        } catch (Exception e) {\n            Assert.fail(\"Failed starting ZK test server\");\n        }\n\n        // 2. Start controller\n        try {\n            controllerWrapper = new ControllerWrapper(zkTestServer.getConnectString(), false, true,\n                    controllerPort, serviceHost, servicePort, containerCount);\n        } catch (Exception e) {\n            Assert.fail(\"Failed starting ControllerWrapper\");\n        }\n    }\n\n    @After\n    public void cleanup() throws Exception {\n        if (controllerWrapper != null) {\n            controllerWrapper.close();\n            controllerWrapper.awaitTerminated();\n        }\n        if (server != null) {\n            server.close();\n        }\n        if (zkTestServer != null) {\n            zkTestServer.close();\n        }\n    }\n\n    @Test(timeout = 20000)\n    public void bootstrapTest() throws Exception {\n        Controller controller = controllerWrapper.getController();\n\n        // Create test scope. This operation should succeed.\n        Boolean scopeStatus = controller.createScope(SCOPE).join();\n        Assert.assertEquals(true, scopeStatus);\n\n        // Try creating a stream. It should not complete until Pravega host has started.\n        // After Pravega host starts, stream should be successfully created.\n        StreamConfiguration streamConfiguration = StreamConfiguration.builder()\n                .scope(SCOPE)\n                .streamName(STREAM)\n                .scalingPolicy(ScalingPolicy.fixed(1))\n                .build();\n        CompletableFuture<Boolean> streamStatus = controller.createStream(streamConfiguration);\n        Assert.assertTrue(!streamStatus.isDone());\n\n        // Create transaction should fail.\n        CompletableFuture<UUID> txIdFuture = controller.createTransaction(new StreamImpl(SCOPE, STREAM),\n                10000, 30000, 30000);\n\n        try {\n            txIdFuture.join();\n            Assert.fail();\n        } catch (CompletionException ce) {\n            Assert.assertEquals(IllegalStateException.class, ce.getCause().getClass());\n            Assert.assertTrue(\"Expected failure\", true);\n        }\n\n        // Now start Pravega service.\n        ServiceBuilder serviceBuilder = ServiceBuilder.newInMemoryBuilder(ServiceBuilderConfig.getDefaultConfig());\n        serviceBuilder.initialize().get();\n        StreamSegmentStore store = serviceBuilder.createStreamSegmentService();\n\n        server = new PravegaConnectionListener(false, servicePort, store);\n        server.startListening();\n\n        // Ensure that create stream succeeds.\n        try {\n            Boolean status = streamStatus.join();\n            Assert.assertEquals(true, status);\n        } catch (CompletionException ce) {\n            Assert.fail();\n        }\n\n        // Sleep for a while for initialize to complete\n        boolean initialized = controllerWrapper.awaitTasksModuleInitialization(5000, TimeUnit.MILLISECONDS);\n        Assert.assertTrue(initialized);\n\n        // Now create transaction should succeed.\n        txIdFuture = controller.createTransaction(new StreamImpl(SCOPE, STREAM), 10000, 30000, 30000);\n\n        try {\n            UUID id = txIdFuture.join();\n            Assert.assertNotNull(id);\n        } catch (CompletionException ce) {\n            Assert.fail();\n        }\n\n        controllerWrapper.awaitRunning();\n    }\n}\n"
    },
    {
      "path": "integrationtests/src/test/java/com/emc/pravega/integrationtests/ControllerStreamMetadataTest.java",
      "content": "/**\n *\n *  Copyright (c) 2017 Dell Inc., or its subsidiaries.\n *\n */\npackage com.emc.pravega.integrationtests;\n\nimport com.emc.pravega.StreamManager;\nimport com.emc.pravega.common.concurrent.FutureHelpers;\nimport com.emc.pravega.demo.ControllerWrapper;\nimport com.emc.pravega.service.contracts.StreamSegmentStore;\nimport com.emc.pravega.service.server.host.handler.PravegaConnectionListener;\nimport com.emc.pravega.service.server.store.ServiceBuilder;\nimport com.emc.pravega.service.server.store.ServiceBuilderConfig;\nimport com.emc.pravega.stream.ScalingPolicy;\nimport com.emc.pravega.stream.StreamConfiguration;\nimport com.emc.pravega.stream.impl.Controller;\nimport com.emc.pravega.stream.impl.StreamManagerImpl;\nimport com.emc.pravega.testcommon.TestUtils;\nimport lombok.extern.slf4j.Slf4j;\nimport org.apache.curator.test.TestingServer;\nimport org.junit.After;\nimport org.junit.Before;\nimport org.junit.Test;\n\nimport static org.junit.Assert.assertFalse;\nimport static org.junit.Assert.assertTrue;\n\n/**\n * Controller stream metadata tests.\n */\n@Slf4j\npublic class ControllerStreamMetadataTest {\n    private static final String SCOPE = \"testScope\";\n    private static final String STREAM = \"testStream\";\n    private TestingServer zkTestServer = null;\n    private PravegaConnectionListener server = null;\n    private ControllerWrapper controllerWrapper = null;\n    private Controller controller = null;\n    private StreamConfiguration streamConfiguration = null;\n\n    @Before\n    public void setUp() throws Exception {\n        final int controllerPort = TestUtils.randomPort();\n        final String serviceHost = \"localhost\";\n        final int servicePort = TestUtils.randomPort();\n        final int containerCount = 4;\n\n        try {\n            // 1. Start ZK\n            this.zkTestServer = new TestingServer();\n\n            // 2. Start Pravega service.\n            ServiceBuilder serviceBuilder = ServiceBuilder.newInMemoryBuilder(ServiceBuilderConfig.getDefaultConfig());\n            serviceBuilder.initialize().get();\n            StreamSegmentStore store = serviceBuilder.createStreamSegmentService();\n\n            this.server = new PravegaConnectionListener(false, servicePort, store);\n            this.server.startListening();\n\n            // 3. Start controller\n            this.controllerWrapper = new ControllerWrapper(zkTestServer.getConnectString(), false, true,\n                    controllerPort, serviceHost, servicePort, containerCount);\n            this.controller = controllerWrapper.getController();\n            this.streamConfiguration = StreamConfiguration.builder()\n                    .scope(SCOPE)\n                    .streamName(STREAM)\n                    .scalingPolicy(ScalingPolicy.fixed(1))\n                    .build();\n        } catch (Exception e) {\n            log.error(\"Error during setup\", e);\n            throw e;\n        }\n    }\n\n    @After\n    public void tearDown() {\n        try {\n            if (this.controllerWrapper != null) {\n                this.controllerWrapper.close();\n                this.controllerWrapper = null;\n            }\n            if (this.server != null) {\n                this.server.close();\n                this.server = null;\n            }\n            if (this.zkTestServer != null) {\n                this.zkTestServer.close();\n                this.zkTestServer = null;\n            }\n        } catch (Exception e) {\n            log.warn(\"Exception while tearing down\", e);\n        }\n    }\n\n    @Test(timeout = 2000000)\n    public void streamMetadataTest() throws Exception {\n        // Create test scope. This operation should succeed.\n        assertTrue(controller.createScope(SCOPE).join());\n\n        // Delete the test scope. This operation should also succeed.\n        assertTrue(controller.deleteScope(SCOPE).join());\n\n        // Try creating a stream. It should fail, since the scope does not exist.\n        assertFalse(FutureHelpers.await(controller.createStream(streamConfiguration)));\n\n        // Again create the scope.\n        assertTrue(controller.createScope(SCOPE).join());\n\n        // Try creating the stream again. It should succeed now, since the scope exists.\n        assertTrue(controller.createStream(streamConfiguration).join());\n\n        // Delete test scope. This operation should fail, since it is not empty.\n        assertFalse(FutureHelpers.await(controller.deleteScope(SCOPE)));\n\n        // Try creating already existing scope.\n        assertFalse(controller.createScope(SCOPE).join());\n\n        // Try creating already existing stream.\n        assertFalse(controller.createStream(streamConfiguration).join());\n\n        // Delete test stream. This operation should fail, since it is not yet SEALED.\n        assertFalse(FutureHelpers.await(controller.deleteStream(SCOPE, STREAM)));\n\n        // Seal the test stream. This operation should succeed.\n        assertTrue(controller.sealStream(SCOPE, STREAM).join());\n\n        // Delete test stream. This operation should succeed.\n        assertTrue(controller.deleteStream(SCOPE, STREAM).join());\n\n        // Delete test stream again. Now it should fail.\n        assertFalse(controller.deleteStream(SCOPE, STREAM).join());\n\n        // Delete test scope. This operation sholud succeed.\n        assertTrue(controller.deleteScope(SCOPE).join());\n\n        // Delete a non-existent scope.\n        assertFalse(controller.deleteScope(\"non_existent_scope\").join());\n\n        // Create a scope with invalid characters. It should fail.\n        assertFalse(FutureHelpers.await(controller.createScope(\"abc/def\")));\n\n        // Try creating stream with invalid characters. It should fail.\n        assertFalse(FutureHelpers.await(controller.createStream(StreamConfiguration.builder()\n                                                                                   .scope(SCOPE)\n                                                                                   .streamName(\"abc/def\")\n                                                                                   .scalingPolicy(ScalingPolicy.fixed(1))\n                                                                                   .build())));\n    }\n\n    @Test(timeout = 10000)\n    public void streamManagerImpltest() {\n        StreamManager streamManager = new StreamManagerImpl(controller);\n\n        // Create and delete scope\n        assertTrue(streamManager.createScope(SCOPE));\n        assertTrue(streamManager.deleteScope(SCOPE));\n\n        // Create scope twice\n        assertTrue(streamManager.createScope(SCOPE));\n        assertFalse(streamManager.createScope(SCOPE));\n        assertTrue(streamManager.deleteScope(SCOPE));\n\n        // Delete twice\n        assertFalse(streamManager.deleteScope(SCOPE));\n    }\n}\n"
    }
  ]
}