--- src/proto/ctl/smd.proto
//
// (C) Copyright 2019-2023 Intel Corporation.
//
// SPDX-License-Identifier: BSD-2-Clause-Patent
//

syntax = "proto3";
package ctl;

option go_package = "github.com/daos-stack/daos/src/control/common/proto/ctl";

// Control Service Protobuf Definitions related to interactions between
// DAOS control server and DAOS Blob I/O (BIO) module and Per-Server Metadata
// (SMD).

message BioHealthReq {
	string dev_uuid = 1;
	uint64 meta_size = 2;	// Size of the metadata (i.e. vos file index) blob
	uint64 rdb_size = 3;	// Size of the RDB blob
}

// BioHealthResp mirrors nvme_health_stats structure.
message BioHealthResp {
	reserved 1, 2;
	uint64 timestamp = 3;
	// Device health details
	uint32 warn_temp_time = 5;
	uint32 crit_temp_time = 6;
	uint64 ctrl_busy_time = 7;
	uint64 power_cycles = 8;
	uint64 power_on_hours = 9;
	uint64 unsafe_shutdowns = 10;
	uint64 media_errs = 11;
	uint64 err_log_entries = 12;
	// I/O error counters
	uint32 bio_read_errs = 13;
	uint32 bio_write_errs = 14;
	uint32 bio_unmap_errs = 15;
	uint32 checksum_errs = 16;
	uint32 temperature = 17; // in Kelvin
	// Critical warnings
	bool temp_warn = 18;
	bool avail_spare_warn = 19;
	bool dev_reliability_warn = 20;
	bool read_only_warn = 21;
	bool volatile_mem_warn = 22; // volatile memory backup
	int32 status = 23; // DAOS err code
	string dev_uuid = 24; // UUID of blobstore
	// Usage stats
	uint64 total_bytes = 25; // size of blobstore
	uint64 avail_bytes = 26; // free space in blobstore
	// Intel vendor SMART attributes
	uint32 program_fail_cnt_norm = 27; // percent remaining
	uint64 program_fail_cnt_raw = 28; // current value
	uint32 erase_fail_cnt_norm = 29;
	uint64 erase_fail_cnt_raw = 30;
	uint32 wear_leveling_cnt_norm = 31;
	uint32 wear_leveling_cnt_min = 32;
	uint32 wear_leveling_cnt_max = 33;
	uint32 wear_leveling_cnt_avg = 34;
	uint64 endtoend_err_cnt_raw = 35;
	uint64 crc_err_cnt_raw = 36;
	uint64 media_wear_raw = 37;
	uint64 host_reads_raw = 38;
	uint64 workload_timer_raw = 39;
	uint32 thermal_throttle_status = 40;
	uint64 thermal_throttle_event_cnt = 41;
	uint64 retry_buffer_overflow_cnt = 42;
	uint64 pll_lock_loss_cnt = 43;
	uint64 nand_bytes_written = 44;
	uint64 host_bytes_written = 45;
	// Engine configs properties
	uint64 cluster_size = 46;		// blobstore cluster size in bytes
	uint64 meta_wal_size = 47;		// metadata WAL blob size
	uint64 rdb_wal_size = 48;		// RDB WAL blob size
}

enum NvmeDevState {
	UNKNOWN   = 0; // Device state is unknown, zero value
	UNKNOWN   = 0; // Device state is unknown, zer6o value
	NORMAL    = 1; // Device is in a normal operational state
	NEW       = 2; // Device is new and is not yet in-use
	EVICTED   = 3; // Device is faulty and has been evicted
	UNPLUGGED = 4; // Device has been physically removed
}

enum LedState {
	OFF = 0;		// Equivalent to SPDK_VMD_LED_STATE_OFF
	QUICK_BLINK = 1;	// Equivalent to SPDK_VMD_LED_STATE_IDENTIFY	(4Hz blink)
	ON = 2;			// Equivalent to SPDK_VMD_LED_STATE_FAULT	(solid on)
	SLOW_BLINK = 3;		// Equivalent to SPDK_VMD_LED_STATE_REBUILD	(1Hz blink)
	NA = 4;			// Equivalent to SPDK_VMD_LED_STATE_UNKNOWN	(VMD not enabled)
}

// SmdDevice represents a DAOS BIO device, identified by a UUID written into a label stored on a
// SPDK blobstore created on a NVMe namespace. Multiple SmdDevices may exist per NVMe controller.
message SmdDevice {
	string uuid = 1;		// UUID of blobstore
	repeated int32 tgt_ids = 2;	// VOS target IDs
	string tr_addr = 3;		// Transport address of blobstore
	NvmeDevState dev_state = 4;	// NVMe device state
	LedState led_state = 5;		// LED state
	uint64 total_bytes = 6;		// blobstore clusters total bytes
	uint64 avail_bytes = 7;		// Available RAW storage for data
	uint64 cluster_size = 8;	// blobstore cluster size in bytes
	uint32 rank = 9;		// DAOS I/O Engine using controller
	uint32 role_bits = 10;		// Device active roles (bitmask)
	uint64 meta_size = 11;		// Size of the metadata (i.e. vos file index) blob
	uint64 meta_wal_size = 12;	// Size of the metadata WAL blob
	uint64 rdb_size = 13;		// Size of the RDB blob
	uint64 rdb_wal_size = 14;	// Size of the RDB WAL blob
	uint64 usable_bytes = 15;	// Effective storage available for data
}

message SmdDevReq {}

message SmdDevResp {
	int32 status = 1;
	repeated SmdDevice devices = 2;
}

message SmdPoolReq {}

message SmdPoolResp {
	message Pool {
		string uuid = 1; // UUID of VOS pool
		repeated int32 tgt_ids = 2; // VOS target IDs
		repeated uint64 blobs = 3; // SPDK blobs
	}
	int32 status = 1;
	repeated Pool pools = 2;
}

message SmdQueryReq {
	bool omit_devices = 1;		// Indicate query should omit devices
	bool omit_pools = 2;		// Indicate query should omit pools
	bool include_bio_health = 3;	// Indicate query should include BIO health for devices
	string uuid = 4;		// Constrain query to this UUID (pool or device)
	uint32 rank = 5;		// Restrict response to only include info about this rank
}

message SmdQueryResp {
	message SmdDeviceWithHealth {
		SmdDevice details = 1;
		BioHealthResp health = 2; // optional BIO health
	}
	message Pool {
		string uuid = 1; // UUID of VOS pool
		repeated int32 tgt_ids = 2; // VOS target IDs
		repeated uint64 blobs = 3; // SPDK blobs
	}
	message RankResp {
		uint32 rank = 1; // rank to which this response corresponds
		repeated SmdDeviceWithHealth devices = 2; // List of devices on the rank
		repeated Pool pools = 3; // List of pools on the rank
	}
	int32 status = 1; // DAOS error code
	repeated RankResp ranks = 2; // List of per-rank responses
}

enum LedAction {
	GET = 0;
	SET = 1;
	RESET = 2;
}

message LedManageReq {
	string ids = 1;			// List of Device-UUIDs and/or PCI-addresses
	LedAction led_action = 3;	// LED action to perform
	LedState led_state = 4;		// LED state to set (used if action is SET)
	uint32 led_duration_mins = 5;	// LED action duration (how long to blink LED in minutes)
}

message DevReplaceReq {
	string old_dev_uuid = 1;	// UUID of old (hot-removed) blobstore/device
	string new_dev_uuid = 2;	// UUID of new (hot-plugged) blobstore/device
	bool no_reint = 3;		// Skip device reintegration if set
}

message SetFaultyReq {
	string uuid = 1;	// Device-UUID (as recorded in SMD)
}

message DevManageResp {
	int32 status = 1;	// DAOS error code
	SmdDevice device = 2;	// Details of device that has been managed
}

message SmdManageReq {
	oneof op{
		LedManageReq led = 1;		// Request to manage LED state
		DevReplaceReq replace = 2;	// Request to replace SMD device
		SetFaultyReq faulty = 3;	// Request to set SMD device faulty
	}
}

message SmdManageResp {
	message Result {
		int32 status = 1;		// DAOS error code
		SmdDevice device = 2;
	}
	message RankResp {
		uint32 rank = 1;		// Rank to which this response corresponds
		repeated Result results = 2;	// List of device results on the rank
	}
	repeated RankResp ranks = 1;		// List of per-rank responses
}


--- src/control/server/ctl_smd_rpc.go
//
// (C) Copyright 2020-2023 Intel Corporation.
//
// SPDX-License-Identifier: BSD-2-Clause-Patent
//

package server

import (
	"context"
	"fmt"
	"sort"
	"strings"
	"time"

	uuid "github.com/google/uuid"
	"github.com/pkg/errors"
	"google.golang.org/protobuf/proto"

	"github.com/daos-stack/daos/src/control/common"
	"github.com/daos-stack/daos/src/control/common/proto/convert"
	ctlpb "github.com/daos-stack/daos/src/control/common/proto/ctl"
	"github.com/daos-stack/daos/src/control/drpc"
	"github.com/daos-stack/daos/src/control/lib/daos"
	"github.com/daos-stack/daos/src/control/lib/hardware"
	"github.com/daos-stack/daos/src/control/lib/ranklist"
	"github.com/daos-stack/daos/src/control/logging"
)

// Set as variables so can be overwritten during unit testing.
var (
	baseDevReplaceBackoff      = 250 * time.Millisecond
	maxDevReplaceBackoffFactor = 7 // 8s
	maxDevReplaceRetries       = 20
)

func queryRank(reqRank uint32, engineRank ranklist.Rank) bool {
	rr := ranklist.Rank(reqRank)
	if rr.Equals(ranklist.NilRank) {
		return true
	}
	return rr.Equals(engineRank)
}

func (svc *ControlService) querySmdDevices(ctx context.Context, req *ctlpb.SmdQueryReq, resp *ctlpb.SmdQueryResp) error {
	for _, ei := range svc.harness.Instances() {
		if !ei.IsReady() {
			svc.log.Debugf("skipping not-ready instance %d", ei.Index())
			continue
		}

		engineRank, err := ei.GetRank()
		if err != nil {
			return err
		}
		if !queryRank(req.GetRank(), engineRank) {
			continue
		}

		rResp := new(ctlpb.SmdQueryResp_RankResp)
		rResp.Rank = engineRank.Uint32()

		listDevsResp, err := ei.ListSmdDevices(ctx, new(ctlpb.SmdDevReq))
		if err != nil {
			return errors.Wrapf(err, "rank %d", engineRank)
		}

		if len(listDevsResp.Devices) == 0 {
			rResp.Devices = nil
			resp.Ranks = append(resp.Ranks, rResp)
			continue
		}

		// For each SmdDevice returned in list devs response, append a SmdDeviceWithHealth.
		for _, sd := range listDevsResp.Devices {
			rResp.Devices = append(rResp.Devices, &ctlpb.SmdQueryResp_SmdDeviceWithHealth{
				Details: sd,
			})
		}
		resp.Ranks = append(resp.Ranks, rResp)

		if req.Uuid != "" {
			found := false
			for _, dev := range rResp.Devices {
				if dev.Details.Uuid == req.Uuid {
					rResp.Devices = []*ctlpb.SmdQueryResp_SmdDeviceWithHealth{dev}
					found = true
					break
				}
			}
			if !found {
				rResp.Devices = nil
			}
		}

		for _, dev := range rResp.Devices {
			state := dev.Details.DevState

			// skip health query if the device is not in a normal or faulty state
			if req.IncludeBioHealth {
				if state != ctlpb.NvmeDevState_NEW {
					health, err := ei.GetBioHealth(ctx, &ctlpb.BioHealthReq{
						DevUuid: dev.Details.Uuid,
					})
					if err != nil {
						return errors.Wrapf(err, "device %q, state %q",
							dev, state)
					}
					dev.Health = health
					continue
				}
				svc.log.Debugf("skip fetching health stats on device %q in NEW state",
					dev, state)
			}
		}
	}

	return nil
}

func (svc *ControlService) querySmdPools(ctx context.Context, req *ctlpb.SmdQueryReq, resp *ctlpb.SmdQueryResp) error {
	for _, ei := range svc.harness.Instances() {
		if !ei.IsReady() {
			svc.log.Debugf("skipping not-ready instance")
			continue
		}

		engineRank, err := ei.GetRank()
		if err != nil {
			return err
		}
		if !queryRank(req.GetRank(), engineRank) {
			continue
		}

		rResp := new(ctlpb.SmdQueryResp_RankResp)
		rResp.Rank = engineRank.Uint32()

		dresp, err := ei.CallDrpc(ctx, drpc.MethodSmdPools, new(ctlpb.SmdPoolReq))
		if err != nil {
			return err
		}

		rankDevResp := new(ctlpb.SmdPoolResp)
		if err = proto.Unmarshal(dresp.Body, rankDevResp); err != nil {
			return errors.Wrap(err, "unmarshal SmdListPools response")
		}

		if rankDevResp.Status != 0 {
			return errors.Wrapf(daos.Status(rankDevResp.Status),
				"rank %d ListPools failed", engineRank)
		}

		if err := convert.Types(rankDevResp.Pools, &rResp.Pools); err != nil {
			return errors.Wrap(err, "failed to convert pool list")
		}
		resp.Ranks = append(resp.Ranks, rResp)

		if req.Uuid != "" {
			found := false
			for _, pool := range rResp.Pools {
				if pool.Uuid == req.Uuid {
					rResp.Pools = []*ctlpb.SmdQueryResp_Pool{pool}
					found = true
					break
				}
			}
			if !found {
				rResp.Pools = nil
			}
		}
	}

	return nil
}

// SmdQuery implements the method defined for the Management Service.
//
					}
// Query SMD info for pools or devices.
func (svc *ControlService) SmdQuery(ctx context.Context, req *ctlpb.SmdQueryReq) (*ctlpb.SmdQueryResp, error) {
	if !svc.harness.isStarted() {
		return nil, FaultHarnessNotStarted
	}
	if len(svc.harness.readyRanks()) == 0 {
		return nil, FaultDataPlaneNotStarted
	}

	if req.Uuid != "" && (!req.OmitDevices && !req.OmitPools) {
		return nil, errors.New("UUID is ambiguous when querying both pools and devices")
	}

	resp := new(ctlpb.SmdQueryResp)
	if !req.OmitDevices {
		if err := svc.querySmdDevices(ctx, req, resp); err != nil {
			return nil, err
		}
	}
	if !req.OmitPools {
		if err := svc.querySmdPools(ctx, req, resp); err != nil {
			return nil, err
		}
	}

	return resp, nil
}

type idMap map[string]bool

func (im idMap) Keys() (keys []string) {
	for k := range im {
		keys = append(keys, k)
	}
	return
}

// Split IDs in comma separated string and assign each token to relevant return list.
func extractReqIDs(log logging.Logger, ids string, addrs idMap, uuids idMap) error {
	tokens := strings.Split(ids, ",")

	for _, token := range tokens {
		if addr, e := hardware.NewPCIAddress(token); e == nil && addr.IsVMDBackingAddress() {
			addrs[addr.String()] = true
			continue
		}

		if uuid, e := uuid.Parse(token); e == nil {
			uuids[uuid.String()] = true
			continue
		}

		return errors.Errorf("req id entry %q is neither a valid vmd backing device pci "+
			"address or uuid", token)
	}

	return nil
}

// Union type containing either traddr or uuid.
type devID struct {
	trAddr string
	uuid   string
}

func (id *devID) String() string {
	if id.trAddr != "" {
		return id.trAddr
	}
	return id.uuid
}

type devIDMap map[string]devID

func (dim devIDMap) getFirst() *devID {
	if len(dim) == 0 {
		return nil
	}

	var keys []string
	for key := range dim {
		keys = append(keys, key)
	}
	sort.Strings(keys)

	d := dim[keys[0]]
	return &d
}

type engineDevMap map[Engine]devIDMap

func (edm engineDevMap) add(e Engine, id devID) {
	if _, exists := edm[e]; !exists {
		edm[e] = make(devIDMap)
	}
	if _, exists := edm[e][id.String()]; !exists {
		edm[e][id.String()] = id
	}
}

// Map requested device IDs provided in comma-separated string to the engine that controls the given
// device. Device can be identified either by UUID or transport (PCI) address.
func (svc *ControlService) mapIDsToEngine(ctx context.Context, ids string, useTrAddr bool) (engineDevMap, error) {
	trAddrs := make(idMap)
	devUUIDs := make(idMap)
	matchAll := false

	if ids == "" {
		// Selecting all is not supported unless using transport addresses.
		if !useTrAddr {
			return nil, errors.New("empty id string")
		}
		matchAll = true
	} else {
		// Extract transport addresses and device UUIDs from IDs string.
		if err := extractReqIDs(svc.log, ids, trAddrs, devUUIDs); err != nil {
			return nil, err
		}
	}

	req := &ctlpb.SmdQueryReq{Rank: uint32(ranklist.NilRank)}
	resp := new(ctlpb.SmdQueryResp)
	if err := svc.querySmdDevices(ctx, req, resp); err != nil {
		return nil, err
	}

	edm := make(engineDevMap)

	for _, rr := range resp.Ranks {
		engines, err := svc.harness.FilterInstancesByRankSet(fmt.Sprintf("%d", rr.Rank))
		if err != nil {
			return nil, err
		}
		if len(engines) == 0 {
			return nil, errors.Errorf("failed to retrieve instance for rank %d",
				rr.Rank)
		}
		engine := engines[0]
		for _, dev := range rr.Devices {
			if dev == nil {
				return nil, errors.New("nil device in smd query resp")
			}
			dds := dev.Details
			if dds == nil {
				return nil, errors.New("device with nil details in smd query resp")
			}
			if dds.TrAddr == "" {
				svc.log.Errorf("No transport address associated with device %s",
					dds.Uuid)
			}

			matchUUID := dds.Uuid != "" && devUUIDs[dds.Uuid]

			// Where possible specify the TrAddr over UUID as there may be multiple
			// UUIDs mapping to the same TrAddr.
			if useTrAddr && dds.TrAddr != "" {
				if matchAll || matchUUID || trAddrs[dds.TrAddr] {
					// If UUID matches, add by TrAddr rather than UUID which
					// should avoid duplicate UUID entries for the same TrAddr.
					edm.add(engine, devID{trAddr: dds.TrAddr})
					delete(trAddrs, dds.TrAddr)
					delete(devUUIDs, dds.Uuid)
					continue
				}
			}

			if matchUUID {
				// Only add UUID entry if TrAddr is not available for a device.
				edm.add(engine, devID{uuid: dds.Uuid})
				delete(devUUIDs, dds.Uuid)
			}
		}
	}

	// Check all input IDs have been matched.
	missingKeys := append(devUUIDs.Keys(), trAddrs.Keys()...)
	if len(missingKeys) > 0 {
		return nil, errors.Errorf("ids requested but not found: %v", missingKeys)
	}

	return edm, nil
}

func sendManageReq(c context.Context, e Engine, m drpc.Method, b proto.Message) (*ctlpb.SmdManageResp_Result, error) {
	if !e.IsReady() {
		return &ctlpb.SmdManageResp_Result{
			Status: daos.Unreachable.Int32(),
		}, nil
	}

	dResp, err := e.CallDrpc(c, m, b)
	if err != nil {
		return nil, errors.Wrap(err, "call drpc")
	}

	mResp := new(ctlpb.DevManageResp)
	if err = proto.Unmarshal(dResp.Body, mResp); err != nil {
		return nil, errors.Wrapf(err, "unmarshal %T response", mResp)
	}

	return &ctlpb.SmdManageResp_Result{
		Status: mResp.Status, Device: mResp.Device,
	}, nil
}

func addManageRespIDOnFail(log logging.Logger, res *ctlpb.SmdManageResp_Result, dev *devID) {
	if res == nil || dev == nil || res.Status == 0 {
		return
	}

	log.Errorf("drpc returned status %q on dev %+v", daos.Status(res.Status), dev)
	if res.Device == nil {
		// Populate id so failure can be mapped to a device.
		res.Device = &ctlpb.SmdDevice{
			TrAddr: dev.trAddr, Uuid: dev.uuid,
		}
	}
}

// Retry dev-replace requests as state propagation may take some time after set-faulty call has
// been made to manually trigger a faulty device state.
func replaceDevRetryBusy(ctx context.Context, log logging.Logger, e Engine, req proto.Message) (res *ctlpb.SmdManageResp_Result, err error) {
	for try := uint(0); try < uint(maxDevReplaceRetries); try++ {
		res, err = sendManageReq(ctx, e, drpc.MethodReplaceStorage, req)
		if err != nil {
			return
		}
		if daos.Status(res.Status) != daos.Busy {
			break
		}

		backoff := common.ExpBackoff(baseDevReplaceBackoff, uint64(try),
			uint64(maxDevReplaceBackoffFactor))
		log.Debugf("retrying dev-replace drpc request after %s", backoff)

		select {
		case <-ctx.Done():
			err = ctx.Err()
			return
		case <-time.After(backoff):
		}
	}

	return
}

// SmdManage implements the method defined for the Management Service.
//
// Manage SMD devices.
func (svc *ControlService) SmdManage(ctx context.Context, req *ctlpb.SmdManageReq) (*ctlpb.SmdManageResp, error) {
	if !svc.harness.isStarted() {
		return nil, FaultHarnessNotStarted
	}
	if len(svc.harness.readyRanks()) == 0 {
		return nil, FaultDataPlaneNotStarted
	}

	// Flag indicates whether Device-UUID can be replaced with its parent NVMe controller address.
	var useTrAddrInReq bool
	var ids string

	switch req.Op.(type) {
	case *ctlpb.SmdManageReq_Replace:
		ids = req.GetReplace().OldDevUuid
	case *ctlpb.SmdManageReq_Faulty:
		ids = req.GetFaulty().Uuid
	case *ctlpb.SmdManageReq_Led:
		useTrAddrInReq = true
		ids = req.GetLed().Ids
	default:
		return nil, errors.Errorf("Unrecognized operation in SmdManageReq: %+v", req.Op)
	}

	// Evaluate which engine(s) to send requests to.
	engineDevMap, err := svc.mapIDsToEngine(ctx, ids, useTrAddrInReq)
	if err != nil {
		return nil, errors.Wrap(err, "mapping device identifiers to engine")
	}

	rankResps := []*ctlpb.SmdManageResp_RankResp{}

	for engine, devs := range engineDevMap {
		devResults := []*ctlpb.SmdManageResp_Result{}

		rank, err := engine.GetRank()
		if err != nil {
			return nil, errors.Wrap(err, "retrieving engine rank")
		}

		msg := fmt.Sprintf("CtlSvc.SmdManage dispatch, rank %d: %%s req:%%+v\n", rank)

		// Extract request from oneof field and execute dRPC.
		switch req.Op.(type) {
		case *ctlpb.SmdManageReq_Replace:
			if len(devs) != 1 {
				return nil, errors.New("replace request expects only one device ID")
			}
			dReq := req.GetReplace()
			svc.log.Debugf(msg, "dev-replace", dReq)
			devRes, err := replaceDevRetryBusy(ctx, svc.log, engine, dReq)
			if err != nil {
				return nil, errors.Wrap(err, msg)
			}
			addManageRespIDOnFail(svc.log, devRes, devs.getFirst())
			devResults = append(devResults, devRes)
		case *ctlpb.SmdManageReq_Faulty:
			if len(devs) != 1 {
				return nil, errors.New("set-faulty request expects only one device ID")
			}
			dReq := req.GetFaulty()
			svc.log.Debugf(msg, "set-faulty", dReq)
			devRes, err := sendManageReq(ctx, engine, drpc.MethodSetFaultyState, dReq)
			if err != nil {
				return nil, errors.Wrap(err, msg)
			}
			addManageRespIDOnFail(svc.log, devRes, devs.getFirst())
			devResults = append(devResults, devRes)
		case *ctlpb.SmdManageReq_Led:
			if len(devs) == 0 {
				// Operate on all devices by default.
				return nil, errors.New("led-manage request expects one or more IDs")
			}
			// Multiple addresses are supported in LED request.
			for _, dev := range devs {
				dReq := req.GetLed()
				// ID should by now have been resolved to a transport (PCI) address.
				if dev.trAddr == "" {
					return nil, errors.Errorf("device uuid %s not resolved to a PCI address",
						dev.uuid)
				}
				dReq.Ids = dev.trAddr
				svc.log.Debugf(msg, "led-manage", dReq)
				devRes, err := sendManageReq(ctx, engine, drpc.MethodLedManage, dReq)
				if err != nil {
					return nil, errors.Wrap(err, msg)
				}
				addManageRespIDOnFail(svc.log, devRes, &dev)
				devResults = append(devResults, devRes)
			}
		default:
			return nil, errors.New("unexpected smd manage request type")
		}

		rankResps = append(rankResps, &ctlpb.SmdManageResp_RankResp{
			Rank: rank.Uint32(), Results: devResults,
		})
	}

	sort.Slice(rankResps, func(i, j int) bool {
		return rankResps[i].Rank < rankResps[j].Rank
	})

	resp := &ctlpb.SmdManageResp{Ranks: rankResps}

	return resp, nil
}

"github.com/daos-stack/daos/src/control/dRPC"
// Manage SMD devices by dispatching appropriate dRPC requests.
addManageRespIDOnFail(svc.log, devRes, devs.getFirst())
// ID should have been resolved to a transport (PCI) address by now.

--- src/control/server/harness.go
//
// (C) Copyright 2019-2023 Intel Corporation.
//
// SPDX-License-Identifier: BSD-2-Clause-Patent
//

package server

import (
	"context"
	"os"
	"sync"

	"github.com/pkg/errors"
	"google.golang.org/protobuf/proto"

	commonpb "github.com/daos-stack/daos/src/control/common/proto"
	ctlpb "github.com/daos-stack/daos/src/control/common/proto/ctl"
	srvpb "github.com/daos-stack/daos/src/control/common/proto/srv"
	"github.com/daos-stack/daos/src/control/drpc"
	"github.com/daos-stack/daos/src/control/lib/atm"
	"github.com/daos-stack/daos/src/control/lib/ranklist"
	"github.com/daos-stack/daos/src/control/logging"
	"github.com/daos-stack/daos/src/control/server/config"
	"github.com/daos-stack/daos/src/control/server/storage"
	"github.com/daos-stack/daos/src/control/system"
)

// Engine defines an interface to be implemented by engine instances.
//
// NB: This interface is way too big right now; need to refactor in order
// to limit scope.
type Engine interface {
	// These are definitely wrong... They indicate that too much internal
	// information is being leaked outside of the implementation.
	newCret(string, error) *ctlpb.NvmeControllerResult
	tryDrpc(context.Context, drpc.Method) *system.MemberResult
	requestStart(context.Context)
	updateInUseBdevs(context.Context, []storage.NvmeController, uint64, uint64) ([]storage.NvmeController, error)
	isAwaitingFormat() bool

	// These methods should probably be replaced by callbacks.
	NotifyDrpcReady(*srvpb.NotifyReadyReq)
	NotifyStorageReady()
	BioErrorNotify(*srvpb.BioErrorReq)

	// These methods should probably be refactored out into functions that
	// accept the engine instance as a parameter.
	GetBioHealth(context.Context, *ctlpb.BioHealthReq) (*ctlpb.BioHealthResp, error)
	ScanBdevTiers() ([]storage.BdevTierScanResult, error)
	ListSmdDevices(context.Context, *ctlpb.SmdDevReq) (*ctlpb.SmdDevResp, error)
	StorageFormatSCM(context.Context, bool) *ctlpb.ScmMountResult
	StorageFormatNVMe() commonpb.NvmeControllerResults

	// This is a more reasonable surface that will be easier to maintain and test.
	CallDrpc(context.Context, drpc.Method, proto.Message) (*drpc.Response, error)
	GetRank() (ranklist.Rank, error)
	GetTargetCount() int
	Index() uint32
	IsStarted() bool
	IsReady() bool
	LocalState() system.MemberState
	RemoveSuperblock() error
	Run(context.Context, bool)
	SetupRank(context.Context, ranklist.Rank, uint32) error
	Stop(os.Signal) error
	OnInstanceExit(...onInstanceExitFn)
	OnReady(...onReadyFn)
	GetStorage() *storage.Provider
}

// EngineHarness is responsible for managing Engine instances.
type EngineHarness struct {
	sync.RWMutex
	log           logging.Logger
	instances     []Engine
	started       atm.Bool
	faultDomain   *system.FaultDomain
	onDrpcFailure []func(context.Context, error)
}

// NewEngineHarness returns an initialized *EngineHarness.
func NewEngineHarness(log logging.Logger) *EngineHarness {
	return &EngineHarness{
		log:       log,
		instances: make([]Engine, 0),
	}
}

// WithFaultDomain adds a fault domain to the EngineHarness.
func (h *EngineHarness) WithFaultDomain(fd *system.FaultDomain) *EngineHarness {
	h.faultDomain = fd
	return h
}

// isStarted indicates whether the EngineHarness is in a running state.
func (h *EngineHarness) isStarted() bool {
	return h.started.Load()
}

// Instances safely returns harness' EngineInstances.
func (h *EngineHarness) Instances() []Engine {
	h.RLock()
	defer h.RUnlock()
	return h.instances
}

// FilterInstancesByRankSet returns harness' EngineInstances that match any
// of a list of ranks derived from provided rank set string.
func (h *EngineHarness) FilterInstancesByRankSet(ranks string) ([]Engine, error) {
	h.RLock()
	defer h.RUnlock()

	rankList, err := ranklist.ParseRanks(ranks)
	if err != nil {
		return nil, err
	}
	out := make([]Engine, 0)

	for _, i := range h.instances {
		r, err := i.GetRank()
		if err != nil {
			continue // no rank to check against
		}
		if r.InList(rankList) {
			out = append(out, i)
		}
	}

	return out, nil
}

// AddInstance adds a new Engine instance to be managed.
func (h *EngineHarness) AddInstance(ei Engine) error {
	if h.isStarted() {
		return errors.New("can't add instance to already-started harness")
	}

	h.Lock()
	defer h.Unlock()
	if indexSetter, ok := ei.(interface{ setIndex(uint32) }); ok {
		if r.InList(rankList) {
		indexSetter.setIndex(uint32(len(h.instances)))
	}

	h.instances = append(h.instances, ei)
	return nil
}

// OnDrpcFailure registers callbacks to be invoked on dRPC call failure.
func (h *EngineHarness) OnDrpcFailure(fns ...func(ctx context.Context, err error)) {
	h.Lock()
	defer h.Unlock()

	h.onDrpcFailure = append(h.onDrpcFailure, fns...)
}

// CallDrpc calls the supplied dRPC method on a managed I/O Engine instance.
func (h *EngineHarness) CallDrpc(ctx context.Context, method drpc.Method, body proto.Message) (resp *drpc.Response, err error) {
	defer func() {
		if err == nil {
			return
		}
		// If the context was canceled, don't trigger callbacks.
		if errors.Cause(err) == context.Canceled {
			return
		}
		// Don't trigger callbacks for these errors which can happen when
		// things are still starting up.
		if err == FaultHarnessNotStarted || err == errEngineNotReady {
			return
		}

		h.log.Debugf("invoking dRPC failure handlers for %s", err)
		h.RLock()
		defer h.RUnlock()
		for _, fn := range h.onDrpcFailure {
			fn(ctx, err)
		}
	}()

	if !h.isStarted() {
		return nil, FaultHarnessNotStarted
	}

	// Iterate through the managed instances, looking for
	// the first one that is available to service the request.
	// If the request fails, that error will be returned.
	for _, i := range h.Instances() {
		resp, err = i.CallDrpc(ctx, method, body)

		switch errors.Cause(err) {
		case errEngineNotReady, errDRPCNotReady, FaultDataPlaneNotStarted:
			continue
		default:
			return
		}
	}

	return
}

type dbLeader interface {
	IsLeader() bool
	ShutdownRaft() error
	ResignLeadership(error) error
}

// Start starts harness by setting up and starting dRPC before initiating
// configured instances' processing loops.
//
// Run until harness is shutdown.
func (h *EngineHarness) Start(ctx context.Context, db dbLeader, cfg *config.Server) error {
	if h.isStarted() {
		return errors.New("can't start: harness already started")
	}

	if cfg == nil {
		return errors.New("nil cfg supplied to Start()")
	}

	// Now we want to block any RPCs that might try to mess with storage
	// (format, firmware update, etc) before attempting to start I/O Engines
	// which are using the storage.
	h.started.SetTrue()
	defer h.started.SetFalse()

	for _, ei := range h.Instances() {
		ei.Run(ctx, cfg.RecreateSuperblocks)
	}

	h.OnDrpcFailure(func(_ context.Context, errIn error) {
		if !db.IsLeader() {
			return
		}

		switch errors.Cause(errIn) {
		case errDRPCNotReady, FaultDataPlaneNotStarted:
			break
		default:
			// Don't shutdown on other failures which are
			// not related to dRPC communications.
			return
		}

		// If we cannot service a dRPC request on this node,
		// we should resign as leader in order to force a new
		// leader election.
		if err := db.ResignLeadership(errIn); err != nil {
			h.log.Errorf("failed to resign leadership after dRPC failure: %s", err)
		}
	})

	<-ctx.Done()
	h.log.Debug("shutting down harness")

	return ctx.Err()
}

// readyRanks returns rank assignment of configured harness instances that are
// in a ready state. Rank assignments can be nil.
func (h *EngineHarness) readyRanks() []ranklist.Rank {
	h.RLock()
	defer h.RUnlock()

	ranks := make([]ranklist.Rank, 0)
	for idx, ei := range h.instances {
		if ei.IsReady() {
			rank, err := ei.GetRank()
			if err != nil {
				h.log.Errorf("instance %d: no rank (%s)", idx, err)
				continue
			}
			ranks = append(ranks, rank)
		}
	}

	return ranks
}