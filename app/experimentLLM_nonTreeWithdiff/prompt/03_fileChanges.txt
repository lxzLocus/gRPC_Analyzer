{
  "changed_files": [
    {
      "path": "src/bio/smd.pb-c.c",
      "content": Omitted as it is not in context
    },
    {
      "path": "src/control/common/proto/ctl/smd.pb.go",
      "content": Omitted as it is not in context
    },
    {
      "path": "src/control/server/ctl_smd_rpc.go",
      "content": "//\n// (C) Copyright 2020-2023 Intel Corporation.\n//\n// SPDX-License-Identifier: BSD-2-Clause-Patent\n//\n\npackage server\n\nimport (\n\t\"context\"\n\t\"fmt\"\n\t\"sort\"\n\t\"strings\"\n\t\"time\"\n\n\tuuid \"github.com/google/uuid\"\n\t\"github.com/pkg/errors\"\n\t\"google.golang.org/protobuf/proto\"\n\n\t\"github.com/daos-stack/daos/src/control/common\"\n\t\"github.com/daos-stack/daos/src/control/common/proto/convert\"\n\tctlpb \"github.com/daos-stack/daos/src/control/common/proto/ctl\"\n\t\"github.com/daos-stack/daos/src/control/drpc\"\n\t\"github.com/daos-stack/daos/src/control/lib/daos\"\n\t\"github.com/daos-stack/daos/src/control/lib/hardware\"\n\t\"github.com/daos-stack/daos/src/control/lib/ranklist\"\n\t\"github.com/daos-stack/daos/src/control/logging\"\n)\n\n// Set as variables so can be overwritten during unit testing.\nvar (\n\tbaseDevReplaceBackoff      = 250 * time.Millisecond\n\tmaxDevReplaceBackoffFactor = 7 // 8s\n\tmaxDevReplaceRetries       = 20\n)\n\nfunc queryRank(reqRank uint32, engineRank ranklist.Rank) bool {\n\trr := ranklist.Rank(reqRank)\n\tif rr.Equals(ranklist.NilRank) {\n\t\treturn true\n\t}\n\treturn rr.Equals(engineRank)\n}\n\nfunc (svc *ControlService) querySmdDevices(ctx context.Context, req *ctlpb.SmdQueryReq, resp *ctlpb.SmdQueryResp) error {\n\tfor _, ei := range svc.harness.Instances() {\n\t\tif !ei.IsReady() {\n\t\t\tsvc.log.Debugf(\"skipping not-ready instance %d\", ei.Index())\n\t\t\tcontinue\n\t\t}\n\n\t\tengineRank, err := ei.GetRank()\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\tif !queryRank(req.GetRank(), engineRank) {\n\t\t\tcontinue\n\t\t}\n\n\t\trResp := new(ctlpb.SmdQueryResp_RankResp)\n\t\trResp.Rank = engineRank.Uint32()\n\n\t\tlistDevsResp, err := ei.ListSmdDevices(ctx, new(ctlpb.SmdDevReq))\n\t\tif err != nil {\n\t\t\treturn errors.Wrapf(err, \"rank %d\", engineRank)\n\t\t}\n\n\t\tif len(listDevsResp.Devices) == 0 {\n\t\t\trResp.Devices = nil\n\t\t\tresp.Ranks = append(resp.Ranks, rResp)\n\t\t\tcontinue\n\t\t}\n\n\t\t// For each SmdDevice returned in list devs response, append a SmdDeviceWithHealth.\n\t\tfor _, sd := range listDevsResp.Devices {\n\t\t\trResp.Devices = append(rResp.Devices, &ctlpb.SmdQueryResp_SmdDeviceWithHealth{\n\t\t\t\tDetails: sd,\n\t\t\t})\n\t\t}\n\t\tresp.Ranks = append(resp.Ranks, rResp)\n\n\t\tif req.Uuid != \"\" {\n\t\t\tfound := false\n\t\t\tfor _, dev := range rResp.Devices {\n\t\t\t\tif dev.Details.Uuid == req.Uuid {\n\t\t\t\t\trResp.Devices = []*ctlpb.SmdQueryResp_SmdDeviceWithHealth{dev}\n\t\t\t\t\tfound = true\n\t\t\t\t\tbreak\n\t\t\t\t}\n\t\t\t}\n\t\t\tif !found {\n\t\t\t\trResp.Devices = nil\n\t\t\t}\n\t\t}\n\n\t\tfor _, dev := range rResp.Devices {\n\t\t\tstate := dev.Details.DevState\n\n\t\t\t// skip health query if the device is not in a normal or faulty state\n\t\t\tif req.IncludeBioHealth {\n\t\t\t\tif state != ctlpb.NvmeDevState_NEW {\n\t\t\t\t\thealth, err := ei.GetBioHealth(ctx, &ctlpb.BioHealthReq{\n\t\t\t\t\t\tDevUuid: dev.Details.Uuid,\n\t\t\t\t\t})\n\t\t\t\t\tif err != nil {\n\t\t\t\t\t\treturn errors.Wrapf(err, \"device %q, state %q\",\n\t\t\t\t\t\t\tdev, state)\n\t\t\t\t\t}\n\t\t\t\t\tdev.Health = health\n\t\t\t\t\tcontinue\n\t\t\t\t}\n\t\t\t\tsvc.log.Debugf(\"skip fetching health stats on device %q in NEW state\",\n\t\t\t\t\tdev, state)\n\t\t\t}\n\t\t}\n\t}\n\n\treturn nil\n}\n\nfunc (svc *ControlService) querySmdPools(ctx context.Context, req *ctlpb.SmdQueryReq, resp *ctlpb.SmdQueryResp) error {\n\tfor _, ei := range svc.harness.Instances() {\n\t\tif !ei.IsReady() {\n\t\t\tsvc.log.Debugf(\"skipping not-ready instance\")\n\t\t\tcontinue\n\t\t}\n\n\t\tengineRank, err := ei.GetRank()\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\tif !queryRank(req.GetRank(), engineRank) {\n\t\t\tcontinue\n\t\t}\n\n\t\trResp := new(ctlpb.SmdQueryResp_RankResp)\n\t\trResp.Rank = engineRank.Uint32()\n\n\t\tdresp, err := ei.CallDrpc(ctx, drpc.MethodSmdPools, new(ctlpb.SmdPoolReq))\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\n\t\trankDevResp := new(ctlpb.SmdPoolResp)\n\t\tif err = proto.Unmarshal(dresp.Body, rankDevResp); err != nil {\n\t\t\treturn errors.Wrap(err, \"unmarshal SmdListPools response\")\n\t\t}\n\n\t\tif rankDevResp.Status != 0 {\n\t\t\treturn errors.Wrapf(daos.Status(rankDevResp.Status),\n\t\t\t\t\"rank %d ListPools failed\", engineRank)\n\t\t}\n\n\t\tif err := convert.Types(rankDevResp.Pools, &rResp.Pools); err != nil {\n\t\t\treturn errors.Wrap(err, \"failed to convert pool list\")\n\t\t}\n\t\tresp.Ranks = append(resp.Ranks, rResp)\n\n\t\tif req.Uuid != \"\" {\n\t\t\tfound := false\n\t\t\tfor _, pool := range rResp.Pools {\n\t\t\t\tif pool.Uuid == req.Uuid {\n\t\t\t\t\trResp.Pools = []*ctlpb.SmdQueryResp_Pool{pool}\n\t\t\t\t\tfound = true\n\t\t\t\t\tbreak\n\t\t\t\t}\n\t\t\t}\n\t\t\tif !found {\n\t\t\t\trResp.Pools = nil\n\t\t\t}\n\t\t}\n\t}\n\n\treturn nil\n}\n\n// SmdQuery implements the method defined for the Management Service.\n//\n// Query SMD info for pools or devices.\nfunc (svc *ControlService) SmdQuery(ctx context.Context, req *ctlpb.SmdQueryReq) (*ctlpb.SmdQueryResp, error) {\n\tif !svc.harness.isStarted() {\n\t\treturn nil, FaultHarnessNotStarted\n\t}\n\tif len(svc.harness.readyRanks()) == 0 {\n\t\treturn nil, FaultDataPlaneNotStarted\n\t}\n\n\tif req.Uuid != \"\" && (!req.OmitDevices && !req.OmitPools) {\n\t\treturn nil, errors.New(\"UUID is ambiguous when querying both pools and devices\")\n\t}\n\n\tresp := new(ctlpb.SmdQueryResp)\n\tif !req.OmitDevices {\n\t\tif err := svc.querySmdDevices(ctx, req, resp); err != nil {\n\t\t\treturn nil, err\n\t\t}\n\t}\n\tif !req.OmitPools {\n\t\tif err := svc.querySmdPools(ctx, req, resp); err != nil {\n\t\t\treturn nil, err\n\t\t}\n\t}\n\n\treturn resp, nil\n}\n\ntype idMap map[string]bool\n\nfunc (im idMap) Keys() (keys []string) {\n\tfor k := range im {\n\t\tkeys = append(keys, k)\n\t}\n\treturn\n}\n\n// Split IDs in comma separated string and assign each token to relevant return list.\nfunc extractReqIDs(log logging.Logger, ids string, addrs idMap, uuids idMap) error {\n\ttokens := strings.Split(ids, \",\")\n\n\tfor _, token := range tokens {\n\t\tif addr, e := hardware.NewPCIAddress(token); e == nil && addr.IsVMDBackingAddress() {\n\t\t\taddrs[addr.String()] = true\n\t\t\tcontinue\n\t\t}\n\n\t\tif uuid, e := uuid.Parse(token); e == nil {\n\t\t\tuuids[uuid.String()] = true\n\t\t\tcontinue\n\t\t}\n\n\t\treturn errors.Errorf(\"req id entry %q is neither a valid vmd backing device pci \"+\n\t\t\t\"address or uuid\", token)\n\t}\n\n\treturn nil\n}\n\n// Union type containing either traddr or uuid.\ntype devID struct {\n\ttrAddr string\n\tuuid   string\n}\n\nfunc (id *devID) String() string {\n\tif id.trAddr != \"\" {\n\t\treturn id.trAddr\n\t}\n\treturn id.uuid\n}\n\ntype devIDMap map[string]devID\n\nfunc (dim devIDMap) getFirst() *devID {\n\tif len(dim) == 0 {\n\t\treturn nil\n\t}\n\n\tvar keys []string\n\tfor key := range dim {\n\t\tkeys = append(keys, key)\n\t}\n\tsort.Strings(keys)\n\n\td := dim[keys[0]]\n\treturn &d\n}\n\ntype engineDevMap map[Engine]devIDMap\n\nfunc (edm engineDevMap) add(e Engine, id devID) {\n\tif _, exists := edm[e]; !exists {\n\t\tedm[e] = make(devIDMap)\n\t}\n\tif _, exists := edm[e][id.String()]; !exists {\n\t\tedm[e][id.String()] = id\n\t}\n}\n\n// Map requested device IDs provided in comma-separated string to the engine that controls the given\n// device. Device can be identified either by UUID or transport (PCI) address.\nfunc (svc *ControlService) mapIDsToEngine(ctx context.Context, ids string, useTrAddr bool) (engineDevMap, error) {\n\ttrAddrs := make(idMap)\n\tdevUUIDs := make(idMap)\n\tmatchAll := false\n\n\tif ids == \"\" {\n\t\t// Selecting all is not supported unless using transport addresses.\n\t\tif !useTrAddr {\n\t\t\treturn nil, errors.New(\"empty id string\")\n\t\t}\n\t\tmatchAll = true\n\t} else {\n\t\t// Extract transport addresses and device UUIDs from IDs string.\n\t\tif err := extractReqIDs(svc.log, ids, trAddrs, devUUIDs); err != nil {\n\t\t\treturn nil, err\n\t\t}\n\t}\n\n\treq := &ctlpb.SmdQueryReq{Rank: uint32(ranklist.NilRank)}\n\tresp := new(ctlpb.SmdQueryResp)\n\tif err := svc.querySmdDevices(ctx, req, resp); err != nil {\n\t\treturn nil, err\n\t}\n\n\tedm := make(engineDevMap)\n\n\tfor _, rr := range resp.Ranks {\n\t\tengines, err := svc.harness.FilterInstancesByRankSet(fmt.Sprintf(\"%d\", rr.Rank))\n\t\tif err != nil {\n\t\t\treturn nil, err\n\t\t}\n\t\tif len(engines) == 0 {\n\t\t\treturn nil, errors.Errorf(\"failed to retrieve instance for rank %d\",\n\t\t\t\trr.Rank)\n\t\t}\n\t\tengine := engines[0]\n\t\tfor _, dev := range rr.Devices {\n\t\t\tif dev == nil {\n\t\t\t\treturn nil, errors.New(\"nil device in smd query resp\")\n\t\t\t}\n\t\t\tdds := dev.Details\n\t\t\tif dds == nil {\n\t\t\t\treturn nil, errors.New(\"device with nil details in smd query resp\")\n\t\t\t}\n\t\t\tif dds.TrAddr == \"\" {\n\t\t\t\tsvc.log.Errorf(\"No transport address associated with device %s\",\n\t\t\t\t\tdds.Uuid)\n\t\t\t}\n\n\t\t\tmatchUUID := dds.Uuid != \"\" && devUUIDs[dds.Uuid]\n\n\t\t\t// Where possible specify the TrAddr over UUID as there may be multiple\n\t\t\t// UUIDs mapping to the same TrAddr.\n\t\t\tif useTrAddr && dds.TrAddr != \"\" {\n\t\t\t\tif matchAll || matchUUID || trAddrs[dds.TrAddr] {\n\t\t\t\t\t// If UUID matches, add by TrAddr rather than UUID which\n\t\t\t\t\t// should avoid duplicate UUID entries for the same TrAddr.\n\t\t\t\t\tedm.add(engine, devID{trAddr: dds.TrAddr})\n\t\t\t\t\tdelete(trAddrs, dds.TrAddr)\n\t\t\t\t\tdelete(devUUIDs, dds.Uuid)\n\t\t\t\t\tcontinue\n\t\t\t\t}\n\t\t\t}\n\n\t\t\tif matchUUID {\n\t\t\t\t// Only add UUID entry if TrAddr is not available for a device.\n\t\t\t\tedm.add(engine, devID{uuid: dds.Uuid})\n\t\t\t\tdelete(devUUIDs, dds.Uuid)\n\t\t\t}\n\t\t}\n\t}\n\n\t// Check all input IDs have been matched.\n\tmissingKeys := append(devUUIDs.Keys(), trAddrs.Keys()...)\n\tif len(missingKeys) > 0 {\n\t\treturn nil, errors.Errorf(\"ids requested but not found: %v\", missingKeys)\n\t}\n\n\treturn edm, nil\n}\n\nfunc sendManageReq(c context.Context, e Engine, m drpc.Method, b proto.Message) (*ctlpb.SmdManageResp_Result, error) {\n\tif !e.IsReady() {\n\t\treturn &ctlpb.SmdManageResp_Result{\n\t\t\tStatus: daos.Unreachable.Int32(),\n\t\t}, nil\n\t}\n\n\tdResp, err := e.CallDrpc(c, m, b)\n\tif err != nil {\n\t\treturn nil, errors.Wrap(err, \"call drpc\")\n\t}\n\n\tmResp := new(ctlpb.DevManageResp)\n\tif err = proto.Unmarshal(dResp.Body, mResp); err != nil {\n\t\treturn nil, errors.Wrapf(err, \"unmarshal %T response\", mResp)\n\t}\n\n\treturn &ctlpb.SmdManageResp_Result{\n\t\tStatus: mResp.Status, Device: mResp.Device,\n\t}, nil\n}\n\nfunc addManageRespIDOnFail(log logging.Logger, res *ctlpb.SmdManageResp_Result, dev *devID) {\n\tif res == nil || dev == nil || res.Status == 0 {\n\t\treturn\n\t}\n\n\tlog.Errorf(\"drpc returned status %q on dev %+v\", daos.Status(res.Status), dev)\n\tif res.Device == nil {\n\t\t// Populate id so failure can be mapped to a device.\n\t\tres.Device = &ctlpb.SmdDevice{\n\t\t\tTrAddr: dev.trAddr, Uuid: dev.uuid,\n\t\t}\n\t}\n}\n\n// Retry dev-replace requests as state propagation may take some time after set-faulty call has\n// been made to manually trigger a faulty device state.\nfunc replaceDevRetryBusy(ctx context.Context, log logging.Logger, e Engine, req proto.Message) (res *ctlpb.SmdManageResp_Result, err error) {\n\tfor try := uint(0); try < uint(maxDevReplaceRetries); try++ {\n\t\tres, err = sendManageReq(ctx, e, drpc.MethodReplaceStorage, req)\n\t\tif err != nil {\n\t\t\treturn\n\t\t}\n\t\tif daos.Status(res.Status) != daos.Busy {\n\t\t\tbreak\n\t\t}\n\n\t\tbackoff := common.ExpBackoff(baseDevReplaceBackoff, uint64(try),\n\t\t\tuint64(maxDevReplaceBackoffFactor))\n\t\tlog.Debugf(\"retrying dev-replace drpc request after %s\", backoff)\n\n\t\tselect {\n\t\tcase <-ctx.Done():\n\t\t\terr = ctx.Err()\n\t\t\treturn\n\t\tcase <-time.After(backoff):\n\t\t}\n\t}\n\n\treturn\n}\n\n// SmdManage implements the method defined for the Management Service.\n//\n// Manage SMD devices.\nfunc (svc *ControlService) SmdManage(ctx context.Context, req *ctlpb.SmdManageReq) (*ctlpb.SmdManageResp, error) {\n\tif !svc.harness.isStarted() {\n\t\treturn nil, FaultHarnessNotStarted\n\t}\n\tif len(svc.harness.readyRanks()) == 0 {\n\t\treturn nil, FaultDataPlaneNotStarted\n\t}\n\n\t// Flag indicates whether Device-UUID can be replaced with its parent NVMe controller address.\n\tvar useTrAddrInReq bool\n\tvar ids string\n\n\tswitch req.Op.(type) {\n\tcase *ctlpb.SmdManageReq_Replace:\n\t\tids = req.GetReplace().OldDevUuid\n\tcase *ctlpb.SmdManageReq_Faulty:\n\t\tids = req.GetFaulty().Uuid\n\tcase *ctlpb.SmdManageReq_Led:\n\t\tuseTrAddrInReq = true\n\t\tids = req.GetLed().Ids\n\tdefault:\n\t\treturn nil, errors.Errorf(\"Unrecognized operation in SmdManageReq: %+v\", req.Op)\n\t}\n\n\t// Evaluate which engine(s) to send requests to.\n\tengineDevMap, err := svc.mapIDsToEngine(ctx, ids, useTrAddrInReq)\n\tif err != nil {\n\t\treturn nil, errors.Wrap(err, \"mapping device identifiers to engine\")\n\t}\n\n\trankResps := []*ctlpb.SmdManageResp_RankResp{}\n\n\tfor engine, devs := range engineDevMap {\n\t\tdevResults := []*ctlpb.SmdManageResp_Result{}\n\n\t\trank, err := engine.GetRank()\n\t\tif err != nil {\n\t\t\treturn nil, errors.Wrap(err, \"retrieving engine rank\")\n\t\t}\n\n\t\tmsg := fmt.Sprintf(\"CtlSvc.SmdManage dispatch, rank %d: %%s req:%%+v\\n\", rank)\n\n\t\t// Extract request from oneof field and execute dRPC.\n\t\tswitch req.Op.(type) {\n\t\tcase *ctlpb.SmdManageReq_Replace:\n\t\t\tif len(devs) != 1 {\n\t\t\t\treturn nil, errors.New(\"replace request expects only one device ID\")\n\t\t\t}\n\t\t\tdReq := req.GetReplace()\n\t\t\tsvc.log.Debugf(msg, \"dev-replace\", dReq)\n\t\t\tdevRes, err := replaceDevRetryBusy(ctx, svc.log, engine, dReq)\n\t\t\tif err != nil {\n\t\t\t\treturn nil, errors.Wrap(err, msg)\n\t\t\t}\n\t\t\taddManageRespIDOnFail(svc.log, devRes, devs.getFirst())\n\t\t\tdevResults = append(devResults, devRes)\n\t\tcase *ctlpb.SmdManageReq_Faulty:\n\t\t\tif len(devs) != 1 {\n\t\t\t\treturn nil, errors.New(\"set-faulty request expects only one device ID\")\n\t\t\t}\n\t\t\tdReq := req.GetFaulty()\n\t\t\tsvc.log.Debugf(msg, \"set-faulty\", dReq)\n\t\t\tdevRes, err := sendManageReq(ctx, engine, drpc.MethodSetFaultyState, dReq)\n\t\t\tif err != nil {\n\t\t\t\treturn nil, errors.Wrap(err, msg)\n\t\t\t}\n\t\t\taddManageRespIDOnFail(svc.log, devRes, devs.getFirst())\n\t\t\tdevResults = append(devResults, devRes)\n\t\tcase *ctlpb.SmdManageReq_Led:\n\t\t\tif len(devs) == 0 {\n\t\t\t\t// Operate on all devices by default.\n\t\t\t\treturn nil, errors.New(\"led-manage request expects one or more IDs\")\n\t\t\t}\n\t\t\t// Multiple addresses are supported in LED request.\n\t\t\tfor _, dev := range devs {\n\t\t\t\tdReq := req.GetLed()\n\t\t\t\t// ID should by now have been resolved to a transport (PCI) address.\n\t\t\t\tif dev.trAddr == \"\" {\n\t\t\t\t\treturn nil, errors.Errorf(\"device uuid %s not resolved to a PCI address\",\n\t\t\t\t\t\tdev.uuid)\n\t\t\t\t}\n\t\t\t\tdReq.Ids = dev.trAddr\n\t\t\t\tsvc.log.Debugf(msg, \"led-manage\", dReq)\n\t\t\t\tdevRes, err := sendManageReq(ctx, engine, drpc.MethodLedManage, dReq)\n\t\t\t\tif err != nil {\n\t\t\t\t\treturn nil, errors.Wrap(err, msg)\n\t\t\t\t}\n\t\t\t\taddManageRespIDOnFail(svc.log, devRes, &dev)\n\t\t\t\tdevResults = append(devResults, devRes)\n\t\t\t}\n\t\tdefault:\n\t\t\treturn nil, errors.New(\"unexpected smd manage request type\")\n\t\t}\n\n\t\trankResps = append(rankResps, &ctlpb.SmdManageResp_RankResp{\n\t\t\tRank: rank.Uint32(), Results: devResults,\n\t\t})\n\t}\n\n\tsort.Slice(rankResps, func(i, j int) bool {\n\t\treturn rankResps[i].Rank < rankResps[j].Rank\n\t})\n\n\tresp := &ctlpb.SmdManageResp{Ranks: rankResps}\n\n\treturn resp, nil\n}\n"
    },
    {
      "path": "src/control/server/harness.go",
      "content": "//\n// (C) Copyright 2019-2023 Intel Corporation.\n//\n// SPDX-License-Identifier: BSD-2-Clause-Patent\n//\n\npackage server\n\nimport (\n\t\"context\"\n\t\"os\"\n\t\"sync\"\n\n\t\"github.com/pkg/errors\"\n\t\"google.golang.org/protobuf/proto\"\n\n\tcommonpb \"github.com/daos-stack/daos/src/control/common/proto\"\n\tctlpb \"github.com/daos-stack/daos/src/control/common/proto/ctl\"\n\tsrvpb \"github.com/daos-stack/daos/src/control/common/proto/srv\"\n\t\"github.com/daos-stack/daos/src/control/drpc\"\n\t\"github.com/daos-stack/daos/src/control/lib/atm\"\n\t\"github.com/daos-stack/daos/src/control/lib/ranklist\"\n\t\"github.com/daos-stack/daos/src/control/logging\"\n\t\"github.com/daos-stack/daos/src/control/server/config\"\n\t\"github.com/daos-stack/daos/src/control/server/storage\"\n\t\"github.com/daos-stack/daos/src/control/system\"\n)\n\n// Engine defines an interface to be implemented by engine instances.\n//\n// NB: This interface is way too big right now; need to refactor in order\n// to limit scope.\ntype Engine interface {\n\t// These are definitely wrong... They indicate that too much internal\n\t// information is being leaked outside of the implementation.\n\tnewCret(string, error) *ctlpb.NvmeControllerResult\n\ttryDrpc(context.Context, drpc.Method) *system.MemberResult\n\trequestStart(context.Context)\n\tupdateInUseBdevs(context.Context, []storage.NvmeController, uint64, uint64) ([]storage.NvmeController, error)\n\tisAwaitingFormat() bool\n\n\t// These methods should probably be replaced by callbacks.\n\tNotifyDrpcReady(*srvpb.NotifyReadyReq)\n\tNotifyStorageReady()\n\tBioErrorNotify(*srvpb.BioErrorReq)\n\n\t// These methods should probably be refactored out into functions that\n\t// accept the engine instance as a parameter.\n\tGetBioHealth(context.Context, *ctlpb.BioHealthReq) (*ctlpb.BioHealthResp, error)\n\tScanBdevTiers() ([]storage.BdevTierScanResult, error)\n\tListSmdDevices(context.Context, *ctlpb.SmdDevReq) (*ctlpb.SmdDevResp, error)\n\tStorageFormatSCM(context.Context, bool) *ctlpb.ScmMountResult\n\tStorageFormatNVMe() commonpb.NvmeControllerResults\n\n\t// This is a more reasonable surface that will be easier to maintain and test.\n\tCallDrpc(context.Context, drpc.Method, proto.Message) (*drpc.Response, error)\n\tGetRank() (ranklist.Rank, error)\n\tGetTargetCount() int\n\tIndex() uint32\n\tIsStarted() bool\n\tIsReady() bool\n\tLocalState() system.MemberState\n\tRemoveSuperblock() error\n\tRun(context.Context, bool)\n\tSetupRank(context.Context, ranklist.Rank, uint32) error\n\tStop(os.Signal) error\n\tOnInstanceExit(...onInstanceExitFn)\n\tOnReady(...onReadyFn)\n\tGetStorage() *storage.Provider\n}\n\n// EngineHarness is responsible for managing Engine instances.\ntype EngineHarness struct {\n\tsync.RWMutex\n\tlog           logging.Logger\n\tinstances     []Engine\n\tstarted       atm.Bool\n\tfaultDomain   *system.FaultDomain\n\tonDrpcFailure []func(context.Context, error)\n}\n\n// NewEngineHarness returns an initialized *EngineHarness.\nfunc NewEngineHarness(log logging.Logger) *EngineHarness {\n\treturn &EngineHarness{\n\t\tlog:       log,\n\t\tinstances: make([]Engine, 0),\n\t}\n}\n\n// WithFaultDomain adds a fault domain to the EngineHarness.\nfunc (h *EngineHarness) WithFaultDomain(fd *system.FaultDomain) *EngineHarness {\n\th.faultDomain = fd\n\treturn h\n}\n\n// isStarted indicates whether the EngineHarness is in a running state.\nfunc (h *EngineHarness) isStarted() bool {\n\treturn h.started.Load()\n}\n\n// Instances safely returns harness' EngineInstances.\nfunc (h *EngineHarness) Instances() []Engine {\n\th.RLock()\n\tdefer h.RUnlock()\n\treturn h.instances\n}\n\n// FilterInstancesByRankSet returns harness' EngineInstances that match any\n// of a list of ranks derived from provided rank set string.\nfunc (h *EngineHarness) FilterInstancesByRankSet(ranks string) ([]Engine, error) {\n\th.RLock()\n\tdefer h.RUnlock()\n\n\trankList, err := ranklist.ParseRanks(ranks)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\tout := make([]Engine, 0)\n\n\tfor _, i := range h.instances {\n\t\tr, err := i.GetRank()\n\t\tif err != nil {\n\t\t\tcontinue // no rank to check against\n\t\t}\n\t\tif r.InList(rankList) {\n\t\t\tout = append(out, i)\n\t\t}\n\t}\n\n\treturn out, nil\n}\n\n// AddInstance adds a new Engine instance to be managed.\nfunc (h *EngineHarness) AddInstance(ei Engine) error {\n\tif h.isStarted() {\n\t\treturn errors.New(\"can't add instance to already-started harness\")\n\t}\n\n\th.Lock()\n\tdefer h.Unlock()\n\tif indexSetter, ok := ei.(interface{ setIndex(uint32) }); ok {\n\t\tindexSetter.setIndex(uint32(len(h.instances)))\n\t}\n\n\th.instances = append(h.instances, ei)\n\treturn nil\n}\n\n// OnDrpcFailure registers callbacks to be invoked on dRPC call failure.\nfunc (h *EngineHarness) OnDrpcFailure(fns ...func(ctx context.Context, err error)) {\n\th.Lock()\n\tdefer h.Unlock()\n\n\th.onDrpcFailure = append(h.onDrpcFailure, fns...)\n}\n\n// CallDrpc calls the supplied dRPC method on a managed I/O Engine instance.\nfunc (h *EngineHarness) CallDrpc(ctx context.Context, method drpc.Method, body proto.Message) (resp *drpc.Response, err error) {\n\tdefer func() {\n\t\tif err == nil {\n\t\t\treturn\n\t\t}\n\t\t// If the context was canceled, don't trigger callbacks.\n\t\tif errors.Cause(err) == context.Canceled {\n\t\t\treturn\n\t\t}\n\t\t// Don't trigger callbacks for these errors which can happen when\n\t\t// things are still starting up.\n\t\tif err == FaultHarnessNotStarted || err == errEngineNotReady {\n\t\t\treturn\n\t\t}\n\n\t\th.log.Debugf(\"invoking dRPC failure handlers for %s\", err)\n\t\th.RLock()\n\t\tdefer h.RUnlock()\n\t\tfor _, fn := range h.onDrpcFailure {\n\t\t\tfn(ctx, err)\n\t\t}\n\t}()\n\n\tif !h.isStarted() {\n\t\treturn nil, FaultHarnessNotStarted\n\t}\n\n\t// Iterate through the managed instances, looking for\n\t// the first one that is available to service the request.\n\t// If the request fails, that error will be returned.\n\tfor _, i := range h.Instances() {\n\t\tresp, err = i.CallDrpc(ctx, method, body)\n\n\t\tswitch errors.Cause(err) {\n\t\tcase errEngineNotReady, errDRPCNotReady, FaultDataPlaneNotStarted:\n\t\t\tcontinue\n\t\tdefault:\n\t\t\treturn\n\t\t}\n\t}\n\n\treturn\n}\n\ntype dbLeader interface {\n\tIsLeader() bool\n\tShutdownRaft() error\n\tResignLeadership(error) error\n}\n\n// Start starts harness by setting up and starting dRPC before initiating\n// configured instances' processing loops.\n//\n// Run until harness is shutdown.\nfunc (h *EngineHarness) Start(ctx context.Context, db dbLeader, cfg *config.Server) error {\n\tif h.isStarted() {\n\t\treturn errors.New(\"can't start: harness already started\")\n\t}\n\n\tif cfg == nil {\n\t\treturn errors.New(\"nil cfg supplied to Start()\")\n\t}\n\n\t// Now we want to block any RPCs that might try to mess with storage\n\t// (format, firmware update, etc) before attempting to start I/O Engines\n\t// which are using the storage.\n\th.started.SetTrue()\n\tdefer h.started.SetFalse()\n\n\tfor _, ei := range h.Instances() {\n\t\tei.Run(ctx, cfg.RecreateSuperblocks)\n\t}\n\n\th.OnDrpcFailure(func(_ context.Context, errIn error) {\n\t\tif !db.IsLeader() {\n\t\t\treturn\n\t\t}\n\n\t\tswitch errors.Cause(errIn) {\n\t\tcase errDRPCNotReady, FaultDataPlaneNotStarted:\n\t\t\tbreak\n\t\tdefault:\n\t\t\t// Don't shutdown on other failures which are\n\t\t\t// not related to dRPC communications.\n\t\t\treturn\n\t\t}\n\n\t\t// If we cannot service a dRPC request on this node,\n\t\t// we should resign as leader in order to force a new\n\t\t// leader election.\n\t\tif err := db.ResignLeadership(errIn); err != nil {\n\t\t\th.log.Errorf(\"failed to resign leadership after dRPC failure: %s\", err)\n\t\t}\n\t})\n\n\t<-ctx.Done()\n\th.log.Debug(\"shutting down harness\")\n\n\treturn ctx.Err()\n}\n\n// readyRanks returns rank assignment of configured harness instances that are\n// in a ready state. Rank assignments can be nil.\nfunc (h *EngineHarness) readyRanks() []ranklist.Rank {\n\th.RLock()\n\tdefer h.RUnlock()\n\n\tranks := make([]ranklist.Rank, 0)\n\tfor idx, ei := range h.instances {\n\t\tif ei.IsReady() {\n\t\t\trank, err := ei.GetRank()\n\t\t\tif err != nil {\n\t\t\t\th.log.Errorf(\"instance %d: no rank (%s)\", idx, err)\n\t\t\t\tcontinue\n\t\t\t}\n\t\t\tranks = append(ranks, rank)\n\t\t}\n\t}\n\n\treturn ranks\n}\n"
    },
    {
      "path": "src/control/server/harness_test.go",
      "content": "//\n// (C) Copyright 2019-2023 Intel Corporation.\n//\n// SPDX-License-Identifier: BSD-2-Clause-Patent\n//\n\npackage server\n\nimport (\n\t\"context\"\n\t\"fmt\"\n\t\"os\"\n\t\"path/filepath\"\n\t\"strconv\"\n\t\"sync\"\n\t\"sync/atomic\"\n\t\"testing\"\n\t\"time\"\n\n\t\"github.com/google/go-cmp/cmp\"\n\t\"github.com/pkg/errors\"\n\t\"google.golang.org/protobuf/proto\"\n\n\t\"github.com/daos-stack/daos/src/control/common/test\"\n\t. \"github.com/daos-stack/daos/src/control/common/test\"\n\t\"github.com/daos-stack/daos/src/control/drpc\"\n\t\"github.com/daos-stack/daos/src/control/lib/atm\"\n\t\"github.com/daos-stack/daos/src/control/lib/control\"\n\t\"github.com/daos-stack/daos/src/control/lib/ranklist\"\n\t\"github.com/daos-stack/daos/src/control/logging\"\n\tsysprov \"github.com/daos-stack/daos/src/control/provider/system\"\n\t\"github.com/daos-stack/daos/src/control/security\"\n\t\"github.com/daos-stack/daos/src/control/server/config\"\n\t\"github.com/daos-stack/daos/src/control/server/engine\"\n\t\"github.com/daos-stack/daos/src/control/server/storage\"\n\t\"github.com/daos-stack/daos/src/control/server/storage/bdev\"\n\t\"github.com/daos-stack/daos/src/control/server/storage/scm\"\n\t\"github.com/daos-stack/daos/src/control/system\"\n\t\"github.com/daos-stack/daos/src/control/system/raft\"\n)\n\nconst (\n\ttestShortTimeout   = 50 * time.Millisecond\n\ttestLongTimeout    = 1 * time.Minute\n\tdelayedFailTimeout = 20 * testShortTimeout\n\tmaxEngines         = 2\n)\n\nfunc TestServer_Harness_Start(t *testing.T) {\n\tfor name, tc := range map[string]struct {\n\t\ttrc              *engine.TestRunnerConfig\n\t\tisAP             bool                     // is first instance an AP/MS replica/bootstrap\n\t\trankInSuperblock bool                     // rank already set in superblock when starting\n\t\tinstanceUuids    map[int]string           // UUIDs for each instance.Index()\n\t\tdontNotifyReady  bool                     // skip sending notify ready on dRPC channel\n\t\twaitTimeout      time.Duration            // time after which test context is cancelled\n\t\texpStartErr      error                    // error from harness.Start()\n\t\texpStartCount    uint32                   // number of instance.runner.Start() calls\n\t\texpDrpcCalls     map[uint32][]drpc.Method // method ids called for each instance.Index()\n\t\texpGrpcCalls     map[uint32][]string      // string repr of call for each instance.Index()\n\t\texpRanks         map[uint32]ranklist.Rank // ranks to have been set during Start()\n\t\texpMembers       system.Members           // members to have been registered during Start()\n\t\texpIoErrs        map[uint32]error         // errors expected from instances\n\t}{\n\t\t\"normal startup/shutdown\": {\n\t\t\ttrc: &engine.TestRunnerConfig{\n\t\t\t\tRunnerExitInfoCb: func(ctx context.Context) *engine.RunnerExitInfo {\n\t\t\t\t\tselect {\n\t\t\t\t\tcase <-ctx.Done():\n\t\t\t\t\tcase <-time.After(testLongTimeout):\n\t\t\t\t\t}\n\t\t\t\t\treturn &engine.RunnerExitInfo{\n\t\t\t\t\t\tError: errors.New(\"ending\"),\n\t\t\t\t\t}\n\t\t\t\t},\n\t\t\t},\n\t\t\tinstanceUuids: map[int]string{\n\t\t\t\t0: MockUUID(0),\n\t\t\t\t1: MockUUID(1),\n\t\t\t},\n\t\t\texpStartCount: maxEngines,\n\t\t\texpDrpcCalls: map[uint32][]drpc.Method{\n\t\t\t\t0: {\n\t\t\t\t\tdrpc.MethodSetRank,\n\t\t\t\t\tdrpc.MethodSetUp,\n\t\t\t\t},\n\t\t\t\t1: {\n\t\t\t\t\tdrpc.MethodSetRank,\n\t\t\t\t\tdrpc.MethodSetUp,\n\t\t\t\t},\n\t\t\t},\n\t\t\texpGrpcCalls: map[uint32][]string{\n\t\t\t\t0: {fmt.Sprintf(\"Join %d\", ranklist.NilRank)},\n\t\t\t\t1: {fmt.Sprintf(\"Join %d\", ranklist.NilRank)},\n\t\t\t},\n\t\t\texpRanks: map[uint32]ranklist.Rank{\n\t\t\t\t0: ranklist.Rank(0),\n\t\t\t\t1: ranklist.Rank(1),\n\t\t\t},\n\t\t},\n\t\t\"startup/shutdown with preset ranks\": {\n\t\t\ttrc: &engine.TestRunnerConfig{\n\t\t\t\tRunnerExitInfoCb: func(ctx context.Context) *engine.RunnerExitInfo {\n\t\t\t\t\tselect {\n\t\t\t\t\tcase <-ctx.Done():\n\t\t\t\t\tcase <-time.After(testLongTimeout):\n\t\t\t\t\t}\n\t\t\t\t\treturn &engine.RunnerExitInfo{\n\t\t\t\t\t\tError: errors.New(\"ending\"),\n\t\t\t\t\t}\n\t\t\t\t},\n\t\t\t},\n\t\t\trankInSuperblock: true,\n\t\t\texpStartCount:    maxEngines,\n\t\t\texpDrpcCalls: map[uint32][]drpc.Method{\n\t\t\t\t0: {\n\t\t\t\t\tdrpc.MethodSetRank,\n\t\t\t\t\tdrpc.MethodSetUp,\n\t\t\t\t},\n\t\t\t\t1: {\n\t\t\t\t\tdrpc.MethodSetRank,\n\t\t\t\t\tdrpc.MethodSetUp,\n\t\t\t\t},\n\t\t\t},\n\t\t\texpGrpcCalls: map[uint32][]string{\n\t\t\t\t0: {\"Join 1\"}, // rank == instance.Index() + 1\n\t\t\t\t1: {\"Join 2\"},\n\t\t\t},\n\t\t\texpRanks: map[uint32]ranklist.Rank{\n\t\t\t\t0: ranklist.Rank(1),\n\t\t\t\t1: ranklist.Rank(2),\n\t\t\t},\n\t\t},\n\t\t\"fails to start\": {\n\t\t\ttrc:           &engine.TestRunnerConfig{StartErr: errors.New(\"no\")},\n\t\t\twaitTimeout:   10 * testShortTimeout,\n\t\t\texpStartErr:   context.DeadlineExceeded,\n\t\t\texpStartCount: 2, // both start but don't proceed so context times out\n\t\t},\n\t\t\"delayed failure occurs before notify ready\": {\n\t\t\tdontNotifyReady: true,\n\t\t\twaitTimeout:     30 * testShortTimeout,\n\t\t\texpStartErr:     context.DeadlineExceeded,\n\t\t\ttrc: &engine.TestRunnerConfig{\n\t\t\t\tRunnerExitInfoCb: func(ctx context.Context) *engine.RunnerExitInfo {\n\t\t\t\t\tselect {\n\t\t\t\t\tcase <-ctx.Done():\n\t\t\t\t\tcase <-time.After(delayedFailTimeout):\n\t\t\t\t\t}\n\t\t\t\t\treturn &engine.RunnerExitInfo{\n\t\t\t\t\t\tError: errors.New(\"oops\"),\n\t\t\t\t\t}\n\t\t\t\t},\n\t\t\t},\n\t\t\texpStartCount: maxEngines,\n\t\t\texpRanks: map[uint32]ranklist.Rank{\n\t\t\t\t0: ranklist.NilRank,\n\t\t\t\t1: ranklist.NilRank,\n\t\t\t},\n\t\t\texpIoErrs: map[uint32]error{\n\t\t\t\t0: errors.New(\"oops\"),\n\t\t\t\t1: errors.New(\"oops\"),\n\t\t\t},\n\t\t},\n\t\t\"delayed failure occurs after ready\": {\n\t\t\twaitTimeout: 100 * testShortTimeout,\n\t\t\texpStartErr: context.DeadlineExceeded,\n\t\t\ttrc: &engine.TestRunnerConfig{\n\t\t\t\tRunnerExitInfoCb: func(ctx context.Context) *engine.RunnerExitInfo {\n\t\t\t\t\tselect {\n\t\t\t\t\tcase <-ctx.Done():\n\t\t\t\t\tcase <-time.After(delayedFailTimeout):\n\t\t\t\t\t}\n\t\t\t\t\treturn &engine.RunnerExitInfo{\n\t\t\t\t\t\tError: errors.New(\"oops\"),\n\t\t\t\t\t}\n\t\t\t\t},\n\t\t\t},\n\t\t\tinstanceUuids: map[int]string{\n\t\t\t\t0: MockUUID(0),\n\t\t\t\t1: MockUUID(1),\n\t\t\t},\n\t\t\texpStartCount: maxEngines,\n\t\t\texpDrpcCalls: map[uint32][]drpc.Method{\n\t\t\t\t0: {\n\t\t\t\t\tdrpc.MethodSetRank,\n\t\t\t\t\tdrpc.MethodSetUp,\n\t\t\t\t},\n\t\t\t\t1: {\n\t\t\t\t\tdrpc.MethodSetRank,\n\t\t\t\t\tdrpc.MethodSetUp,\n\t\t\t\t},\n\t\t\t},\n\t\t\texpGrpcCalls: map[uint32][]string{\n\t\t\t\t0: {fmt.Sprintf(\"Join %d\", ranklist.NilRank)},\n\t\t\t\t1: {fmt.Sprintf(\"Join %d\", ranklist.NilRank)},\n\t\t\t},\n\t\t\texpRanks: map[uint32]ranklist.Rank{\n\t\t\t\t0: ranklist.Rank(0),\n\t\t\t\t1: ranklist.Rank(1),\n\t\t\t},\n\t\t\texpIoErrs: map[uint32]error{\n\t\t\t\t0: errors.New(\"oops\"),\n\t\t\t\t1: errors.New(\"oops\"),\n\t\t\t},\n\t\t},\n\t} {\n\t\tt.Run(name, func(t *testing.T) {\n\t\t\tlog, buf := logging.NewTestLogger(t.Name())\n\t\t\tdefer ShowBufferOnFailure(t, buf)\n\n\t\t\ttestDir, cleanup := CreateTestDir(t)\n\t\t\tdefer cleanup()\n\n\t\t\tengineCfgs := make([]*engine.Config, maxEngines)\n\t\t\tfor i := 0; i < maxEngines; i++ {\n\t\t\t\tengineCfgs[i] = engine.MockConfig().\n\t\t\t\t\tWithStorage(\n\t\t\t\t\t\tstorage.NewTierConfig().\n\t\t\t\t\t\t\tWithStorageClass(\"ram\").\n\t\t\t\t\t\t\tWithScmRamdiskSize(1).\n\t\t\t\t\t\t\tWithScmMountPoint(filepath.Join(testDir, strconv.Itoa(i))),\n\t\t\t\t\t)\n\t\t\t}\n\t\t\tconfig := config.DefaultServer().\n\t\t\t\tWithEngines(engineCfgs...).\n\t\t\t\tWithSocketDir(testDir).\n\t\t\t\tWithTransportConfig(&security.TransportConfig{AllowInsecure: true})\n\n\t\t\tjoinMu := sync.Mutex{}\n\t\t\tjoinRequests := make(map[uint32][]string)\n\t\t\tvar instanceStarts uint32\n\t\t\tharness := NewEngineHarness(log)\n\t\t\tfor i, engineCfg := range config.Engines {\n\t\t\t\tif err := os.MkdirAll(engineCfg.Storage.Tiers[0].Scm.MountPoint, 0777); err != nil {\n\t\t\t\t\tt.Fatal(err)\n\t\t\t\t}\n\n\t\t\t\tif tc.trc == nil {\n\t\t\t\t\ttc.trc = &engine.TestRunnerConfig{}\n\t\t\t\t}\n\t\t\t\tif tc.trc.StartCb == nil {\n\t\t\t\t\ttc.trc.StartCb = func() {\n\t\t\t\t\t\t_ = atomic.AddUint32(&instanceStarts, 1)\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t\trunner := engine.NewTestRunner(tc.trc, engineCfg)\n\n\t\t\t\tmsc := &sysprov.MockSysConfig{IsMountedBool: true}\n\t\t\t\tsysp := sysprov.NewMockSysProvider(log, msc)\n\t\t\t\tprovider := storage.MockProvider(\n\t\t\t\t\tlog, 0, &engineCfg.Storage,\n\t\t\t\t\tsysp,\n\t\t\t\t\tscm.NewMockProvider(log, nil, msc),\n\t\t\t\t\tbdev.NewMockProvider(log, &bdev.MockBackendConfig{}),\n\t\t\t\t\tnil,\n\t\t\t\t)\n\n\t\t\t\tidx := uint32(i)\n\t\t\t\tjoinFn := func(_ context.Context, req *control.SystemJoinReq) (*control.SystemJoinResp, error) {\n\t\t\t\t\t// appease the race detector\n\t\t\t\t\tjoinMu.Lock()\n\t\t\t\t\tdefer joinMu.Unlock()\n\t\t\t\t\tjoinRequests[idx] = []string{fmt.Sprintf(\"Join %d\", req.Rank)}\n\t\t\t\t\treturn &control.SystemJoinResp{\n\t\t\t\t\t\tRank: ranklist.Rank(idx),\n\t\t\t\t\t}, nil\n\t\t\t\t}\n\n\t\t\t\tei := NewEngineInstance(log, provider, joinFn, runner)\n\t\t\t\tvar isAP bool\n\t\t\t\tif tc.isAP && i == 0 { // first instance will be AP & bootstrap MS\n\t\t\t\t\tisAP = true\n\t\t\t\t}\n\t\t\t\tvar uuid string\n\t\t\t\tif UUID, exists := tc.instanceUuids[i]; exists {\n\t\t\t\t\tuuid = UUID\n\t\t\t\t}\n\t\t\t\tvar rank *ranklist.Rank\n\t\t\t\tvar isValid bool\n\t\t\t\tif tc.rankInSuperblock {\n\t\t\t\t\trank = ranklist.NewRankPtr(uint32(i + 1))\n\t\t\t\t\tisValid = true\n\t\t\t\t} else if isAP { // bootstrap will assume rank 0\n\t\t\t\t\trank = new(ranklist.Rank)\n\t\t\t\t}\n\t\t\t\tei.setSuperblock(&Superblock{\n\t\t\t\t\tUUID: uuid, Rank: rank, ValidRank: isValid,\n\t\t\t\t})\n\n\t\t\t\tif err := harness.AddInstance(ei); err != nil {\n\t\t\t\t\tt.Fatal(err)\n\t\t\t\t}\n\t\t\t}\n\n\t\t\tinstances := harness.Instances()\n\n\t\t\t// set mock dRPC client to record call details\n\t\t\tfor _, e := range instances {\n\t\t\t\tei := e.(*EngineInstance)\n\t\t\t\tei.setDrpcClient(newMockDrpcClient(&mockDrpcClientConfig{\n\t\t\t\t\tSendMsgResponse: &drpc.Response{},\n\t\t\t\t}))\n\t\t\t}\n\n\t\t\tctx, cancel := context.WithCancel(test.Context(t))\n\t\t\tif tc.waitTimeout != 0 {\n\t\t\t\tctx, cancel = context.WithTimeout(ctx, tc.waitTimeout)\n\t\t\t}\n\t\t\tdefer cancel()\n\n\t\t\t// start harness async and signal completion\n\t\t\tvar gotErr error\n\t\t\tsysdb := raft.MockDatabase(t, log)\n\t\t\tmembership := system.MockMembership(t, log, sysdb, mockTCPResolver)\n\t\t\tdone := make(chan struct{})\n\t\t\tgo func(ctxIn context.Context) {\n\t\t\t\tgotErr = harness.Start(ctxIn, sysdb, config)\n\t\t\t\tclose(done)\n\t\t\t}(ctx)\n\n\t\t\twaitDrpcReady := make(chan struct{})\n\t\t\tt.Log(\"waiting for dRPC to be ready\")\n\t\t\tgo func(ctxIn context.Context) {\n\t\t\t\tfor {\n\t\t\t\t\tready := true\n\t\t\t\t\tfor _, ei := range instances {\n\t\t\t\t\t\tif ei.(*EngineInstance).waitDrpc.IsFalse() {\n\t\t\t\t\t\t\tready = false\n\t\t\t\t\t\t}\n\t\t\t\t\t}\n\t\t\t\t\tif ready {\n\t\t\t\t\t\tclose(waitDrpcReady)\n\t\t\t\t\t\treturn\n\t\t\t\t\t}\n\t\t\t\t\tselect {\n\t\t\t\t\tcase <-time.After(testShortTimeout):\n\t\t\t\t\tcase <-ctxIn.Done():\n\t\t\t\t\t\treturn\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t}(ctx)\n\n\t\t\tselect {\n\t\t\tcase <-waitDrpcReady:\n\t\t\t\tt.Log(\"dRPC is ready\")\n\t\t\tcase <-ctx.Done():\n\t\t\t\tif tc.expStartErr != nil {\n\t\t\t\t\t<-done\n\t\t\t\t\tCmpErr(t, tc.expStartErr, gotErr)\n\t\t\t\t\tif atomic.LoadUint32(&instanceStarts) != tc.expStartCount {\n\t\t\t\t\t\tt.Fatalf(\"expected %d starts, got %d\",\n\t\t\t\t\t\t\ttc.expStartCount, instanceStarts)\n\t\t\t\t\t}\n\t\t\t\t\treturn\n\t\t\t\t}\n\t\t\t\t// deadline exceeded as expected but desired state not reached\n\t\t\t\tt.Fatalf(\"instances did not get to waiting for dRPC state: %s\", ctx.Err())\n\t\t\t}\n\t\t\tt.Log(\"instances ready and waiting for dRPC ready notification\")\n\n\t\t\t// simulate receiving notify ready whilst instances\n\t\t\t// running in harness (unless dontNotifyReady flag is set)\n\t\t\tfor _, ei := range instances {\n\t\t\t\tif tc.dontNotifyReady {\n\t\t\t\t\tcontinue\n\t\t\t\t}\n\t\t\t\treq := getTestNotifyReadyReq(t, \"/tmp/instance_test.sock\", 0)\n\t\t\t\tgo func(ctxIn context.Context, i *EngineInstance) {\n\t\t\t\t\tselect {\n\t\t\t\t\tcase i.drpcReady <- req:\n\t\t\t\t\tcase <-ctxIn.Done():\n\t\t\t\t\t}\n\t\t\t\t}(ctx, ei.(*EngineInstance))\n\t\t\t\tt.Logf(\"sent drpc ready to instance %d\", ei.Index())\n\t\t\t}\n\n\t\t\twaitReady := make(chan struct{})\n\t\t\tt.Log(\"waitng for ready\")\n\t\t\tgo func(ctxIn context.Context) {\n\t\t\t\tfor {\n\t\t\t\t\tif len(harness.readyRanks()) == len(instances) {\n\t\t\t\t\t\tclose(waitReady)\n\t\t\t\t\t\treturn\n\t\t\t\t\t}\n\t\t\t\t\tselect {\n\t\t\t\t\tcase <-time.After(testShortTimeout):\n\t\t\t\t\tcase <-ctxIn.Done():\n\t\t\t\t\t\treturn\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t}(ctx)\n\n\t\t\tselect {\n\t\t\tcase <-waitReady:\n\t\t\t\tt.Log(\"instances setup and ready\")\n\t\t\tcase <-ctx.Done():\n\t\t\t\tt.Logf(\"instances did not get to ready state (%s)\", ctx.Err())\n\t\t\t}\n\n\t\t\tif atomic.LoadUint32(&instanceStarts) != tc.expStartCount {\n\t\t\t\tt.Fatalf(\"expected %d starts, got %d\", tc.expStartCount, instanceStarts)\n\t\t\t}\n\n\t\t\tif tc.waitTimeout == 0 { // if custom timeout, don't cancel\n\t\t\t\tcancel() // all ranks have been started, run finished\n\t\t\t}\n\t\t\t<-done\n\t\t\tif gotErr != context.Canceled || tc.expStartErr != nil {\n\t\t\t\tCmpErr(t, tc.expStartErr, gotErr)\n\t\t\t\tif tc.expStartErr != nil {\n\t\t\t\t\treturn\n\t\t\t\t}\n\t\t\t}\n\n\t\t\tjoinMu.Lock()\n\t\t\tdefer joinMu.Unlock()\n\t\t\t// verify expected RPCs were made, ranks allocated and\n\t\t\t// members added to membership\n\t\t\tfor _, e := range instances {\n\t\t\t\tei := e.(*EngineInstance)\n\t\t\t\tdc, err := ei.getDrpcClient()\n\t\t\t\tif err != nil {\n\t\t\t\t\tt.Fatal(err)\n\t\t\t\t}\n\t\t\t\tgotDrpcCalls := dc.(*mockDrpcClient).CalledMethods()\n\t\t\t\tAssertEqual(t, tc.expDrpcCalls[ei.Index()], gotDrpcCalls,\n\t\t\t\t\tfmt.Sprintf(\"%s: unexpected dRPCs for instance %d\", name, ei.Index()))\n\n\t\t\t\tif diff := cmp.Diff(tc.expGrpcCalls[ei.Index()], joinRequests[ei.Index()]); diff != \"\" {\n\t\t\t\t\tt.Fatalf(\"unexpected gRPCs for instance %d (-want, +got):\\n%s\\n\",\n\t\t\t\t\t\tei.Index(), diff)\n\t\t\t\t}\n\t\t\t\trank, _ := ei.GetRank()\n\t\t\t\tif diff := cmp.Diff(tc.expRanks[ei.Index()], rank); diff != \"\" {\n\t\t\t\t\tt.Fatalf(\"unexpected rank for instance %d (-want, +got):\\n%s\\n\",\n\t\t\t\t\t\tei.Index(), diff)\n\t\t\t\t}\n\t\t\t\tCmpErr(t, tc.expIoErrs[ei.Index()], ei._lastErr)\n\t\t\t}\n\t\t\tmembers, err := membership.Members(nil)\n\t\t\tif err != nil {\n\t\t\t\tt.Fatal(err)\n\t\t\t}\n\t\t\tAssertEqual(t, len(tc.expMembers), len(members), \"unexpected number in membership\")\n\t\t\tfor i, member := range members {\n\t\t\t\tif diff := cmp.Diff(fmt.Sprintf(\"%v\", member),\n\t\t\t\t\tfmt.Sprintf(\"%v\", tc.expMembers[i])); diff != \"\" {\n\n\t\t\t\t\tt.Fatalf(\"unexpected system membership (-want, +got):\\n%s\\n\", diff)\n\t\t\t\t}\n\t\t\t}\n\t\t})\n\t}\n}\n\nfunc TestServer_Harness_WithFaultDomain(t *testing.T) {\n\tharness := &EngineHarness{}\n\tfd, err := system.NewFaultDomainFromString(\"/one/two\")\n\tif err != nil {\n\t\tt.Fatalf(\"couldn't create fault domain: %s\", err)\n\t}\n\n\tupdatedHarness := harness.WithFaultDomain(fd)\n\n\t// Updated to include the fault domain\n\tif diff := cmp.Diff(harness.faultDomain, fd); diff != \"\" {\n\t\tt.Fatalf(\"unexpected results (-want, +got):\\n%s\\n\", diff)\n\t}\n\t// updatedHarness is the same as harness\n\tAssertEqual(t, updatedHarness, harness, \"not the same structure\")\n}\n\ntype mockdb struct {\n\tisLeader    bool\n\tshutdown    bool\n\tshutdownErr error\n}\n\nfunc (db *mockdb) IsLeader() bool {\n\treturn db.isLeader\n}\n\nfunc (db *mockdb) ShutdownRaft() error {\n\tdb.shutdown = true\n\treturn db.shutdownErr\n}\n\nfunc (db *mockdb) ResignLeadership(error) error {\n\tdb.isLeader = false\n\treturn nil\n}\n\nfunc TestServer_Harness_CallDrpc(t *testing.T) {\n\tfor name, tc := range map[string]struct {\n\t\tmics           []*MockInstanceConfig\n\t\tmethod         drpc.Method\n\t\tbody           proto.Message\n\t\tnotStarted     bool\n\t\tnotLeader      bool\n\t\tresignCause    error\n\t\texpShutdown    bool\n\t\texpNotLeader   bool\n\t\texpFailHandler bool\n\t\texpErr         error\n\t}{\n\t\t\"success\": {\n\t\t\tmics: []*MockInstanceConfig{\n\t\t\t\t{\n\t\t\t\t\tReady: atm.NewBool(true),\n\t\t\t\t},\n\t\t\t},\n\t\t},\n\t\t\"one not ready, one ready\": {\n\t\t\tmics: []*MockInstanceConfig{\n\t\t\t\t{\n\t\t\t\t\tReady:       atm.NewBool(true),\n\t\t\t\t\tCallDrpcErr: errDRPCNotReady,\n\t\t\t\t},\n\t\t\t\t{\n\t\t\t\t\tReady: atm.NewBool(true),\n\t\t\t\t},\n\t\t\t},\n\t\t},\n\t\t\"one not ready, one fails\": {\n\t\t\tmics: []*MockInstanceConfig{\n\t\t\t\t{\n\t\t\t\t\tReady:       atm.NewBool(true),\n\t\t\t\t\tCallDrpcErr: errDRPCNotReady,\n\t\t\t\t},\n\t\t\t\t{\n\t\t\t\t\tReady:       atm.NewBool(true),\n\t\t\t\t\tCallDrpcErr: errors.New(\"whoops\"),\n\t\t\t\t},\n\t\t\t},\n\t\t\texpErr:         errors.New(\"whoops\"),\n\t\t\texpFailHandler: true,\n\t\t},\n\t\t\"instance not ready\": {\n\t\t\tmics: []*MockInstanceConfig{\n\t\t\t\t{\n\t\t\t\t\tStarted:     atm.NewBool(true),\n\t\t\t\t\tCallDrpcErr: errEngineNotReady,\n\t\t\t\t},\n\t\t\t},\n\t\t\texpErr: errEngineNotReady,\n\t\t},\n\t\t\"harness not started\": {\n\t\t\tmics:       []*MockInstanceConfig{},\n\t\t\tnotStarted: true,\n\t\t\texpErr:     FaultHarnessNotStarted,\n\t\t},\n\t\t\"first fails (other)\": {\n\t\t\tmics: []*MockInstanceConfig{\n\t\t\t\t{\n\t\t\t\t\tReady:       atm.NewBool(true),\n\t\t\t\t\tCallDrpcErr: errors.New(\"whoops\"),\n\t\t\t\t},\n\t\t\t\t{\n\t\t\t\t\tReady: atm.NewBool(true),\n\t\t\t\t},\n\t\t\t},\n\t\t\texpErr:         errors.New(\"whoops\"),\n\t\t\texpFailHandler: true,\n\t\t},\n\t\t\"none available\": {\n\t\t\tmics: []*MockInstanceConfig{\n\t\t\t\t{\n\t\t\t\t\tReady:       atm.NewBool(true),\n\t\t\t\t\tCallDrpcErr: errDRPCNotReady,\n\t\t\t\t},\n\t\t\t\t{\n\t\t\t\t\tReady:       atm.NewBool(true),\n\t\t\t\t\tCallDrpcErr: FaultDataPlaneNotStarted,\n\t\t\t\t},\n\t\t\t},\n\t\t\texpNotLeader:   true,\n\t\t\texpErr:         FaultDataPlaneNotStarted,\n\t\t\texpFailHandler: true,\n\t\t},\n\t\t\"context canceled\": {\n\t\t\tmics: []*MockInstanceConfig{\n\t\t\t\t{\n\t\t\t\t\tReady:       atm.NewBool(true),\n\t\t\t\t\tCallDrpcErr: context.Canceled,\n\t\t\t\t},\n\t\t\t},\n\t\t\texpErr: context.Canceled,\n\t\t},\n\t} {\n\t\tt.Run(name, func(t *testing.T) {\n\t\t\tlog, buf := logging.NewTestLogger(name)\n\t\t\tdefer test.ShowBufferOnFailure(t, buf)\n\n\t\t\th := NewEngineHarness(log)\n\t\t\tfor _, mic := range tc.mics {\n\t\t\t\tif err := h.AddInstance(NewMockInstance(mic)); err != nil {\n\t\t\t\t\tt.Fatal(err)\n\t\t\t\t}\n\t\t\t}\n\n\t\t\tvar drpcFailureInvoked atm.Bool\n\t\t\th.OnDrpcFailure(func(_ context.Context, err error) {\n\t\t\t\tdrpcFailureInvoked.SetTrue()\n\t\t\t})\n\n\t\t\tctx, cancel := context.WithCancel(test.Context(t))\n\t\t\tdb := &mockdb{\n\t\t\t\tisLeader: !tc.notLeader,\n\t\t\t}\n\n\t\t\tstartErr := make(chan error)\n\t\t\tgo func() {\n\t\t\t\tif err := h.Start(ctx, db, config.DefaultServer()); err != nil {\n\t\t\t\t\tstartErr <- err\n\t\t\t\t}\n\t\t\t\tclose(startErr)\n\t\t\t}()\n\t\t\tfor {\n\t\t\t\tif h.isStarted() {\n\t\t\t\t\tbreak\n\t\t\t\t}\n\t\t\t}\n\t\t\tdefer func() {\n\t\t\t\tif err := <-startErr; err != nil {\n\t\t\t\t\tif err != context.Canceled {\n\t\t\t\t\t\tt.Fatal(err)\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t}()\n\t\t\tdefer cancel()\n\n\t\t\tif tc.notStarted {\n\t\t\t\th.started.SetFalse()\n\t\t\t}\n\n\t\t\t_, gotErr := h.CallDrpc(ctx, tc.method, tc.body)\n\t\t\ttest.CmpErr(t, tc.expErr, gotErr)\n\t\t\ttest.AssertEqual(t, db.shutdown, tc.expShutdown, \"unexpected shutdown state\")\n\t\t\ttest.AssertEqual(t, db.isLeader, !tc.expNotLeader, \"unexpected leader state\")\n\t\t\ttest.AssertEqual(t, drpcFailureInvoked.Load(), tc.expFailHandler, \"unexpected fail handler invocation\")\n\t\t})\n\t}\n}\n"
    },
    {
      "path": "src/mgmt/smd.pb-c.c",
      "content": Omitted as it is not in context
    },
    {
      "path": "src/proto/ctl/smd.proto",
      "content": "//\n// (C) Copyright 2019-2023 Intel Corporation.\n//\n// SPDX-License-Identifier: BSD-2-Clause-Patent\n//\n\nsyntax = \"proto3\";\npackage ctl;\n\noption go_package = \"github.com/daos-stack/daos/src/control/common/proto/ctl\";\n\n// Control Service Protobuf Definitions related to interactions between\n// DAOS control server and DAOS Blob I/O (BIO) module and Per-Server Metadata\n// (SMD).\n\nmessage BioHealthReq {\n\tstring dev_uuid = 1;\n\tuint64 meta_size = 2;\t// Size of the metadata (i.e. vos file index) blob\n\tuint64 rdb_size = 3;\t// Size of the RDB blob\n}\n\n// BioHealthResp mirrors nvme_health_stats structure.\nmessage BioHealthResp {\n\treserved 1, 2;\n\tuint64 timestamp = 3;\n\t// Device health details\n\tuint32 warn_temp_time = 5;\n\tuint32 crit_temp_time = 6;\n\tuint64 ctrl_busy_time = 7;\n\tuint64 power_cycles = 8;\n\tuint64 power_on_hours = 9;\n\tuint64 unsafe_shutdowns = 10;\n\tuint64 media_errs = 11;\n\tuint64 err_log_entries = 12;\n\t// I/O error counters\n\tuint32 bio_read_errs = 13;\n\tuint32 bio_write_errs = 14;\n\tuint32 bio_unmap_errs = 15;\n\tuint32 checksum_errs = 16;\n\tuint32 temperature = 17; // in Kelvin\n\t// Critical warnings\n\tbool temp_warn = 18;\n\tbool avail_spare_warn = 19;\n\tbool dev_reliability_warn = 20;\n\tbool read_only_warn = 21;\n\tbool volatile_mem_warn = 22; // volatile memory backup\n\tint32 status = 23; // DAOS err code\n\tstring dev_uuid = 24; // UUID of blobstore\n\t// Usage stats\n\tuint64 total_bytes = 25; // size of blobstore\n\tuint64 avail_bytes = 26; // free space in blobstore\n\t// Intel vendor SMART attributes\n\tuint32 program_fail_cnt_norm = 27; // percent remaining\n\tuint64 program_fail_cnt_raw = 28; // current value\n\tuint32 erase_fail_cnt_norm = 29;\n\tuint64 erase_fail_cnt_raw = 30;\n\tuint32 wear_leveling_cnt_norm = 31;\n\tuint32 wear_leveling_cnt_min = 32;\n\tuint32 wear_leveling_cnt_max = 33;\n\tuint32 wear_leveling_cnt_avg = 34;\n\tuint64 endtoend_err_cnt_raw = 35;\n\tuint64 crc_err_cnt_raw = 36;\n\tuint64 media_wear_raw = 37;\n\tuint64 host_reads_raw = 38;\n\tuint64 workload_timer_raw = 39;\n\tuint32 thermal_throttle_status = 40;\n\tuint64 thermal_throttle_event_cnt = 41;\n\tuint64 retry_buffer_overflow_cnt = 42;\n\tuint64 pll_lock_loss_cnt = 43;\n\tuint64 nand_bytes_written = 44;\n\tuint64 host_bytes_written = 45;\n\t// Engine configs properties\n\tuint64 cluster_size = 46;\t\t// blobstore cluster size in bytes\n\tuint64 meta_wal_size = 47;\t\t// metadata WAL blob size\n\tuint64 rdb_wal_size = 48;\t\t// RDB WAL blob size\n}\n\nenum NvmeDevState {\n\tUNKNOWN   = 0; // Device state is unknown, zer6o value\n\tNORMAL    = 1; // Device is in a normal operational state\n\tNEW       = 2; // Device is new and is not yet in-use\n\tEVICTED   = 3; // Device is faulty and has been evicted\n\tUNPLUGGED = 4; // Device has been physically removed\n}\n\nenum LedState {\n\tOFF = 0;\t\t// Equivalent to SPDK_VMD_LED_STATE_OFF\n\tQUICK_BLINK = 1;\t// Equivalent to SPDK_VMD_LED_STATE_IDENTIFY\t(4Hz blink)\n\tON = 2;\t\t\t// Equivalent to SPDK_VMD_LED_STATE_FAULT\t(solid on)\n\tSLOW_BLINK = 3;\t\t// Equivalent to SPDK_VMD_LED_STATE_REBUILD\t(1Hz blink)\n\tNA = 4;\t\t\t// Equivalent to SPDK_VMD_LED_STATE_UNKNOWN\t(VMD not enabled)\n}\n\n// SmdDevice represents a DAOS BIO device, identified by a UUID written into a label stored on a\n// SPDK blobstore created on a NVMe namespace. Multiple SmdDevices may exist per NVMe controller.\nmessage SmdDevice {\n\tstring uuid = 1;\t\t// UUID of blobstore\n\trepeated int32 tgt_ids = 2;\t// VOS target IDs\n\tstring tr_addr = 3;\t\t// Transport address of blobstore\n\tNvmeDevState dev_state = 4;\t// NVMe device state\n\tLedState led_state = 5;\t\t// LED state\n\tuint64 total_bytes = 6;\t\t// blobstore clusters total bytes\n\tuint64 avail_bytes = 7;\t\t// Available RAW storage for data\n\tuint64 cluster_size = 8;\t// blobstore cluster size in bytes\n\tuint32 rank = 9;\t\t// DAOS I/O Engine using controller\n\tuint32 role_bits = 10;\t\t// Device active roles (bitmask)\n\tuint64 meta_size = 11;\t\t// Size of the metadata (i.e. vos file index) blob\n\tuint64 meta_wal_size = 12;\t// Size of the metadata WAL blob\n\tuint64 rdb_size = 13;\t\t// Size of the RDB blob\n\tuint64 rdb_wal_size = 14;\t// Size of the RDB WAL blob\n\tuint64 usable_bytes = 15;\t// Effective storage available for data\n}\n\nmessage SmdDevReq {}\n\nmessage SmdDevResp {\n\tint32 status = 1;\n\trepeated SmdDevice devices = 2;\n}\n\nmessage SmdPoolReq {}\n\nmessage SmdPoolResp {\n\tmessage Pool {\n\t\tstring uuid = 1; // UUID of VOS pool\n\t\trepeated int32 tgt_ids = 2; // VOS target IDs\n\t\trepeated uint64 blobs = 3; // SPDK blobs\n\t}\n\tint32 status = 1;\n\trepeated Pool pools = 2;\n}\n\nmessage SmdQueryReq {\n\tbool omit_devices = 1;\t\t// Indicate query should omit devices\n\tbool omit_pools = 2;\t\t// Indicate query should omit pools\n\tbool include_bio_health = 3;\t// Indicate query should include BIO health for devices\n\tstring uuid = 4;\t\t// Constrain query to this UUID (pool or device)\n\tuint32 rank = 5;\t\t// Restrict response to only include info about this rank\n}\n\nmessage SmdQueryResp {\n\tmessage SmdDeviceWithHealth {\n\t\tSmdDevice details = 1;\n\t\tBioHealthResp health = 2; // optional BIO health\n\t}\n\tmessage Pool {\n\t\tstring uuid = 1; // UUID of VOS pool\n\t\trepeated int32 tgt_ids = 2; // VOS target IDs\n\t\trepeated uint64 blobs = 3; // SPDK blobs\n\t}\n\tmessage RankResp {\n\t\tuint32 rank = 1; // rank to which this response corresponds\n\t\trepeated SmdDeviceWithHealth devices = 2; // List of devices on the rank\n\t\trepeated Pool pools = 3; // List of pools on the rank\n\t}\n\tint32 status = 1; // DAOS error code\n\trepeated RankResp ranks = 2; // List of per-rank responses\n}\n\nenum LedAction {\n\tGET = 0;\n\tSET = 1;\n\tRESET = 2;\n}\n\nmessage LedManageReq {\n\tstring ids = 1;\t\t\t// List of Device-UUIDs and/or PCI-addresses\n\tLedAction led_action = 3;\t// LED action to perform\n\tLedState led_state = 4;\t\t// LED state to set (used if action is SET)\n\tuint32 led_duration_mins = 5;\t// LED action duration (how long to blink LED in minutes)\n}\n\nmessage DevReplaceReq {\n\tstring old_dev_uuid = 1;\t// UUID of old (hot-removed) blobstore/device\n\tstring new_dev_uuid = 2;\t// UUID of new (hot-plugged) blobstore/device\n\tbool no_reint = 3;\t\t// Skip device reintegration if set\n}\n\nmessage SetFaultyReq {\n\tstring uuid = 1;\t// Device-UUID (as recorded in SMD)\n}\n\nmessage DevManageResp {\n\tint32 status = 1;\t// DAOS error code\n\tSmdDevice device = 2;\t// Details of device that has been managed\n}\n\nmessage SmdManageReq {\n\toneof op{\n\t\tLedManageReq led = 1;\t\t// Request to manage LED state\n\t\tDevReplaceReq replace = 2;\t// Request to replace SMD device\n\t\tSetFaultyReq faulty = 3;\t// Request to set SMD device faulty\n\t}\n}\n\nmessage SmdManageResp {\n\tmessage Result {\n\t\tint32 status = 1;\t\t// DAOS error code\n\t\tSmdDevice device = 2;\n\t}\n\tmessage RankResp {\n\t\tuint32 rank = 1;\t\t// Rank to which this response corresponds\n\t\trepeated Result results = 2;\t// List of device results on the rank\n\t}\n\trepeated RankResp ranks = 1;\t\t// List of per-rank responses\n}\n"
    }
  ]
}