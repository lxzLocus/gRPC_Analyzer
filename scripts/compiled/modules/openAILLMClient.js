/**
 * OpenAI LLM Client
 * OpenAI APIã¨ã®é€šä¿¡ã‚’æ‹…å½“
 */
export class OpenAILLMClient {
    constructor(config, apiKey) {
        this.isInitialized = false;
        this.config = config;
        // ç’°å¢ƒå¤‰æ•°ã‹ã‚‰ APIã‚­ãƒ¼ã‚’å–å¾—
        const finalApiKey = apiKey || process.env.OPENAI_TOKEN || process.env.OPENAI_API_KEY || '';
        console.log(`ğŸ”‘ OpenAILLMClient: Using API key length: ${finalApiKey.length}`);
        console.log(`ğŸ”‘ Available env vars: OPENAI_TOKEN=${!!process.env.OPENAI_TOKEN}, OPENAI_API_KEY=${!!process.env.OPENAI_API_KEY}`);
        console.log(`ğŸ¤– OpenAILLMClient: Using model: ${this.config.get('llm.model', 'gpt-4.1')}`);
        // OpenAIã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã®åˆæœŸåŒ–ã‚’éåŒæœŸã§è¡Œã†
        this.initPromise = this.initializeClient(finalApiKey);
    }
    async initializeClient(apiKey) {
        try {
            // å‹•çš„importã‚’ä½¿ç”¨ã—ã¦ES moduleså¯¾å¿œ
            const { default: OpenAI } = await import('openai');
            this.client = new OpenAI({
                apiKey: apiKey,
                timeout: this.config.get('llm.timeout', 30000)
            });
            this.isInitialized = true;
            console.log('âœ… OpenAI client initialized successfully');
        }
        catch (error) {
            console.error('âŒ Failed to initialize OpenAI client:', error);
            throw error;
        }
    }
    async waitForInitialization() {
        return this.initPromise;
    }
    isReady() {
        return this.isInitialized;
    }
    getProviderName() {
        return 'OpenAI';
    }
    async generateContent(request) {
        if (!this.isInitialized) {
            await this.waitForInitialization();
        }
        try {
            const model = request.model || this.config.get('llm.model', 'gpt-4.1');
            const maxTokens = request.maxTokens || this.config.get('llm.maxTokens', 4000);
            const temperature = request.temperature || this.config.get('llm.temperature', 0.1);
            console.log(`ğŸš€ OpenAI request: model=${model}, maxTokens=${maxTokens}, temperature=${temperature}`);
            const response = await this.client.chat.completions.create({
                model: model,
                messages: request.messages,
                max_tokens: maxTokens,
                temperature: temperature
            });
            const content = response.choices[0]?.message?.content || '';
            const usage = response.usage;
            console.log(`âœ… OpenAI response: ${content.length} chars, usage: ${usage?.total_tokens || 0} tokens`);
            return {
                content,
                usage: usage ? {
                    promptTokens: usage.prompt_tokens,
                    completionTokens: usage.completion_tokens,
                    totalTokens: usage.total_tokens
                } : undefined,
                model: response.model,
                finishReason: response.choices[0]?.finish_reason || 'unknown'
            };
        }
        catch (error) {
            console.error('âŒ OpenAI API error:', error);
            throw error;
        }
    }
}
export default OpenAILLMClient;
//# sourceMappingURL=openAILLMClient.js.map