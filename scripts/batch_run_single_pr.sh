#!/bin/sh
#
# ãƒãƒƒãƒPRå®Ÿè¡Œã‚¹ã‚¯ãƒªãƒ—ãƒˆ
# å„PRã‚’ç‹¬ç«‹ã—ãŸnodeãƒ—ãƒ­ã‚»ã‚¹ã§å®Ÿè¡Œã—ã€æ¥ç¶šã‚¨ãƒ©ãƒ¼ã‚’å›é¿
#
# ä½¿ã„æ–¹: 
#   sh scripts/batch_run_single_pr.sh [dataset_index] [--resume <repo> <cat> <pr>]
#
# ä¾‹:
#   sh scripts/batch_run_single_pr.sh 0  # filtered_fewChangedï¼ˆæœ€åˆã‹ã‚‰ï¼‰
#   sh scripts/batch_run_single_pr.sh 4  # filtered_bugs
#   sh scripts/batch_run_single_pr.sh 0 --resume "boulder" "issue" "pr_title"  # é€”ä¸­ã‹ã‚‰å†é–‹
#
# é€²æ—ç¢ºèª:
#   sh scripts/check_batch_progress.sh  # å‰å›ã®é€²æ—ã‚’ç¢ºèªã—ã¦resumeã‚³ãƒãƒ³ãƒ‰ã‚’è¡¨ç¤º
#

set -e  # ã‚¨ãƒ©ãƒ¼ã§åœæ­¢

# ã‚«ãƒ©ãƒ¼å‡ºåŠ›
RED='\033[0;31m'
GREEN='\033[0;32m'
YELLOW='\033[1;33m'
BLUE='\033[0;34m'
NC='\033[0m' # No Color

# ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆé¸æŠé–¢æ•°ï¼ˆé…åˆ—ã®ä»£ã‚ã‚Šï¼‰
get_dataset_path() {
    case $1 in
        0) echo "/app/dataset/filtered_fewChanged" ;;
        1) echo "/app/dataset/filtered_confirmed" ;;
        2) echo "/app/dataset/filtered_commit" ;;
        3) echo "/app/dataset/filtered_protoChanged" ;;
        4) echo "/app/dataset/filtered_bugs" ;;
        5) echo "/app/dataset/incorrect_few" ;;
        *) echo "" ;;
    esac
}

# ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã¯ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹0ï¼ˆfiltered_fewChangedï¼‰
DATASET_INDEX=${1:-0}

# Resumeæ©Ÿèƒ½ã®ãŸã‚ã®å¤‰æ•°
RESUME_MODE=false
RESUME_REPO=""
RESUME_CATEGORY=""
RESUME_PR=""
RESUME_FOUND=false

# å¼•æ•°è§£æï¼ˆ--resume ã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰
shift_count=1
if [ "$2" = "--resume" ]; then
    RESUME_MODE=true
    RESUME_REPO="$3"
    RESUME_CATEGORY="$4"
    RESUME_PR="$5"
    shift_count=5
    
    if [ -z "$RESUME_REPO" ] || [ -z "$RESUME_CATEGORY" ] || [ -z "$RESUME_PR" ]; then
        echo "${RED}âŒ --resume requires 3 arguments: repository category pr_title${NC}"
        echo "Usage: sh $0 [dataset_index] --resume <repository> <category> <pr_title>"
        exit 1
    fi
    
    echo "${YELLOW}ğŸ”„ Resume mode enabled${NC}"
    echo "${YELLOW}   Will skip until after: ${RESUME_REPO}/${RESUME_CATEGORY}/${RESUME_PR}${NC}"
    echo ""
fi

# ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆé¸æŠã®æ¤œè¨¼
DATASET_DIR=$(get_dataset_path $DATASET_INDEX)

if [ -z "$DATASET_DIR" ]; then
    echo "${RED}âŒ Invalid dataset index: ${DATASET_INDEX}${NC}"
    echo "${BLUE}ğŸ“‚ Available datasets:${NC}"
    echo "   0: /app/dataset/filtered_fewChanged"
    echo "   1: /app/dataset/filtered_confirmed"
    echo "   2: /app/dataset/filtered_commit"
    echo "   3: /app/dataset/filtered_protoChanged"
    echo "   4: /app/dataset/filtered_bugs"
    echo "   5: /app/dataset/incorrect_few"
    exit 1
fi
OUTPUT_BASE="/app/output/batch_$(date +%Y%m%d_%H%M%S)"
LOG_FILE="${OUTPUT_BASE}/batch_execution.log"
SUMMARY_FILE="${OUTPUT_BASE}/batch_summary.json"

# å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªä½œæˆ
mkdir -p "$OUTPUT_BASE"

# ãƒãƒƒãƒå®Ÿè¡Œçµ±è¨ˆ
TOTAL_PRS=0
SUCCESSFUL_PRS=0
FAILED_PRS=0
SKIPPED_PRS=0
START_TIME=$(date +%s)

echo "${GREEN}â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—${NC}"
echo "${GREEN}â•‘         ğŸ”¬ gRPC Analyzer - Batch PR Execution              â•‘${NC}"
echo "${GREEN}â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•${NC}"
echo ""
echo "${BLUE}ğŸ“‚ Dataset: ${DATASET_DIR}${NC}"
echo "${BLUE}ğŸ“ Output: ${OUTPUT_BASE}${NC}"
echo "${BLUE}ğŸ“ Log: ${LOG_FILE}${NC}"
echo "${BLUE}ğŸ› Process ID: $$${NC}"
echo ""

# ãƒ­ã‚°åˆæœŸåŒ–
{
    echo "==================================================="
    echo "Batch PR Execution Started"
    echo "Dataset: ${DATASET_DIR}"
    echo "Output: ${OUTPUT_BASE}"
    echo "Start Time: $(date)"
    echo "==================================================="
    echo ""
} > "$LOG_FILE"

# ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆå†…ã®PRã‚’æ¤œç´¢ã—ã¦å®Ÿè¡Œ
find_and_execute_prs() {
    local dataset="$1"
    
    echo "${YELLOW}ğŸ” Scanning dataset for PRs...${NC}"
    
    # ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆæ§‹é€ : dataset/repository/category/pr_title/
    for repo_dir in "$dataset"/*; do
        [ -d "$repo_dir" ] || continue
        
        repo_name=$(basename "$repo_dir")
        
        for category_dir in "$repo_dir"/*; do
            [ -d "$category_dir" ] || continue
            
            category_name=$(basename "$category_dir")
            
            for pr_dir in "$category_dir"/*; do
                [ -d "$pr_dir" ] || continue
                
                pr_title=$(basename "$pr_dir")
                
                # Resumeæ©Ÿèƒ½: æŒ‡å®šã•ã‚ŒãŸPRã¾ã§ã‚¹ã‚­ãƒƒãƒ—
                if [ "$RESUME_MODE" = true ] && [ "$RESUME_FOUND" = false ]; then
                    if [ "$repo_name" = "$RESUME_REPO" ] && [ "$category_name" = "$RESUME_CATEGORY" ] && [ "$pr_title" = "$RESUME_PR" ]; then
                        echo "${YELLOW}âœ“ Found resume point: ${repo_name}/${category_name}/${pr_title}${NC}" | tee -a "$LOG_FILE"
                        echo "${YELLOW}  Starting from next PR...${NC}" | tee -a "$LOG_FILE"
                        RESUME_FOUND=true
                    fi
                    # ã¾ã resumeãƒã‚¤ãƒ³ãƒˆã«åˆ°é”ã—ã¦ã„ãªã„ã®ã§ã‚¹ã‚­ãƒƒãƒ—
                    continue
                fi
                
                # PRãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªå†…ã«å¿…è¦ãªãƒ•ã‚¡ã‚¤ãƒ«ãŒã‚ã‚‹ã‹ç¢ºèª
                # target.diff, modified.diff ã¾ãŸã¯ 01_proto.txt ãªã©ã®ç•ªå·ä»˜ããƒ•ã‚¡ã‚¤ãƒ«
                if [ ! -f "$pr_dir/target.diff" ] && [ ! -f "$pr_dir/modified.diff" ] && [ ! -f "$pr_dir/01_proto.txt" ]; then
                    echo "${YELLOW}â­ï¸  Skipping (no data files): $repo_name/$category_name/$pr_title${NC}" | tee -a "$LOG_FILE"
                    SKIPPED_PRS=$((SKIPPED_PRS + 1))
                    continue
                fi
                
                TOTAL_PRS=$((TOTAL_PRS + 1))
                
                echo "" | tee -a "$LOG_FILE"
                echo "${BLUE}â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”${NC}" | tee -a "$LOG_FILE"
                echo "${GREEN}ğŸš€ Processing PR #${TOTAL_PRS}${NC}" | tee -a "$LOG_FILE"
                echo "${BLUE}   Repository: ${repo_name}${NC}" | tee -a "$LOG_FILE"
                echo "${BLUE}   Category: ${category_name}${NC}" | tee -a "$LOG_FILE"
                echo "${BLUE}   PR: ${pr_title}${NC}" | tee -a "$LOG_FILE"
                echo "${BLUE}â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”${NC}" | tee -a "$LOG_FILE"
                
                # ä¸€æ™‚çš„ãªãƒ¯ãƒ¼ã‚«ãƒ¼ã‚¹ã‚¯ãƒªãƒ—ãƒˆã‚’ä½œæˆ
                WORKER_SCRIPT="${OUTPUT_BASE}/worker_${repo_name}_${category_name}_${pr_title}.js"
                
                cat > "$WORKER_SCRIPT" << 'EOF'
/**
 * è‡ªå‹•ç”Ÿæˆãƒ¯ãƒ¼ã‚«ãƒ¼ã‚¹ã‚¯ãƒªãƒ—ãƒˆ - å˜ä¸€PRå®Ÿè¡Œ
 */
import path from 'path';
import { config } from 'dotenv';
import { fileURLToPath } from 'url';
import fs from 'fs';

const __filename = fileURLToPath(import.meta.url);
const __dirname = path.dirname(__filename);
config({ path: path.join(__dirname, '..', '.env') });

const TARGET_PR_CONFIG = {
    datasetDir: process.env.DATASET_DIR,
    repositoryName: process.env.REPO_NAME,
    category: process.env.CATEGORY_NAME,
    pullRequestTitle: process.env.PR_TITLE,
    outputDir: process.env.OUTPUT_DIR
};

const PROCESSING_OPTIONS = {
    baseOutputDir: TARGET_PR_CONFIG.outputDir,
    maxRetries: 3,
    memoryCleanupInterval: 5,
    timeoutMs: 15 * 60 * 1000,
    enableGarbageCollection: true,
    enablePreVerification: false,
    forceTUI: false,
    quietMode: true,
    targetPullRequest: {
        repositoryName: TARGET_PR_CONFIG.repositoryName,
        category: TARGET_PR_CONFIG.category,
        pullRequestTitle: TARGET_PR_CONFIG.pullRequestTitle
    }
};

async function main() {
    console.log(`\nğŸ¯ Target PR: ${TARGET_PR_CONFIG.repositoryName}/${TARGET_PR_CONFIG.category}/${TARGET_PR_CONFIG.pullRequestTitle}`);
    
    const prPath = path.join(
        TARGET_PR_CONFIG.datasetDir,
        TARGET_PR_CONFIG.repositoryName,
        TARGET_PR_CONFIG.category,
        TARGET_PR_CONFIG.pullRequestTitle
    );
    
    if (!fs.existsSync(prPath)) {
        console.error(`âŒ PR path does not exist: ${prPath}`);
        process.exit(1);
    }
    
    try {
        const { BatchProcessController } = await import('/app/dist/js/controllers/BatchProcessController.js');
        
        const controller = new BatchProcessController({
            generateReport: false,
            generateErrorReport: false,
            ...PROCESSING_OPTIONS
        });
        
        await controller.runBatchProcessing(TARGET_PR_CONFIG.datasetDir);
        
        console.log('\nâœ… PR processing completed successfully');
        process.exit(0);
    } catch (error) {
        console.error('\nâŒ Critical error:', error.message);
        console.error(error.stack);
        process.exit(1);
    }
}

main().catch(error => {
    console.error('ğŸ’¥ Unhandled error:', error);
    console.error(error.stack);
    process.exit(1);
});
EOF
                
                # ç’°å¢ƒå¤‰æ•°ã‚’è¨­å®šã—ã¦å®Ÿè¡Œ
                export DATASET_DIR="$dataset"
                export REPO_NAME="$repo_name"
                export CATEGORY_NAME="$category_name"
                export PR_TITLE="$pr_title"
                export OUTPUT_DIR="${OUTPUT_BASE}/${repo_name}/${category_name}/${pr_title}"
                
                mkdir -p "$OUTPUT_DIR"
                
                # PRãƒ­ã‚°ãƒ•ã‚¡ã‚¤ãƒ«
                PR_LOG="${OUTPUT_DIR}/execution.log"
                
                # å®Ÿè¡Œæ™‚åˆ»ã‚’è¨˜éŒ²
                PR_START_TIME=$(date +%s)
                echo "   â±ï¸  Started at: $(date)" | tee -a "$LOG_FILE"
                
                # node ãƒ—ãƒ­ã‚»ã‚¹ã‚’å®Ÿè¡Œï¼ˆç‹¬ç«‹ã—ãŸãƒ—ãƒ­ã‚»ã‚¹ï¼‰
                if node "$WORKER_SCRIPT" > "$PR_LOG" 2>&1; then
                    PR_END_TIME=$(date +%s)
                    PR_DURATION=$((PR_END_TIME - PR_START_TIME))
                    
                    SUCCESSFUL_PRS=$((SUCCESSFUL_PRS + 1))
                    echo "${GREEN}   âœ… Success (${PR_DURATION}s)${NC}" | tee -a "$LOG_FILE"
                    
                    # ã‚µãƒãƒªãƒ¼ã«è¿½è¨˜
                    {
                        echo "SUCCESS,$repo_name,$category_name,$pr_title,$PR_DURATION"
                    } >> "${OUTPUT_BASE}/results.csv"
                else
                    PR_END_TIME=$(date +%s)
                    PR_DURATION=$((PR_END_TIME - PR_START_TIME))
                    
                    FAILED_PRS=$((FAILED_PRS + 1))
                    echo "${RED}   âŒ Failed (${PR_DURATION}s)${NC}" | tee -a "$LOG_FILE"
                    echo "   ğŸ“‹ See log: $PR_LOG" | tee -a "$LOG_FILE"
                    
                    # ã‚µãƒãƒªãƒ¼ã«è¿½è¨˜
                    {
                        echo "FAILED,$repo_name,$category_name,$pr_title,$PR_DURATION"
                    } >> "${OUTPUT_BASE}/results.csv"
                fi
                
                # ãƒ¯ãƒ¼ã‚«ãƒ¼ã‚¹ã‚¯ãƒªãƒ—ãƒˆã‚’å‰Šé™¤
                rm -f "$WORKER_SCRIPT"
                
                # ç¾åœ¨ã®çµ±è¨ˆã‚’è¡¨ç¤º
                echo "${YELLOW}   ğŸ“Š Progress: ${SUCCESSFUL_PRS} success, ${FAILED_PRS} failed, ${SKIPPED_PRS} skipped / ${TOTAL_PRS} total${NC}"
                
                # çŸ­ã„å¾…æ©Ÿï¼ˆAPIè² è·è»½æ¸›ï¼‰
                sleep 2
            done
        done
    done
}

# CSV ãƒ˜ãƒƒãƒ€ãƒ¼
echo "status,repository,category,pr_title,duration_seconds" > "${OUTPUT_BASE}/results.csv"

# PRå®Ÿè¡Œ
find_and_execute_prs "$DATASET_DIR"

# çµ‚äº†æ™‚åˆ»ã¨çµ±è¨ˆ
END_TIME=$(date +%s)
TOTAL_DURATION=$((END_TIME - START_TIME))

echo "" | tee -a "$LOG_FILE"
echo "${GREEN}â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—${NC}" | tee -a "$LOG_FILE"
echo "${GREEN}â•‘         ğŸ‰ Batch Execution Completed                       â•‘${NC}" | tee -a "$LOG_FILE"
echo "${GREEN}â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•${NC}" | tee -a "$LOG_FILE"
echo "" | tee -a "$LOG_FILE"
echo "${BLUE}ğŸ“Š Final Statistics:${NC}" | tee -a "$LOG_FILE"
echo "${GREEN}   âœ… Successful: ${SUCCESSFUL_PRS}${NC}" | tee -a "$LOG_FILE"
echo "${RED}   âŒ Failed: ${FAILED_PRS}${NC}" | tee -a "$LOG_FILE"
echo "${YELLOW}   â­ï¸  Skipped: ${SKIPPED_PRS}${NC}" | tee -a "$LOG_FILE"
echo "${BLUE}   ğŸ“ Total PRs: ${TOTAL_PRS}${NC}" | tee -a "$LOG_FILE"

if [ $TOTAL_PRS -gt 0 ]; then
    SUCCESS_RATE=$((SUCCESSFUL_PRS * 100 / TOTAL_PRS))
    echo "${BLUE}   ğŸ“ˆ Success Rate: ${SUCCESS_RATE}%${NC}" | tee -a "$LOG_FILE"
fi

HOURS=$((TOTAL_DURATION / 3600))
MINUTES=$(((TOTAL_DURATION % 3600) / 60))
SECONDS=$((TOTAL_DURATION % 60))

echo "${BLUE}   â±ï¸  Total Duration: ${HOURS}h ${MINUTES}m ${SECONDS}s${NC}" | tee -a "$LOG_FILE"
echo "" | tee -a "$LOG_FILE"
echo "${BLUE}ğŸ“ Results saved to:${NC}" | tee -a "$LOG_FILE"
echo "${BLUE}   - Summary CSV: ${OUTPUT_BASE}/results.csv${NC}" | tee -a "$LOG_FILE"
echo "${BLUE}   - Log: ${LOG_FILE}${NC}" | tee -a "$LOG_FILE"
echo "${BLUE}   - Output: ${OUTPUT_BASE}/${NC}" | tee -a "$LOG_FILE"

# æœ€å¾Œã«å‡¦ç†ã—ãŸPRã‚’è¨˜éŒ²ï¼ˆå†é–‹ç”¨ï¼‰
if [ -n "$repo_name" ] && [ -n "$category_name" ] && [ -n "$pr_title" ]; then
    echo "" | tee -a "$LOG_FILE"
    echo "${YELLOW}ğŸ”„ To resume from next PR (if interrupted):${NC}" | tee -a "$LOG_FILE"
    echo "${YELLOW}   sh $0 $DATASET_INDEX --resume \"$repo_name\" \"$category_name\" \"$pr_title\"${NC}" | tee -a "$LOG_FILE"
fi

# JSON ã‚µãƒãƒªãƒ¼ç”Ÿæˆ
cat > "$SUMMARY_FILE" << EOF
{
  "dataset": "${DATASET_DIR}",
  "startTime": "${START_TIME}",
  "endTime": "${END_TIME}",
  "durationSeconds": ${TOTAL_DURATION},
  "statistics": {
    "total": ${TOTAL_PRS},
    "successful": ${SUCCESSFUL_PRS},
    "failed": ${FAILED_PRS},
    "skipped": ${SKIPPED_PRS}
  },
  "outputDirectory": "${OUTPUT_BASE}",
  "logFile": "${LOG_FILE}",
  "resultsCSV": "${OUTPUT_BASE}/results.csv"
}
EOF

echo ""
echo "${GREEN}âœ¨ Batch execution summary saved to: ${SUMMARY_FILE}${NC}"

# çµæœã«å¿œã˜ã¦çµ‚äº†ã‚³ãƒ¼ãƒ‰
if [ $FAILED_PRS -gt 0 ]; then
    exit 1
else
    exit 0
fi
