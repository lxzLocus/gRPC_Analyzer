/**
 * OpenAI LLM Client with Rate Limit Handling
 * OpenAI APIã¨ã®é€šä¿¡ã‚’æ‹…å½“ï¼ˆãƒ¬ãƒ¼ãƒˆãƒªãƒŸãƒƒãƒˆå¯¾å¿œï¼‰
 */
export class OpenAILLMClient {
    constructor(config, apiKey) {
        this.isInitialized = false;
        this.config = config;
        
        // LLM_PROVIDERãŒopenaiã®å ´åˆã®ã¿ãƒ¬ãƒ¼ãƒˆãƒªãƒŸãƒƒãƒˆæ©Ÿèƒ½ã‚’æœ‰åŠ¹åŒ–
        this.rateLimitEnabled = process.env.LLM_PROVIDER === 'openai';
        
        if (this.rateLimitEnabled) {
            // ãƒ¬ãƒ¼ãƒˆãƒªãƒŸãƒƒãƒˆç®¡ç†
            this.rateLimitState = {
                tokensPerMinute: 0,
                requestsPerMinute: 0,
                tokensPerDay: 0,
                lastResetTime: Date.now(),
                lastDayResetTime: Date.now(),
                retryDelay: 1000, // åˆæœŸãƒªãƒˆãƒ©ã‚¤é…å»¶: 1ç§’
                maxRetryDelay: 60000 // æœ€å¤§ãƒªãƒˆãƒ©ã‚¤é…å»¶: 60ç§’
            };
            
            // ãƒ¢ãƒ‡ãƒ«åˆ¥ã®åˆ¶é™å€¤
            this.modelLimits = {
                'gpt-5': {
                    tpm: 40_000_000,     // 40M tokens per minute (Tier 5)
                    rpm: 15_000,         // 15K requests per minute (Tier 5)
                    tpd: 15_000_000_000  // 15B tokens per day (Batch queue limit)
                },
                'gpt-4.1': {
                    tpm: 30_000_000,     // 30M tokens per minute
                    rpm: 10_000,         // 10K requests per minute
                    tpd: 15_000_000_000  // 15B tokens per day
                },
                'gpt-4o': {
                    tpm: 30_000_000,     // 30M tokens per minute
                    rpm: 10_000,         // 10K requests per minute
                    tpd: 15_000_000_000  // 15B tokens per day
                },
                'gpt-4.1-mini': {
                    tpm: 150_000_000,    // 150M tokens per minute
                    rpm: 30_000,         // 30K requests per minute
                    tpd: 15_000_000_000  // 15B tokens per day
                },
                'gpt-4.1-nano': {
                    tpm: 150_000_000,    // 150M tokens per minute
                    rpm: 30_000,         // 30K requests per minute
                    tpd: 15_000_000_000  // 15B tokens per day
                }
            };
            
            console.log('ğŸ›¡ï¸  Rate limiting enabled for OpenAI provider');
        } else {
            console.log('âš¡ Rate limiting disabled (LLM_PROVIDER is not openai)');
        }
        
        // ç’°å¢ƒå¤‰æ•°ã‹ã‚‰ APIã‚­ãƒ¼ã‚’å–å¾—
        const finalApiKey = apiKey || process.env.OPENAI_TOKEN || process.env.OPENAI_API_KEY || '';
        console.log(`ğŸ”‘ OpenAILLMClient: Using API key length: ${finalApiKey.length}`);
        console.log(`ğŸ”‘ Available env vars: OPENAI_TOKEN=${!!process.env.OPENAI_TOKEN}, OPENAI_API_KEY=${!!process.env.OPENAI_API_KEY}`);
        console.log(`ğŸ¤– OpenAILLMClient: Using model: ${this.config.get('llm.model', 'gpt-4.1')}`);
        console.log(`ğŸ“‹ LLM_PROVIDER: ${process.env.LLM_PROVIDER || 'undefined'}`);
        
        // OpenAIã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã®åˆæœŸåŒ–ã‚’éåŒæœŸã§è¡Œã†
        this.initPromise = this.initializeClient(finalApiKey);
    }
    async initializeClient(apiKey) {
        try {
            // å‹•çš„importã‚’ä½¿ç”¨ã—ã¦ES moduleså¯¾å¿œ
            const { default: OpenAI } = await import('openai');
            this.client = new OpenAI({
                apiKey: apiKey,
                timeout: this.config.get('llm.timeout', 120000), // 2åˆ†ã«å»¶é•·
                maxRetries: this.rateLimitEnabled ? 0 : 3  // ãƒ¬ãƒ¼ãƒˆãƒªãƒŸãƒƒãƒˆæœ‰åŠ¹æ™‚ã¯è‡ªå‰ã®ãƒªãƒˆãƒ©ã‚¤ã€ç„¡åŠ¹æ™‚ã¯OpenAIæ¨™æº–ãƒªãƒˆãƒ©ã‚¤
            });
            this.isInitialized = true;
            console.log('âœ… OpenAI client initialized successfully');
            if (this.rateLimitEnabled) {
                console.log('ğŸ›¡ï¸  Rate limiting enabled for gpt-4.1 and gpt-4o models');
            }
        }
        catch (error) {
            console.error('âŒ Failed to initialize OpenAI client:', error);
            throw error;
        }
    }
    
    /**
     * ãƒ¬ãƒ¼ãƒˆãƒªãƒŸãƒƒãƒˆçŠ¶æ…‹ã‚’ãƒªã‚»ãƒƒãƒˆ
     */
    resetRateLimitCounters() {
        if (!this.rateLimitEnabled) return;
        
        const now = Date.now();
        
        // 1åˆ†ã”ã¨ã®ãƒªã‚»ãƒƒãƒˆ
        if (now - this.rateLimitState.lastResetTime >= 60000) {
            this.rateLimitState.tokensPerMinute = 0;
            this.rateLimitState.requestsPerMinute = 0;
            this.rateLimitState.lastResetTime = now;
            console.log('ğŸ”„ Rate limit counters reset (1 minute)');
        }
        
        // 1æ—¥ã”ã¨ã®ãƒªã‚»ãƒƒãƒˆ
        if (now - this.rateLimitState.lastDayResetTime >= 86400000) {
            this.rateLimitState.tokensPerDay = 0;
            this.rateLimitState.lastDayResetTime = now;
            console.log('ğŸ”„ Daily rate limit counters reset');
        }
    }
    
    /**
     * ãƒ¬ãƒ¼ãƒˆãƒªãƒŸãƒƒãƒˆã‚’ãƒã‚§ãƒƒã‚¯
     */
    checkRateLimit(model, estimatedTokens) {
        if (!this.rateLimitEnabled) {
            return { shouldWait: false, waitTime: 0 };
        }
        
        this.resetRateLimitCounters();
        
        const limits = this.modelLimits[model] || this.modelLimits['gpt-4.1'];
        
        // ãƒªã‚¯ã‚¨ã‚¹ãƒˆæ•°åˆ¶é™ãƒã‚§ãƒƒã‚¯
        if (this.rateLimitState.requestsPerMinute >= limits.rpm) {
            const waitTime = 60000 - (Date.now() - this.rateLimitState.lastResetTime);
            console.warn(`âš ï¸  Request rate limit reached for ${model}. Waiting ${Math.ceil(waitTime/1000)}s`);
            return { shouldWait: true, waitTime: Math.max(waitTime, 1000) };
        }
        
        // ãƒˆãƒ¼ã‚¯ãƒ³æ•°åˆ¶é™ãƒã‚§ãƒƒã‚¯ï¼ˆåˆ†ï¼‰
        if (this.rateLimitState.tokensPerMinute + estimatedTokens > limits.tpm) {
            const waitTime = 60000 - (Date.now() - this.rateLimitState.lastResetTime);
            console.warn(`âš ï¸  Token rate limit (TPM) reached for ${model}. Waiting ${Math.ceil(waitTime/1000)}s`);
            return { shouldWait: true, waitTime: Math.max(waitTime, 1000) };
        }
        
        // ãƒˆãƒ¼ã‚¯ãƒ³æ•°åˆ¶é™ãƒã‚§ãƒƒã‚¯ï¼ˆæ—¥ï¼‰
        if (this.rateLimitState.tokensPerDay + estimatedTokens > limits.tpd) {
            console.error(`âŒ Daily token limit (TPD) reached for ${model}. Cannot proceed.`);
            throw new Error(`Daily token limit exceeded for model ${model}`);
        }
        
        return { shouldWait: false, waitTime: 0 };
    }
    
    /**
     * ãƒ¬ãƒ¼ãƒˆãƒªãƒŸãƒƒãƒˆä½¿ç”¨é‡ã‚’æ›´æ–°
     */
    updateRateLimitUsage(tokens) {
        if (!this.rateLimitEnabled) return;
        
        this.rateLimitState.tokensPerMinute += tokens;
        this.rateLimitState.tokensPerDay += tokens;
        this.rateLimitState.requestsPerMinute += 1;
    }
    
    /**
     * æŒ‡æ•°ãƒãƒƒã‚¯ã‚ªãƒ•ã«ã‚ˆã‚‹é…å»¶è¨ˆç®—
     */
    calculateBackoffDelay(retryCount) {
        if (!this.rateLimitEnabled) {
            // ãƒ¬ãƒ¼ãƒˆãƒªãƒŸãƒƒãƒˆç„¡åŠ¹æ™‚ã¯å˜ç´”ãªç·šå½¢é…å»¶
            return Math.min(2000 * (retryCount + 1), 10000);
        }
        
        const baseDelay = this.rateLimitState.retryDelay;
        const backoffDelay = Math.min(
            baseDelay * Math.pow(2, retryCount),
            this.rateLimitState.maxRetryDelay
        );
        
        // ã‚¸ãƒƒã‚¿ãƒ¼ã‚’è¿½åŠ ï¼ˆ25%ã®ãƒ©ãƒ³ãƒ€ãƒ å¤‰å‹•ï¼‰
        const jitter = backoffDelay * 0.25 * Math.random();
        return Math.floor(backoffDelay + jitter);
    }
    
    /**
     * é…å»¶å®Ÿè¡Œ
     */
    async delay(ms) {
        return new Promise(resolve => setTimeout(resolve, ms));
    }
    async waitForInitialization() {
        return this.initPromise;
    }
    isReady() {
        return this.isInitialized;
    }
    getProviderName() {
        return 'OpenAI';
    }
    async generateContent(request) {
        if (!this.isInitialized) {
            await this.waitForInitialization();
        }
        
        const model = request.model || this.config.get('llm.model', 'gpt-4.1');
        const maxTokens = request.maxTokens || this.config.get('llm.maxTokens', 4000);
        const temperature = request.temperature || this.config.get('llm.temperature', 0.1);
        
        console.log(`ğŸš€ OpenAI request: model=${model}, maxTokens=${maxTokens}, temperature=${temperature}, rateLimitEnabled=${this.rateLimitEnabled}`);
        
        // ãƒ¬ãƒ¼ãƒˆãƒªãƒŸãƒƒãƒˆæœ‰åŠ¹æ™‚ã®é«˜åº¦ãªå‡¦ç†
        if (this.rateLimitEnabled) {
            return await this.generateContentWithRateLimit(request, model, maxTokens, temperature);
        }
        
        // ãƒ¬ãƒ¼ãƒˆãƒªãƒŸãƒƒãƒˆç„¡åŠ¹æ™‚ã®å¾“æ¥å‡¦ç†ï¼ˆã‚·ãƒ³ãƒ—ãƒ«ãªãƒªãƒˆãƒ©ã‚¤ã®ã¿ï¼‰
        return await this.generateContentSimple(request, model, maxTokens, temperature);
    }
    
    /**
     * ãƒ¬ãƒ¼ãƒˆãƒªãƒŸãƒƒãƒˆå¯¾å¿œã®ç”Ÿæˆå‡¦ç†
     */
    async generateContentWithRateLimit(request, model, maxTokens, temperature) {
        // ãƒˆãƒ¼ã‚¯ãƒ³æ•°ã‚’æ¨å®šï¼ˆç°¡æ˜“è¨ˆç®—: 1ãƒˆãƒ¼ã‚¯ãƒ³ â‰ˆ 4æ–‡å­—ï¼‰
        const messageText = request.messages.map(m => m.content).join(' ');
        const estimatedInputTokens = Math.ceil(messageText.length / 4);
        const estimatedTotalTokens = estimatedInputTokens + maxTokens;
        
        let retryCount = 0;
        const maxRetries = 5;
        
        while (retryCount <= maxRetries) {
            try {
                // ãƒ¬ãƒ¼ãƒˆãƒªãƒŸãƒƒãƒˆãƒã‚§ãƒƒã‚¯
                const rateLimitCheck = this.checkRateLimit(model, estimatedTotalTokens);
                if (rateLimitCheck.shouldWait) {
                    console.log(`â³ Waiting ${Math.ceil(rateLimitCheck.waitTime/1000)}s due to rate limit...`);
                    await this.delay(rateLimitCheck.waitTime);
                    continue; // ãƒ¬ãƒ¼ãƒˆãƒªãƒŸãƒƒãƒˆå›é¿å¾Œã«å†è©¦è¡Œ
                }
                
                // APIå‘¼ã³å‡ºã—å®Ÿè¡Œ
                const response = await this.client.chat.completions.create({
                    model: model,
                    messages: request.messages,
                    max_tokens: maxTokens,
                    temperature: temperature
                });
                
                const content = response.choices[0]?.message?.content || '';
                const usage = response.usage;
                
                // ãƒ¬ãƒ¼ãƒˆãƒªãƒŸãƒƒãƒˆä½¿ç”¨é‡ã‚’æ›´æ–°
                if (usage) {
                    this.updateRateLimitUsage(usage.total_tokens);
                }
                
                // ãƒªãƒˆãƒ©ã‚¤é…å»¶ã‚’ãƒªã‚»ãƒƒãƒˆï¼ˆæˆåŠŸæ™‚ï¼‰
                this.rateLimitState.retryDelay = 1000;
                
                console.log(`âœ… OpenAI response: ${content.length} chars, usage: ${usage?.total_tokens || 0} tokens`);
                console.log(`ğŸ“Š Rate limit usage: ${this.rateLimitState.requestsPerMinute} RPM, ${this.rateLimitState.tokensPerMinute} TPM, ${this.rateLimitState.tokensPerDay} TPD`);
                
                return {
                    content,
                    usage: usage ? {
                        promptTokens: usage.prompt_tokens,
                        completionTokens: usage.completion_tokens,
                        totalTokens: usage.total_tokens
                    } : undefined,
                    model: response.model,
                    finishReason: response.choices[0]?.finish_reason || 'unknown'
                };
                
            } catch (error) {
                retryCount++;
                
                // ãƒ¬ãƒ¼ãƒˆãƒªãƒŸãƒƒãƒˆã‚¨ãƒ©ãƒ¼ã®å‡¦ç†
                if (error.status === 429) {
                    const retryAfter = error.headers?.['retry-after'];
                    let waitTime;
                    
                    if (retryAfter) {
                        waitTime = parseInt(retryAfter) * 1000; // ç§’ã‚’ãƒŸãƒªç§’ã«å¤‰æ›
                        console.warn(`ğŸš« Rate limit hit (429). Retry-After: ${retryAfter}s`);
                    } else {
                        waitTime = this.calculateBackoffDelay(retryCount - 1);
                        console.warn(`ğŸš« Rate limit hit (429). Using exponential backoff: ${Math.ceil(waitTime/1000)}s`);
                    }
                    
                    if (retryCount <= maxRetries) {
                        console.log(`â³ Waiting ${Math.ceil(waitTime/1000)}s before retry ${retryCount}/${maxRetries}...`);
                        await this.delay(waitTime);
                        
                        // ãƒªãƒˆãƒ©ã‚¤é…å»¶ã‚’å¢—åŠ 
                        this.rateLimitState.retryDelay = Math.min(
                            this.rateLimitState.retryDelay * 1.5,
                            this.rateLimitState.maxRetryDelay
                        );
                        continue;
                    }
                }
                
                // ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆã‚¨ãƒ©ãƒ¼ã®å‡¦ç†
                if (error.code === 'ECONNABORTED' || error.message?.includes('timeout')) {
                    console.error(`â° Request timeout for model ${model} (attempt ${retryCount}/${maxRetries})`);
                    
                    if (retryCount <= maxRetries) {
                        const waitTime = this.calculateBackoffDelay(retryCount - 1);
                        console.log(`â³ Retrying after ${Math.ceil(waitTime/1000)}s...`);
                        await this.delay(waitTime);
                        continue;
                    }
                }
                
                // ãã®ä»–ã®ã‚¨ãƒ©ãƒ¼ã¾ãŸã¯æœ€å¤§ãƒªãƒˆãƒ©ã‚¤å›æ•°ã«é”ã—ãŸå ´åˆ
                console.error(`âŒ OpenAI API error (attempt ${retryCount}/${maxRetries + 1}):`, error.message || error);
                
                if (retryCount > maxRetries) {
                    console.error(`âŒ Max retries (${maxRetries}) exceeded for OpenAI API`);
                    throw error;
                }
                
                // ä¸€èˆ¬çš„ãªã‚¨ãƒ©ãƒ¼ã®å ´åˆã¯çŸ­ã„é…å»¶å¾Œã«ãƒªãƒˆãƒ©ã‚¤
                if (retryCount <= maxRetries) {
                    const waitTime = Math.min(2000 * retryCount, 10000); // 2ç§’, 4ç§’, 6ç§’, 8ç§’, 10ç§’
                    console.log(`â³ Retrying after ${waitTime/1000}s...`);
                    await this.delay(waitTime);
                }
            }
        }
    }
    
    /**
     * ã‚·ãƒ³ãƒ—ãƒ«ãªç”Ÿæˆå‡¦ç†ï¼ˆãƒ¬ãƒ¼ãƒˆãƒªãƒŸãƒƒãƒˆç„¡åŠ¹æ™‚ï¼‰
     */
    async generateContentSimple(request, model, maxTokens, temperature) {
        try {
            const response = await this.client.chat.completions.create({
                model: model,
                messages: request.messages,
                max_tokens: maxTokens,
                temperature: temperature
            });
            
            const content = response.choices[0]?.message?.content || '';
            const usage = response.usage;
            
            console.log(`âœ… OpenAI response: ${content.length} chars, usage: ${usage?.total_tokens || 0} tokens`);
            
            return {
                content,
                usage: usage ? {
                    promptTokens: usage.prompt_tokens,
                    completionTokens: usage.completion_tokens,
                    totalTokens: usage.total_tokens
                } : undefined,
                model: response.model,
                finishReason: response.choices[0]?.finish_reason || 'unknown'
            };
            
        } catch (error) {
            console.error('âŒ OpenAI API error:', error.message || error);
            throw error;
        }
    }
}
export default OpenAILLMClient;
//# sourceMappingURL=openAILLMClient.js.map