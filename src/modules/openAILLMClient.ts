/**
 * OpenAI LLM Client
 * OpenAI APIã¨ã®é€šä¿¡ã‚’æ‹…å½“
 */

import Config from './Config.js';
import { LLMClient, LLMRequest, LLMResponse } from './llmClient.js';

export class OpenAILLMClient implements LLMClient {
    private client: any;
    private initPromise: Promise<void>;
    private config: Config;
    private isInitialized: boolean = false;
    private httpAgent: any = null;
    private restHttpAgent: any = null;
    private agentRecycleInterval: NodeJS.Timeout | null = null;
    private readonly AGENT_LIFETIME_MS = 30 * 60 * 1000; // 30åˆ†

    constructor(config: Config, apiKey?: string) {
        this.config = config;
        
        // ç’°å¢ƒå¤‰æ•°ã‹ã‚‰ APIã‚­ãƒ¼ã‚’å–å¾—
        const finalApiKey = apiKey || process.env.OPENAI_API_KEY || '';
        
        console.log(`ğŸ”‘ OpenAILLMClient: Using API key length: ${finalApiKey.length}`);
        console.log(`ğŸ”‘ Available env vars: OPENAI_API_KEY=${!!process.env.OPENAI_API_KEY}`);
        console.log(`ğŸ¤– OpenAILLMClient: Using model: ${this.config.get('llm.model', this.config.get('openai.model', 'gpt-4o'))}`);
        
        // OpenAIã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã®åˆæœŸåŒ–ã‚’éåŒæœŸã§è¡Œã†
        this.initPromise = this.initializeClient(finalApiKey);
        
        // HTTP Agent è‡ªå‹•ãƒªã‚µã‚¤ã‚¯ãƒ«ã‚¿ã‚¤ãƒãƒ¼ã‚’é–‹å§‹
        this.startAgentRecycling();
    }

    /**
     * HTTP Agentè‡ªå‹•ãƒªã‚µã‚¤ã‚¯ãƒ«æ©Ÿèƒ½
     * 30åˆ†ã”ã¨ã«Agentã‚’ç ´æ£„ã—ã¦æ–°è¦ä½œæˆã™ã‚‹ã“ã¨ã§ã€ã‚½ã‚±ãƒƒãƒˆæ¯æ¸‡ã¨NATã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆã‚’å›é¿
     */
    private startAgentRecycling(): void {
        // åˆå›Agentä½œæˆ
        this.recycleAgents();
        
        // 30åˆ†ã”ã¨ã«è‡ªå‹•ãƒªã‚µã‚¤ã‚¯ãƒ«
        this.agentRecycleInterval = setInterval(() => {
            console.log('ğŸ”„ [Agent Recycle] 30åˆ†çµŒé: HTTP Agentã‚’å†ä½œæˆã—ã¾ã™');
            this.recycleAgents();
        }, this.AGENT_LIFETIME_MS);
        
        console.log(`â™»ï¸  HTTP Agent auto-recycle enabled (interval: 30 minutes)`);
    }

    /**
     * HTTP Agentã®ç ´æ£„ã¨å†ä½œæˆ
     */
    private async recycleAgents(): Promise<void> {
        try {
            const https = await import('https');
            
            // æ—¢å­˜ã®AgentãŒã‚ã‚Œã°ç ´æ£„
            if (this.httpAgent) {
                this.httpAgent.destroy();
                console.log('ğŸ—‘ï¸  Old OpenAI SDK Agent destroyed');
            }
            if (this.restHttpAgent) {
                this.restHttpAgent.destroy();
                console.log('ğŸ—‘ï¸  Old REST API Agent destroyed');
            }
            
            // æ–°è¦Agentã‚’ä½œæˆ
            this.httpAgent = new https.default.Agent({
                keepAlive: true,        // 30åˆ†ä»¥å†…ã¯å†åˆ©ç”¨
                keepAliveMsecs: 1000,   // keep-aliveã‚½ã‚±ãƒƒãƒˆã®åˆæœŸé…å»¶
                maxSockets: 10,         // ä¸¦åˆ—æ¥ç¶šæ•°ä¸Šé™
                maxFreeSockets: 5,      // ãƒ—ãƒ¼ãƒ«ã«ä¿æŒã™ã‚‹ã‚½ã‚±ãƒƒãƒˆæ•°
                timeout: 60000,         // ã‚½ã‚±ãƒƒãƒˆã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆ
            });
            
            this.restHttpAgent = new https.default.Agent({
                keepAlive: true,
                keepAliveMsecs: 1000,
                maxSockets: 10,
                maxFreeSockets: 5,
                timeout: 60000,
            });
            
            console.log('âœ¨ New HTTP Agents created (lifetime: 30 minutes)');
            
        } catch (error) {
            console.error('âŒ Failed to recycle HTTP Agents:', error);
        }
    }

    private async initializeClient(apiKey: string): Promise<void> {
        try {
            // å‹•çš„importã‚’ä½¿ç”¨ã—ã¦ES moduleså¯¾å¿œ
            const { default: OpenAI } = await import('openai');
            
            // ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆè¨­å®šã‚’çµ±ä¸€ï¼šopenai.timeout -> llm.timeout ã®é †ã§å–å¾—
            const timeoutMs = this.config.get('openai.timeout', this.config.get('llm.timeout', 120000));
            console.log(`ğŸ•’ OpenAI client timeout set to: ${timeoutMs}ms`);
            
            // AgentãŒæœªä½œæˆã®å ´åˆã¯ä½œæˆ
            if (!this.httpAgent) {
                await this.recycleAgents();
            }
            
            this.client = new OpenAI({
                apiKey: apiKey,
                timeout: timeoutMs,
                // ã‚½ã‚±ãƒƒãƒˆæ¥ç¶šã®å®‰å®šæ€§å‘ä¸Šè¨­å®š
                maxRetries: 0,  // ãƒ©ã‚¤ãƒ–ãƒ©ãƒªå´ã®ãƒªãƒˆãƒ©ã‚¤ã¯ç„¡åŠ¹åŒ–ï¼ˆã‚¢ãƒ—ãƒªå´ã§åˆ¶å¾¡ï¼‰
                httpAgent: this.httpAgent,  // 30åˆ†ã”ã¨ã«å†ä½œæˆã•ã‚Œã‚‹Agent
            });
            
            this.isInitialized = true;
            console.log('âœ… OpenAI client initialized successfully with managed HTTP Agent');
        } catch (error) {
            console.error('âŒ Failed to initialize OpenAI client:', error);
            throw error;
        }
    }

    async waitForInitialization(): Promise<void> {
        return this.initPromise;
    }

    isReady(): boolean {
        return this.isInitialized;
    }

    getProviderName(): string {
        return 'OpenAI';
    }

    async generateContent(request: LLMRequest): Promise<LLMResponse> {
        if (!this.isInitialized) {
            await this.waitForInitialization();
        }

        // REST APIãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ãŒæŒ‡å®šã•ã‚Œã¦ã„ã‚‹å ´åˆ
        if (process.env.USE_OPENAI_REST_FALLBACK === 'true') {
            console.log('ğŸ”„ Using OpenAI REST API fallback (direct HTTP request)...');
            return this.generateContentViaRestApi(request);
        }

        // ãƒ¢ãƒ‡ãƒ«åã‚’äº‹å‰ã«å–å¾—ï¼ˆã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°ã§ã‚‚ä½¿ç”¨ï¼‰
        const model = request.model || this.config.get('llm.model', this.config.get('openai.model', 'gpt-4o'));
        const maxTokens = request.maxTokens || this.config.get('llm.maxTokens', 4000);
        
        try {
            console.log(`ğŸš€ OpenAI request: model=${model}, maxTokens=${maxTokens}`);

            // APIãƒªã‚¯ã‚¨ã‚¹ãƒˆãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’æº–å‚™
            const apiParams: any = {
                model: model,
                messages: request.messages
            };
            
            // gpt-5ç³»ãƒ¢ãƒ‡ãƒ«ä»¥å¤–ã®å ´åˆã®ã¿max_completion_tokensã‚’è¨­å®š
            const isGpt5Series = model.startsWith('gpt-5');
            if (!isGpt5Series && maxTokens) {
                apiParams.max_completion_tokens = maxTokens;
                console.log(`ğŸ”¢ OpenAI max_completion_tokens: ${maxTokens}`);
            } else if (isGpt5Series) {
                console.log(`â„¹ï¸  ${model}: max_completion_tokensãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã¯ã‚¹ã‚­ãƒƒãƒ—ã•ã‚Œã¾ã™`);
            }

            // gpt-5ç³»ãƒ¢ãƒ‡ãƒ«ä»¥å¤–ã®å ´åˆã®ã¿temperatureã‚’è¨­å®š
            if (!isGpt5Series && request.temperature !== undefined) {
                apiParams.temperature = request.temperature;
                console.log(`ğŸŒ¡ï¸  OpenAI temperature: ${request.temperature}`);
            } else if (isGpt5Series) {
                console.log(`â„¹ï¸  ${model}: temperatureãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã¯ã‚¹ã‚­ãƒƒãƒ—ã•ã‚Œã¾ã™`);
            }

            // ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆä»˜ãã®APIå‘¼ã³å‡ºã—
            // ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆåˆæœŸåŒ–æ™‚ã¨åŒã˜ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆå€¤ã‚’ä½¿ç”¨
            const apiTimeout = this.config.get('openai.timeout', this.config.get('llm.timeout', 120000));
            console.log(`ğŸ•’ OpenAI API call timeout: ${apiTimeout}ms`);
            
            // ã‚½ã‚±ãƒƒãƒˆæ¥ç¶šã‚¨ãƒ©ãƒ¼å¯¾ç­–ï¼šãƒªã‚¯ã‚¨ã‚¹ãƒˆå‰ã«çŸ­ã„é…å»¶ã‚’å…¥ã‚Œã‚‹
            // é€£ç¶šãƒªã‚¯ã‚¨ã‚¹ãƒˆã«ã‚ˆã‚‹ã‚½ã‚±ãƒƒãƒˆæ¯æ¸‡ã‚’é˜²ã
            await new Promise(resolve => setTimeout(resolve, 100));
            
            const apiCall = this.client.chat.completions.create(apiParams);

            const timeoutPromise = new Promise((_, reject) =>
                setTimeout(() => reject(new Error(`OpenAI API timeout after ${apiTimeout}ms`)), apiTimeout)
            );

            const response = await Promise.race([apiCall, timeoutPromise]);

            const content = response.choices[0]?.message?.content || '';
            const usage = response.usage;

            console.log(`âœ… OpenAI response: ${content.length} chars, usage: ${usage?.total_tokens || 0} tokens`);

            return {
                content: content,
                usage: {
                    promptTokens: usage?.prompt_tokens || 0,
                    completionTokens: usage?.completion_tokens || 0,
                    totalTokens: usage?.total_tokens || 0
                }
            };
        } catch (error: any) {
            // OpenAIå°‚ç”¨ã‚¨ãƒ©ãƒ¼ã‚¯ãƒ©ã‚¹ã«ã‚ˆã‚‹è©³ç´°ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°
            const errorName = error?.constructor?.name || error?.name || 'UnknownError';
            
            console.error('âŒ OpenAI API error:', {
                type: errorName,
                message: error.message,
                status: error.status,
                code: error.code,
                type_detail: error.type
            });
            
            // 1. APIConnectionError - æ¥ç¶šã‚¨ãƒ©ãƒ¼
            if (errorName === 'APIConnectionError' || 
                error.code === 'ECONNREFUSED' || 
                error.code === 'ENOTFOUND' || 
                error.code === 'ETIMEDOUT') {
                
                console.error('\nğŸ”¥ API CONNECTION ERROR ğŸ”¥');
                console.error('â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”');
                console.error('Cannot connect to OpenAI API server.');
                console.error('\nğŸ“‹ Error Details:');
                console.error(`  Message: ${error.message}`);
                console.error(`  Code: ${error.code || 'N/A'}`);
                console.error('\nğŸ” Possible Causes:');
                console.error('  1. ğŸŒ No internet connection');
                console.error('  2. ğŸ”’ Firewall/proxy blocking api.openai.com');
                console.error('  3. ğŸ³ Docker network misconfiguration');
                console.error('  4. ğŸš« OpenAI service temporarily unavailable');
                console.error('\nğŸ’¡ Solutions:');
                console.error('  â€¢ Test connection: curl -I https://api.openai.com/v1/models');
                console.error('  â€¢ Run diagnostics: sh /app/scripts/diagnose_connection.sh');
                console.error('  â€¢ Switch to Gemini: Set GEMINI_API_KEY, provider=gemini');
                console.error('  â€¢ Use local LLM: sh /app/scripts/switch_to_local_llm.sh');
                console.error('â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n');
                
                throw new Error(`API Connection Error: ${error.message}. Check network settings.`);
            }
            
            // 2. UnauthorizedError (401) - èªè¨¼ã‚¨ãƒ©ãƒ¼
            if (errorName === 'UnauthorizedError' || error.status === 401) {
                console.error('\nğŸ”‘ AUTHENTICATION ERROR ğŸ”‘');
                console.error('â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”');
                console.error('OpenAI API authentication failed.');
                console.error('\nğŸ” Possible Causes:');
                console.error('  1. âŒ Invalid or expired API key');
                console.error('  2. âŒ API key not set in environment variables');
                console.error('  3. âŒ Wrong API key format');
                console.error('\nğŸ’¡ Solutions:');
                console.error('  â€¢ Check OPENAI_API_KEY in .env file');
                console.error('  â€¢ Verify key at: https://platform.openai.com/api-keys');
                console.error('  â€¢ Regenerate API key if expired');
                console.error('â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n');
                
                throw new Error(`Authentication failed: Invalid or missing API key.`);
            }
            
            // 3. RateLimitError (429) - ãƒ¬ãƒ¼ãƒˆåˆ¶é™
            if (errorName === 'RateLimitError' || error.status === 429) {
                console.error('\nâ±ï¸  RATE LIMIT EXCEEDED â±ï¸');
                console.error('â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”');
                console.error('OpenAI API rate limit exceeded.');
                console.error('\nğŸ’¡ This is automatically retried by the system.');
                console.error('â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n');
                
                throw new Error(`Rate limit exceeded. Retrying with backoff...`);
            }
            
            // 4. APITimeoutError (408) - ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆ
            if (errorName === 'APITimeoutError' || error.status === 408 || error.code === 'ETIMEDOUT') {
                console.error('\nâ° API TIMEOUT â°');
                console.error('â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”');
                console.error('OpenAI API request timed out.');
                console.error(`  Timeout setting: ${this.config.get('llm.timeout', 120000)}ms`);
                console.error('\nğŸ’¡ This is automatically retried by the system.');
                console.error('â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n');
                
                throw new Error(`API timeout. Retrying...`);
            }
            
            // 5. BadRequestError (400) - ãƒˆãƒ¼ã‚¯ãƒ³è¶…éã€ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼
            if (errorName === 'BadRequestError' || error.status === 400) {
                console.error('\nâš ï¸  BAD REQUEST âš ï¸');
                console.error('â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”');
                console.error('Invalid request to OpenAI API.');
                console.error(`  Message: ${error.message}`);
                
                // ãƒˆãƒ¼ã‚¯ãƒ³è¶…éã®å¯èƒ½æ€§ã‚’ãƒã‚§ãƒƒã‚¯
                if (error.message?.includes('token') || error.message?.includes('context_length')) {
                    console.error('\nğŸ” Likely Cause: Token limit exceeded');
                    console.error('ğŸ’¡ The system will automatically summarize and retry.');
                } else if (error.message?.includes('content_filter')) {
                    console.error('\nğŸ” Likely Cause: Content filtered by safety system');
                    console.error('ğŸ’¡ Request contains inappropriate content.');
                }
                console.error('â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n');
                
                throw new Error(`Bad request: ${error.message}`);
            }
            
            // 6. NotFoundError (404) - ãƒ¢ãƒ‡ãƒ«è¦‹ã¤ã‹ã‚‰ãš
            if (errorName === 'NotFoundError' || error.status === 404) {
                console.error('\nğŸ” MODEL NOT FOUND ğŸ”');
                console.error('â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”');
                console.error('The specified model was not found.');
                console.error(`  Model: ${model}`);
                console.error('\nğŸ’¡ Solutions:');
                console.error('  â€¢ Check model name in config_openai.json');
                console.error('  â€¢ Verify model availability in your OpenAI account');
                console.error('  â€¢ Use standard model: gpt-4o, gpt-4-turbo, gpt-3.5-turbo');
                console.error('â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n');
                
                throw new Error(`Model not found: ${model}`);
            }
            
            // 7. InternalServerError (500) - ã‚µãƒ¼ãƒãƒ¼ã‚¨ãƒ©ãƒ¼
            if (errorName === 'InternalServerError' || error.status === 500) {
                console.error('\nğŸ”¥ OPENAI SERVER ERROR ğŸ”¥');
                console.error('â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”');
                console.error('OpenAI service is experiencing issues.');
                console.error('\nğŸ’¡ This is automatically retried by the system.');
                console.error('  Check status: https://status.openai.com/');
                console.error('â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n');
                
                throw new Error(`OpenAI server error. Retrying...`);
            }
            
            // 8. ãã®ä»–ã®ã‚¨ãƒ©ãƒ¼
            console.error('\nâŒ UNEXPECTED ERROR âŒ');
            console.error('â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”');
            console.error(`  Type: ${errorName}`);
            console.error(`  Message: ${error.message}`);
            console.error(`  Status: ${error.status || 'N/A'}`);
            console.error('â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n');
            
            throw error;
        }
    }

    /**
     * REST APIç›´æ¥å‘¼ã³å‡ºã—ã«ã‚ˆã‚‹ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯å®Ÿè£…
     */
    private async generateContentViaRestApi(request: LLMRequest): Promise<LLMResponse> {
        try {
            const axios = await import('axios');
            const axiosInstance = axios.default;
            
            const model = request.model || this.config.get('llm.model', this.config.get('openai.model', 'gpt-4o'));
            const maxTokens = request.maxTokens || this.config.get('llm.maxTokens', 4000);
            const apiKey = process.env.OPENAI_API_KEY || '';
            
            if (!apiKey) {
                throw new Error('OPENAI_API_KEY is not set');
            }
            
            console.log(`ğŸŒ REST API request: model=${model}, maxTokens=${maxTokens}`);
            
            // ãƒªã‚¯ã‚¨ã‚¹ãƒˆãƒœãƒ‡ã‚£ã®æ§‹ç¯‰
            const requestBody: any = {
                model: model,
                messages: request.messages
            };
            
            // gpt-5ç³»ãƒ¢ãƒ‡ãƒ«ä»¥å¤–ã®å ´åˆã®ã¿max_tokensã‚’è¨­å®š
            const isGpt5Series = model.startsWith('gpt-5');
            if (!isGpt5Series && maxTokens) {
                requestBody.max_completion_tokens = maxTokens;
            }
            
            // gpt-5ç³»ãƒ¢ãƒ‡ãƒ«ä»¥å¤–ã®å ´åˆã®ã¿temperatureã‚’è¨­å®š
            if (!isGpt5Series && request.temperature !== undefined) {
                requestBody.temperature = request.temperature;
            }
            
            const timeout = this.config.get('llm.timeout', 120000);
            
            // ã‚½ã‚±ãƒƒãƒˆæ¥ç¶šã‚¨ãƒ©ãƒ¼å¯¾ç­–ï¼šãƒªã‚¯ã‚¨ã‚¹ãƒˆå‰ã«çŸ­ã„é…å»¶ã‚’å…¥ã‚Œã‚‹
            await new Promise(resolve => setTimeout(resolve, 100));
            
            // OpenAI APIã¸ã®HTTPãƒªã‚¯ã‚¨ã‚¹ãƒˆï¼ˆæ¥ç¶šè¨­å®šã‚’æ”¹å–„ï¼‰
            const response = await axiosInstance.post(
                'https://api.openai.com/v1/chat/completions',
                requestBody,
                {
                    headers: {
                        'Content-Type': 'application/json',
                        'Authorization': `Bearer ${apiKey}`
                    },
                    timeout: timeout,
                    // 30åˆ†ã”ã¨ã«å†ä½œæˆã•ã‚Œã‚‹Agentã‚’ä½¿ç”¨
                    httpsAgent: this.restHttpAgent,
                    maxRedirects: 5,
                    validateStatus: (status) => status >= 200 && status < 300
                }
            );
            
            if (!response.data || !response.data.choices || response.data.choices.length === 0) {
                throw new Error('Invalid response from OpenAI API');
            }
            
            const content = response.data.choices[0].message?.content || '';
            const usage = response.data.usage;
            
            console.log(`âœ… REST API response: ${content.length} chars, usage: ${usage?.total_tokens || 0} tokens`);
            
            return {
                content: content,
                usage: {
                    promptTokens: usage?.prompt_tokens || 0,
                    completionTokens: usage?.completion_tokens || 0,
                    totalTokens: usage?.total_tokens || 0
                }
            };
            
        } catch (error: any) {
            console.error('âŒ REST API fallback error:', error.message);
            
            // Axiosã‚¨ãƒ©ãƒ¼ã®è©³ç´°å‡¦ç†
            if (error.response) {
                const status = error.response.status;
                const errorData = error.response.data?.error;
                
                console.error(`âŒ HTTP ${status}: ${errorData?.message || error.message}`);
                
                // ã‚¨ãƒ©ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’çµ±ä¸€å½¢å¼ã§è¿”ã™
                throw new Error(`REST API Error (${status}): ${errorData?.message || error.message}`);
            }
            
            throw error;
        }
    }

    /**
     * ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—å‡¦ç†ï¼ˆãƒ‡ã‚¹ãƒˆãƒ©ã‚¯ã‚¿ä»£æ›¿ï¼‰
     */
    public destroy(): void {
        // ã‚¿ã‚¤ãƒãƒ¼ã‚’åœæ­¢
        if (this.agentRecycleInterval) {
            clearInterval(this.agentRecycleInterval);
            this.agentRecycleInterval = null;
            console.log('â¹ï¸  Agent recycle timer stopped');
        }
        
        // HTTP Agentã‚’ç ´æ£„
        if (this.httpAgent) {
            this.httpAgent.destroy();
            this.httpAgent = null;
            console.log('ğŸ—‘ï¸  OpenAI SDK Agent destroyed (final cleanup)');
        }
        if (this.restHttpAgent) {
            this.restHttpAgent.destroy();
            this.restHttpAgent = null;
            console.log('ğŸ—‘ï¸  REST API Agent destroyed (final cleanup)');
        }
    }
}

export default OpenAILLMClient;
