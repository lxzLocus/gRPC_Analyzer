/**
 * MockLLMTestRunner - OpenAI APIå‘¼ã³å‡ºã—ã‚’ãƒ¢ãƒƒã‚¯ã—ã¦ãƒ«ãƒ¼ãƒ—å‹•ä½œã‚’ãƒ†ã‚¹ãƒˆ
 * Phase 4-1: çµ±åˆãƒ†ã‚¹ãƒˆã®å®Ÿè£…
 */

import path from 'path';
import fs from 'fs';
import LLMFlowController from './llmFlowController.js';
import Config from './Config.js';
import Logger from './Logger.js';

interface MockLLMResponse {
    id: string;
    content: string;
    usage: {
        prompt_tokens: number;
        completion_tokens: number;
        total_tokens: number;
    };
}

interface TestScenario {
    name: string;
    description: string;
    mockResponses: MockLLMResponse[];
    expectedOutcome: 'success' | 'error' | 'timeout';
    expectedSteps: string[];
}

class MockLLMTestRunner {
    private config: Config;
    private logger: Logger;
    private mockResponseIndex = 0;
    private currentScenario: TestScenario | null = null;
    private testResults: Array<{
        scenario: string;
        success: boolean;
        error?: string;
        actualSteps: string[];
        executionTime: number;
    }> = [];

    constructor(config: Config, logger: Logger) {
        this.config = config;
        this.logger = logger;
    }

    /**
     * ãƒ†ã‚¹ãƒˆã‚·ãƒŠãƒªã‚ªã‚’å®šç¾©
     */
    private getTestScenarios(): TestScenario[] {
        return [
            {
                name: 'basic_successful_flow',
                description: 'åŸºæœ¬çš„ãªæˆåŠŸãƒ•ãƒ­ãƒ¼: ãƒ•ã‚¡ã‚¤ãƒ«è¦æ±‚â†’diffç”Ÿæˆâ†’é©ç”¨â†’å®Œäº†',
                mockResponses: [
                    {
                        id: 'mock_1',
                        content: `%_Thought_%
This is a simple bug fix scenario. I need to analyze the code first.
%_Plan_%
1. Request file contents to understand the issue
2. Generate a diff to fix the bug
3. Apply the fix
%_Reply Required_%
[
    {"type": "FILE_CONTENT", "path": "src/example.js"}
]
%_Modified_%
%_Comment_%
Requesting file content for analysis.`,
                        usage: { prompt_tokens: 100, completion_tokens: 50, total_tokens: 150 }
                    },
                    {
                        id: 'mock_2',
                        content: `%_Thought_%
Now I have the file content. I can see the issue and will generate a fix.
%_Plan_%
Generate a diff to fix the bug in the file.
%_Reply Required_%
%_Modified_%
--- src/example.js
+++ src/example.js
@@ -1,5 +1,5 @@
 function example() {
-    return "bug";
+    return "fixed";
 }
 
 module.exports = example;
%_Comment_%
Applied fix to resolve the bug.
%%_Fin_%%`,
                        usage: { prompt_tokens: 200, completion_tokens: 80, total_tokens: 280 }
                    }
                ],
                expectedOutcome: 'success',
                expectedSteps: ['SystemAnalyzeRequest', 'SystemApplyDiff', 'End']
            },
            {
                name: 'error_recovery_flow',
                description: 'ã‚¨ãƒ©ãƒ¼å›å¾©ãƒ•ãƒ­ãƒ¼: diffé©ç”¨å¤±æ•—â†’ã‚¨ãƒ©ãƒ¼è§£æâ†’å†è©¦è¡Œâ†’æˆåŠŸ',
                mockResponses: [
                    {
                        id: 'mock_error_1',
                        content: `%_Thought_%
Let me create a diff with intentional formatting issues to test error recovery.
%_Plan_%
Generate a malformed diff to test error handling.
%_Reply Required_%
%_Modified_%
--- invalid_file.js
+++ invalid_file.js
@@ malformed diff format
 some changes
%_Comment_%
This diff has formatting issues.`,
                        usage: { prompt_tokens: 150, completion_tokens: 60, total_tokens: 210 }
                    },
                    {
                        id: 'mock_error_2',
                        content: `%_Thought_%
I see there was an error with the diff format. Let me correct it.
%_Plan_%
Generate a properly formatted diff.
%_Reply Required_%
%_Modified_%
--- src/example.js
+++ src/example.js
@@ -1,3 +1,3 @@
 function example() {
-    return "original";
+    return "corrected";
 }
%_Comment_%
Corrected the diff format.
%%_Fin_%%`,
                        usage: { prompt_tokens: 180, completion_tokens: 70, total_tokens: 250 }
                    }
                ],
                expectedOutcome: 'success',
                expectedSteps: ['SystemApplyDiff', 'SendErrorToLLM', 'LLMErrorReanalyze', 'SystemApplyDiff', 'End']
            },
            {
                name: 'parsing_error_flow',
                description: 'ãƒ‘ãƒ¼ã‚¹ ã‚¨ãƒ©ãƒ¼ãƒ•ãƒ­ãƒ¼: ç„¡åŠ¹ãªã‚¿ã‚°â†’å†è§£æâ†’æˆåŠŸ',
                mockResponses: [
                    {
                        id: 'mock_parse_error_1',
                        content: `Invalid response without proper tags
This should cause a parsing error
Some random text`,
                        usage: { prompt_tokens: 120, completion_tokens: 30, total_tokens: 150 }
                    },
                    {
                        id: 'mock_parse_error_2',
                        content: `%_Thought_%
Let me provide a properly formatted response.
%_Plan_%
Create a valid response with proper tags.
%_Reply Required_%
%_Modified_%
--- src/test.js
+++ src/test.js
@@ -1,1 +1,1 @@
-console.log("test");
+console.log("corrected");
%_Comment_%
Fixed the parsing issue.
%%_Fin_%%`,
                        usage: { prompt_tokens: 140, completion_tokens: 50, total_tokens: 190 }
                    }
                ],
                expectedOutcome: 'success',
                expectedSteps: ['LLMReanalyze', 'SystemApplyDiff', 'End']
            }
        ];
    }

    /**
     * OpenAI APIå‘¼ã³å‡ºã—ã‚’ãƒ¢ãƒƒã‚¯
     */
    private mockOpenAICall(prompt: string): MockLLMResponse {
        if (!this.currentScenario) {
            throw new Error('No current scenario set');
        }

        const response = this.currentScenario.mockResponses[this.mockResponseIndex];
        if (!response) {
            throw new Error(`No more mock responses available (index: ${this.mockResponseIndex})`);
        }

        this.mockResponseIndex++;
        this.logger.logInfo(`ğŸ“± Mock LLM Response [${response.id}]: ${response.content.substring(0, 100)}...`);
        
        return response;
    }

    /**
     * LLMFlowControllerã®OpenAIå‘¼ã³å‡ºã—ã‚’ãƒ¢ãƒƒã‚¯ã«ç½®ãæ›ãˆ
     */
    private injectMockIntoController(controller: LLMFlowController) {
        // OpenAIClientã®ãƒ¡ã‚½ãƒƒãƒ‰ã‚’ãƒ¢ãƒƒã‚¯ã«ç½®ãæ›ãˆ
        const originalOpenAIClient = (controller as any).openAIClient;
        
        if (originalOpenAIClient) {
            // setMockResponseãƒ¡ã‚½ãƒƒãƒ‰ã‚’ä½¿ç”¨ã—ã¦ãƒ¢ãƒƒã‚¯ã‚’è¨­å®š
            originalOpenAIClient.setMockResponse = (response: any) => {
                originalOpenAIClient.client = {
                    chat: {
                        completions: {
                            create: async () => response
                        }
                    }
                };
            };

            // fetchOpenAPIãƒ¡ã‚½ãƒƒãƒ‰ã‚’ã‚ªãƒ¼ãƒãƒ¼ãƒ©ã‚¤ãƒ‰
            originalOpenAIClient.fetchOpenAPI = async (messages: Array<{ role: string, content: string }>) => {
                const mockResponse = this.mockOpenAICall(messages.map(m => m.content).join('\n'));
                
                // OpenAI APIã®ãƒ¬ã‚¹ãƒãƒ³ã‚¹å½¢å¼ã«åˆã‚ã›ã¦å¤‰æ›
                return {
                    id: mockResponse.id,
                    object: 'chat.completion',
                    created: Date.now(),
                    model: 'gpt-4-mock',
                    choices: [
                        {
                            index: 0,
                            message: {
                                role: 'assistant',
                                content: mockResponse.content
                            },
                            finish_reason: 'stop'
                        }
                    ],
                    usage: mockResponse.usage
                };
            };
        }
    }

    /**
     * å˜ä¸€ãƒ†ã‚¹ãƒˆã‚·ãƒŠãƒªã‚ªã‚’å®Ÿè¡Œ
     */
    private async runSingleScenario(scenario: TestScenario): Promise<void> {
        this.logger.logInfo(`ğŸ§ª Running test scenario: ${scenario.name}`);
        this.logger.logInfo(`ğŸ“ Description: ${scenario.description}`);
        
        this.currentScenario = scenario;
        this.mockResponseIndex = 0;
        const actualSteps: string[] = [];
        const startTime = Date.now();

        try {
            // ãƒ†ã‚¹ãƒˆç”¨ã®è¨­å®šã‚’ä½œæˆ
            const testConfig = new Config('/app/dataset/test');

            // LLMFlowControllerã‚’ä½œæˆ
            const controller = new LLMFlowController('/app/dataset/test');
            
            // OpenAIå‘¼ã³å‡ºã—ã‚’ãƒ¢ãƒƒã‚¯ã«ç½®ãæ›ãˆ
            this.injectMockIntoController(controller);

            // å®Ÿéš›ã®ãƒ«ãƒ¼ãƒ—ã‚’å®Ÿè¡Œ
            await controller.run();

            // å®Ÿè¡Œæ™‚é–“ã‚’æ¸¬å®š
            const executionTime = Date.now() - startTime;

            // ãƒ†ã‚¹ãƒˆçµæœã‚’è¨˜éŒ²
            this.testResults.push({
                scenario: scenario.name,
                success: true,
                actualSteps,
                executionTime
            });

            this.logger.logInfo(`âœ… Test scenario '${scenario.name}' completed successfully in ${executionTime}ms`);

        } catch (error) {
            const executionTime = Date.now() - startTime;
            const errorMessage = error instanceof Error ? error.message : String(error);
            
            this.testResults.push({
                scenario: scenario.name,
                success: false,
                error: errorMessage,
                actualSteps,
                executionTime
            });

            this.logger.logError(`âŒ Test scenario '${scenario.name}' failed: ${errorMessage}`);
        }
    }

    /**
     * å…¨ãƒ†ã‚¹ãƒˆã‚·ãƒŠãƒªã‚ªã‚’å®Ÿè¡Œ
     */
    async runAllTests(): Promise<void> {
        this.logger.logInfo('ğŸš€ Starting Mock LLM Test Runner');
        this.logger.logInfo('==========================================');

        const scenarios = this.getTestScenarios();
        
        for (const scenario of scenarios) {
            await this.runSingleScenario(scenario);
            
            // ã‚·ãƒŠãƒªã‚ªé–“ã®å¾…æ©Ÿæ™‚é–“
            await new Promise(resolve => setTimeout(resolve, 1000));
        }

        // ãƒ†ã‚¹ãƒˆçµæœã®ã‚µãƒãƒªãƒ¼ã‚’å‡ºåŠ›
        this.printTestSummary();
    }

    /**
     * ãƒ†ã‚¹ãƒˆçµæœã®ã‚µãƒãƒªãƒ¼ã‚’å‡ºåŠ›
     */
    private printTestSummary(): void {
        this.logger.logInfo('\nğŸ“Š Test Summary');
        this.logger.logInfo('==========================================');
        
        const totalTests = this.testResults.length;
        const successfulTests = this.testResults.filter(r => r.success).length;
        const failedTests = totalTests - successfulTests;
        
        this.logger.logInfo(`Total Tests: ${totalTests}`);
        this.logger.logInfo(`Successful: ${successfulTests}`);
        this.logger.logInfo(`Failed: ${failedTests}`);
        this.logger.logInfo(`Success Rate: ${((successfulTests / totalTests) * 100).toFixed(1)}%`);

        // è©³ç´°çµæœ
        this.logger.logInfo('\nğŸ“‹ Detailed Results:');
        this.testResults.forEach(result => {
            const status = result.success ? 'âœ…' : 'âŒ';
            this.logger.logInfo(`${status} ${result.scenario} (${result.executionTime}ms)`);
            if (result.error) {
                this.logger.logError(`   Error: ${result.error}`);
            }
        });

        // çµ±è¨ˆãƒ¬ãƒãƒ¼ãƒˆã®ç”Ÿæˆ
        this.generateTestReport();
    }

    /**
     * è©³ç´°ãªãƒ†ã‚¹ãƒˆãƒ¬ãƒãƒ¼ãƒˆã‚’ç”Ÿæˆ
     */
    private generateTestReport(): void {
        const reportData = {
            timestamp: new Date().toISOString(),
            summary: {
                totalTests: this.testResults.length,
                successfulTests: this.testResults.filter(r => r.success).length,
                failedTests: this.testResults.filter(r => !r.success).length,
                averageExecutionTime: this.testResults.reduce((sum, r) => sum + r.executionTime, 0) / this.testResults.length
            },
            results: this.testResults,
            recommendations: this.generateTestRecommendations()
        };

        // ãƒ¬ãƒãƒ¼ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä¿å­˜
        const reportPath = path.join('/app/output', 'mock_test_report.json');
        fs.writeFileSync(reportPath, JSON.stringify(reportData, null, 2));
        
        this.logger.logInfo(`ğŸ“„ Test report saved to: ${reportPath}`);
    }

    /**
     * ãƒ†ã‚¹ãƒˆçµæœã«åŸºã¥ãæ”¹å–„ææ¡ˆã‚’ç”Ÿæˆ
     */
    private generateTestRecommendations(): string[] {
        const recommendations: string[] = [];
        
        const failedTests = this.testResults.filter(r => !r.success);
        if (failedTests.length > 0) {
            recommendations.push('Review failed test scenarios for potential improvements');
        }

        const avgExecutionTime = this.testResults.reduce((sum, r) => sum + r.executionTime, 0) / this.testResults.length;
        if (avgExecutionTime > 5000) {
            recommendations.push('Consider optimizing performance for faster execution');
        }

        if (recommendations.length === 0) {
            recommendations.push('All tests passed successfully - system is operating as expected');
        }

        return recommendations;
    }
}

export default MockLLMTestRunner;
