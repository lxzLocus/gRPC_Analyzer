/**
 * OpenAI API ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆ (ãƒ¬ã‚¬ã‚·ãƒ¼äº’æ›æ€§)
 * æ–°ã—ã„LLMã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã‚·ã‚¹ãƒ†ãƒ ã¨æ—¢å­˜ã‚³ãƒ¼ãƒ‰ã®æ©‹æ¸¡ã—
 */

import Config from './config.js';
import { LLMClientFactory } from './llmClientFactory.js';
import { LLMClient } from './llmClient.js';

class OpenAIClient {
    private llmClient: LLMClient;
    private config: Config;
    public initPromise: Promise<void>;

    constructor(config: Config, apiKey?: string) {
        this.config = config;
        
        console.log(`ğŸ”‘ OPENAI_API_KEY length: ${(process.env.OPENAI_API_KEY || '').length}`);
        console.log(`ğŸ”‘ OPENAI_API_KEY length: ${(process.env.OPENAI_API_KEY || '').length}`);
        console.log(`ğŸŒ NODE_ENV: ${process.env.NODE_ENV || 'undefined'}`);
        console.log(`ï¿½ DEBUG_MODE: ${process.env.DEBUG_MODE || 'undefined'}`);
        
        // ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ã®è‡ªå‹•é¸æŠã¾ãŸã¯è¨­å®šã«åŸºã¥ãé¸æŠ
        const provider = LLMClientFactory.autoSelectProvider(config);
        console.log(`ğŸ¤– Selected LLM provider: ${provider}`);
        
        this.llmClient = LLMClientFactory.create(config, provider);
        this.initPromise = this.llmClient.waitForInitialization();
    }

    async fetchOpenAPI(messages: Array<{ role: string, content: string }>): Promise<any> {
        // æ–°ã—ã„LLMã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã‚’ä½¿ç”¨ã—ã¦ãƒ¬ã‚¹ãƒãƒ³ã‚¹ã‚’å–å¾—
        await this.initPromise;

        try {
            const response = await this.llmClient.generateContent({
                messages: messages.map(msg => ({
                    role: msg.role as 'system' | 'user' | 'assistant',
                    content: msg.content
                }))
            });

            // æ—¢å­˜ã®ã‚³ãƒ¼ãƒ‰ã¨ã®äº’æ›æ€§ã®ãŸã‚ã€OpenAIå½¢å¼ã®ãƒ¬ã‚¹ãƒãƒ³ã‚¹ã‚’æ¨¡å€£
            return {
                choices: [{
                    message: {
                        content: response.content,
                        role: 'assistant'
                    },
                    finish_reason: response.finishReason || 'stop'
                }],
                usage: response.usage ? {
                    prompt_tokens: response.usage.promptTokens,
                    completion_tokens: response.usage.completionTokens,
                    total_tokens: response.usage.totalTokens
                } : {
                    prompt_tokens: 0,
                    completion_tokens: 0,
                    total_tokens: 0
                },
                model: response.model || 'unknown'
            };

        } catch (error) {
            console.error('âŒ LLM API error:', error);
            throw error;
        }
    }

    /**
     * ä½¿ç”¨ä¸­ã®LLMãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼åã‚’å–å¾—
     */
    getProviderName(): string {
        return this.llmClient.getProviderName();
    }

    /**
     * LLMã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆãŒæº–å‚™å®Œäº†ã‹ãƒã‚§ãƒƒã‚¯
     */
    isReady(): boolean {
        return this.llmClient.isReady();
    }

    /**
     * ãƒ¢ãƒƒã‚¯ç”¨ãƒ¡ã‚½ãƒƒãƒ‰: ãƒ¬ã‚¹ãƒãƒ³ã‚¹ã‚’ç›´æ¥è¨­å®šï¼ˆäº’æ›æ€§ã®ãŸã‚æ®‹ã™ï¼‰
     */
    setMockResponse(response: any): void {
        // æ–°ã—ã„ã‚·ã‚¹ãƒ†ãƒ ã§ã¯ãƒ¢ãƒƒã‚¯æ©Ÿèƒ½ã¯å„ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã§å‡¦ç†
        console.warn('âš ï¸  setMockResponse is deprecated in new LLM system');
    }
}

export default OpenAIClient;
