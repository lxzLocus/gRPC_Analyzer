/**
 * LLMå‡¦ç†ã‚µãƒ¼ãƒ“ã‚¹
 * LLMFlowController ã®å®Ÿè¡Œã¨ãƒªãƒˆãƒ©ã‚¤å‡¦ç†ã‚’ç®¡ç†
 */

import { DatasetRepository } from '../Repository/DatasetRepository.js';
import { BatchProcessingOptions, LLMControllerResult } from '../types/BatchProcessTypes.js';
import LLMFlowController from '../modules/llmFlowController.js';
import Logger from '../modules/Logger.js';

// Node.jså‹ã®å®£è¨€
declare const process: any;

export class LLMProcessingService {
    private datasetRepository: DatasetRepository;
    private options: BatchProcessingOptions;
    private currentController: LLMFlowController | null = null;
    private logger: Logger;

    constructor(options: BatchProcessingOptions) {
        this.options = options;
        this.datasetRepository = new DatasetRepository();
        this.logger = new Logger();
    }

    /**
     * ãƒªãƒˆãƒ©ã‚¤æ©Ÿèƒ½ä»˜ãLLMå‡¦ç†å®Ÿè¡Œ
     * Phase 1 (3å›): OpenAIãƒ©ã‚¤ãƒ–ãƒ©ãƒªä½¿ç”¨
     * Phase 2 (3å›): REST APIç›´æ¥å‘¼ã³å‡ºã—ï¼ˆãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼‰
     */
    async processWithRetry(
        premergeDir: string,
        repositoryName: string,
        category: string,
        pullRequestTitle: string
    ): Promise<LLMControllerResult> {
        const maxRetriesPerPhase = this.options.maxRetries || 3;
        const totalMaxRetries = maxRetriesPerPhase * 2; // Phase 1 + Phase 2
        let lastError: Error | null = null;
        let useRestApiFallback = false;

        for (let retry = 0; retry < totalMaxRetries; retry++) {
            try {
                // Phaseåˆ‡ã‚Šæ›¿ãˆ: æœ€åˆã®3å›ãŒå¤±æ•—ã—ãŸã‚‰REST APIãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ã«åˆ‡ã‚Šæ›¿ãˆ
                if (retry >= maxRetriesPerPhase && !useRestApiFallback) {
                    console.log('\nğŸ”„ â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•');
                    console.log('âš ï¸  OpenAI Library failed 3 times. Switching to REST API fallback...');
                    console.log('ğŸ”„ â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n');
                    useRestApiFallback = true;
                }

                const phase = useRestApiFallback ? 2 : 1;
                const phaseRetry = retry % maxRetriesPerPhase;
                const phaseAttempt = phaseRetry + 1;
                
                console.log(`ğŸ”„ Processing (Phase ${phase}, attempt ${phaseAttempt}/${maxRetriesPerPhase}): ${repositoryName}/${category}/${pullRequestTitle}`);
                
                const result = await this.executeLLMController(premergeDir, useRestApiFallback);
                
                if (result.success) {
                    if (useRestApiFallback) {
                        console.log('âœ… Success using REST API fallback!');
                    }
                    return result;
                } else if (!this.isRetryableError(result.errorMessage || '')) {
                    // ãƒªãƒˆãƒ©ã‚¤ä¸å¯èƒ½ãªã‚¨ãƒ©ãƒ¼ã®å ´åˆã¯å³åº§ã«è¿”ã™
                    return result;
                }
                
                lastError = new Error(result.errorMessage);
                
            } catch (error) {
                lastError = error instanceof Error ? error : new Error(String(error));
                
                const phase = useRestApiFallback ? 2 : 1;
                const phaseRetry = retry % maxRetriesPerPhase;
                const phaseAttempt = phaseRetry + 1;
                
                // è©³ç´°ã‚¨ãƒ©ãƒ¼ãƒ­ã‚°ã®è¨˜éŒ²
                this.logger.logLLMParsingError(
                    lastError.message || 'Unknown error',
                    'processWithRetry',
                    'Expected successful LLM processing',
                    `Failed at Phase ${phase}, attempt ${phaseAttempt}/${maxRetriesPerPhase}`,
                    lastError
                );
                
                console.error(`âŒ Error in Phase ${phase}, attempt ${phaseAttempt}/${maxRetriesPerPhase} for ${pullRequestTitle}:`, lastError.message);
                
                if (!this.isRetryableError(lastError.message)) {
                    // ãƒªãƒˆãƒ©ã‚¤ä¸å¯èƒ½ãªã‚¨ãƒ©ãƒ¼ã®å ´åˆ
                    // ãŸã ã—ã€Phase 1ã§ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã‚¨ãƒ©ãƒ¼ã®å ´åˆã¯Phase 2ã¸
                    if (!useRestApiFallback && this.isNetworkError(lastError.message)) {
                        console.log('ğŸ”„ Network error detected in Phase 1. Will try Phase 2 (REST API)...');
                        retry = maxRetriesPerPhase - 1; // Phase 2ã«å¼·åˆ¶é·ç§»
                        continue;
                    }
                    break;
                }
                
                // Phaseå†…ã§ã®æœ€çµ‚ãƒªãƒˆãƒ©ã‚¤ã®å ´åˆã€æ¬¡ã®Phaseã¸
                if (phaseAttempt >= maxRetriesPerPhase && !useRestApiFallback) {
                    console.log('âš ï¸  Phase 1 exhausted. Moving to Phase 2...');
                    continue; // æ¬¡ã®ãƒ«ãƒ¼ãƒ—ã§Phase 2é–‹å§‹
                }
                
                // ãƒªãƒˆãƒ©ã‚¤å¾…æ©Ÿï¼ˆexponential backoff with jitterï¼‰
                // ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã‚¨ãƒ©ãƒ¼ã®å ´åˆã¯å¾…æ©Ÿæ™‚é–“ã‚’å»¶é•·
                const isNetworkErr = this.isNetworkError(lastError.message);
                const baseWaitTime = Math.pow(2, phaseRetry) * (isNetworkErr ? 2000 : 1000);
                // ã‚¸ãƒƒã‚¿ãƒ¼ã‚’è¿½åŠ ã—ã¦åŒæ™‚ãƒªãƒˆãƒ©ã‚¤ã®è¡çªã‚’å›é¿
                const jitter = Math.random() * 1000;
                const waitTime = baseWaitTime + jitter;
                console.log(`â³ Waiting ${Math.round(waitTime)}ms before retry (network error: ${isNetworkErr})...`);
                await new Promise(resolve => setTimeout(resolve, waitTime));
                
            } finally {
                await this.cleanupController();
            }
        }

        return {
            success: false,
            processingTime: 0,
            errorMessage: lastError?.message || 'Unknown error after all retry phases'
        };
    }

    /**
     * LLMFlowController ã®å®Ÿè¡Œ
     * @param premergeDir ãƒ—ãƒªãƒãƒ¼ã‚¸ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª
     * @param useRestApiFallback REST APIãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ã‚’ä½¿ç”¨ã™ã‚‹ã‹
     */
    private async executeLLMController(premergeDir: string, useRestApiFallback: boolean = false): Promise<LLMControllerResult> {
        const startTime = Date.now();

        try {
            // REST APIãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯æ™‚ã¯ç’°å¢ƒå¤‰æ•°ã§é€šçŸ¥
            if (useRestApiFallback) {
                process.env.USE_OPENAI_REST_FALLBACK = 'true';
            } else {
                delete process.env.USE_OPENAI_REST_FALLBACK;
            }
            
            this.currentController = new LLMFlowController(premergeDir, {
                enablePreVerification: this.options.enablePreVerification ?? true
            });
            
            // ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆè¨­å®š
            const timeoutPromise = new Promise<never>((_, reject) => {
                setTimeout(() => {
                    reject(new Error(`Process timeout after ${this.options.timeoutMs! / 1000}s`));
                }, this.options.timeoutMs);
            });

            // å®Ÿéš›ã®å‡¦ç†å®Ÿè¡Œ
            await Promise.race([
                this.currentController.run(),
                timeoutPromise
            ]);

            // ãƒˆãƒ¼ã‚¯ãƒ³ä½¿ç”¨é‡ã‚’å–å¾—
            const tokenUsage = this.currentController.getTokenUsage();
            console.log('ğŸ« LLM Token Usage:', tokenUsage);

            return {
                success: true,
                processingTime: Date.now() - startTime,
                usage: tokenUsage
            };

        } catch (error) {
            // ã‚¨ãƒ©ãƒ¼æ™‚ã‚‚ãƒˆãƒ¼ã‚¯ãƒ³æƒ…å ±ã‚’è¿”ã™ï¼ˆã‚³ãƒ³ãƒˆãƒ­ãƒ¼ãƒ©ãƒ¼ãŒå­˜åœ¨ã™ã‚‹å ´åˆï¼‰
            const tokenUsage = this.currentController ? this.currentController.getTokenUsage() : undefined;
            
            return {
                success: false,
                processingTime: Date.now() - startTime,
                errorMessage: error instanceof Error ? error.message : String(error),
                usage: tokenUsage
            };
        } finally {
            // ç’°å¢ƒå¤‰æ•°ã‚’ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—
            delete process.env.USE_OPENAI_REST_FALLBACK;
        }
    }

    /**
     * å‡¦ç†çµæœã®åˆ†æ
     */
    async analyzeResult(
        repositoryName: string,
        category: string,
        pullRequestTitle: string,
        llmResult: LLMControllerResult
    ): Promise<boolean> {
        if (!llmResult.success) {
            return false;
        }

        try {
            // ãƒ­ã‚°ãƒ•ã‚¡ã‚¤ãƒ«ã®å–å¾—
            const logFiles = await this.datasetRepository.getLogFiles(repositoryName, category, pullRequestTitle);
            if (logFiles.length === 0) {
                console.log(`âš ï¸ No log files found for ${pullRequestTitle}`);
                return false;
            }

            // æœ€æ–°ã®ãƒ­ã‚°ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ç¢ºèª
            const latestLogFile = logFiles.sort().pop();
            if (!latestLogFile) return false;

            const logContent = await this.datasetRepository.readLogFile(
                repositoryName, category, pullRequestTitle, latestLogFile
            );
            
            if (!logContent) return false;

            const logData = JSON.parse(logContent);

            // ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹ã‚’ç¢ºèªï¼ˆ%%_Fin_%%ã‚¿ã‚°ãƒ™ãƒ¼ã‚¹ã®å³å¯†ãªåˆ¤å®šï¼‰
            const status = logData.experiment_metadata?.status || 'Unknown';
            
            // %%_Fin_%%ã‚¿ã‚°ã®å­˜åœ¨ç¢ºèª
            const hasFinTag = logContent.includes('%%_Fin_%%') || status.includes('%%_Fin_%%');
            
            // æ˜ç¤ºçš„ãªã‚¨ãƒ©ãƒ¼ã®ç¢ºèª
            const hasErrors = logContent.includes('400 This model\'s maximum context length') ||
                            logContent.includes('JSON parse failed') ||
                            status.includes('Incomplete') ||
                            status.includes('Error') ||
                            status.includes('Failed');

            // æˆåŠŸæ¡ä»¶: %%_Fin_%%ã‚¿ã‚°ãŒã‚ã‚Šã€é‡å¤§ãªã‚¨ãƒ©ãƒ¼ãŒãªã„
            const isSuccess = hasFinTag && !hasErrors;

            console.log(`ğŸ“Š Processing result for ${pullRequestTitle}:`);
            console.log(`   Status: ${status}`);
            console.log(`   %%_Fin_%% tag: ${hasFinTag ? 'YES' : 'NO'}`);
            console.log(`   Has errors: ${hasErrors ? 'YES' : 'NO'}`);
            console.log(`   Final result: ${isSuccess ? 'SUCCESS' : 'FAILURE'}`);

            return isSuccess;

        } catch (error) {
            console.error(`âŒ Error analyzing processing result for ${pullRequestTitle}:`, error);
            return false;
        }
    }

    /**
     * ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã‚¨ãƒ©ãƒ¼ã‹ã©ã†ã‹ã®åˆ¤å®š
     */
    private isNetworkError(errorMessage: string): boolean {
        const networkPatterns = [
            /connection.*error/i,
            /APIConnectionError/i,
            /ECONNREFUSED|ENOTFOUND|ETIMEDOUT/i,
            /network/i,
            /timeout/i
        ];
        
        const lowerMessage = errorMessage.toLowerCase();
        return networkPatterns.some(pattern => pattern.test(lowerMessage));
    }

    /**
     *      console.log(`   Has errors: ${hasErrors ? 'YES' : 'NO'}`);
            console.log(`   Final result: ${isSuccess ? 'SUCCESS' : 'FAILURE'}`);

            return isSuccess;

        } catch (error) {
            console.error(`âŒ Error analyzing processing result for ${pullRequestTitle}:`, error);
            return false;
        }
    }

    /**
     * ã‚¨ãƒ©ãƒ¼ãŒãƒªãƒˆãƒ©ã‚¤å¯èƒ½ã‹ã©ã†ã‹ã®åˆ¤å®š
     */
    private isRetryableError(errorMessage: string): boolean {
        const retryablePatterns = [
            // ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ãƒ»æ¥ç¶šã‚¨ãƒ©ãƒ¼ï¼ˆãƒªãƒˆãƒ©ã‚¤å¯èƒ½ï¼‰
            /connection.*error/i,
            /network|timeout|connection/i,
            /ECONNREFUSED|ENOTFOUND|ETIMEDOUT/i,
            
            // OpenAI APIç‰¹æœ‰ã®ãƒªãƒˆãƒ©ã‚¤å¯èƒ½ã‚¨ãƒ©ãƒ¼
            /rate.*limit/i,           // RateLimitError (429)
            /APITimeoutError/i,       // ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆ (408)
            /InternalServerError/i,   // ã‚µãƒ¼ãƒãƒ¼ã‚¨ãƒ©ãƒ¼ (500)
            /502|503|504/i,           // Bad Gateway, Service Unavailable, Gateway Timeout
            
            // ä¸€æ™‚çš„ãªã‚¨ãƒ©ãƒ¼
            /temporary|temp/i,
            /try.*again/i,
            
            // JSONãƒ‘ãƒ¼ã‚¹ã‚¨ãƒ©ãƒ¼ï¼ˆLLMãƒ¬ã‚¹ãƒãƒ³ã‚¹ã®å•é¡Œï¼‰
            /JSON.*parse/i,
            /SyntaxError/i,
            /unexpected/i
        ];

        const nonRetryablePatterns = [
            // OpenAI APIç‰¹æœ‰ã®ãƒªãƒˆãƒ©ã‚¤ä¸å¯ã‚¨ãƒ©ãƒ¼
            /UnauthorizedError/i,            // èªè¨¼ã‚¨ãƒ©ãƒ¼ (401)
            /BadRequestError/i,              // ä¸æ­£ãªãƒªã‚¯ã‚¨ã‚¹ãƒˆ (400) - ä¸€éƒ¨é™¤ã
            /NotFoundError/i,                // ãƒ¢ãƒ‡ãƒ«æœªç™ºè¦‹ (404)
            /UnprocessableEntityError/i,     // å‡¦ç†ä¸å¯ (422)
            /ConflictError/i,                // ç«¶åˆ (409)
            
            // HTTPã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹ã‚³ãƒ¼ãƒ‰
            /401|403|404|413|422/i,
            
            // å…·ä½“çš„ãªã‚¨ãƒ©ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸
            /authentication.*failed/i,
            /invalid.*api.*key/i,
            /model.*not.*found/i,
            /invalid.*request/i,
            /malformed/i,
            
            // ãƒ•ã‚¡ã‚¤ãƒ«ã‚·ã‚¹ãƒ†ãƒ ã‚¨ãƒ©ãƒ¼
            /file.*not.*found/i,
            /directory.*not.*found/i,
            /permission.*denied/i
        ];

        const lowerErrorMessage = errorMessage.toLowerCase();
        
        // ç‰¹åˆ¥ãªã‚±ãƒ¼ã‚¹: BadRequestErrorã§ã‚‚ãƒˆãƒ¼ã‚¯ãƒ³è¶…éã®å ´åˆã¯ãƒªãƒˆãƒ©ã‚¤å¯èƒ½
        // ï¼ˆã‚·ã‚¹ãƒ†ãƒ ãŒè‡ªå‹•çš„ã«ã‚µãƒãƒ©ã‚¤ã‚ºã—ã¦å†è©¦è¡Œã™ã‚‹ï¼‰
        if (/token|context_length/i.test(lowerErrorMessage) && /400|BadRequest/i.test(lowerErrorMessage)) {
            console.log(`ğŸ”„ Token limit error detected - System will summarize and retry: ${errorMessage}`);
            return true;
        }

        // éãƒªãƒˆãƒ©ã‚¤å¯¾è±¡ã®ã‚¨ãƒ©ãƒ¼ã‚’ãƒã‚§ãƒƒã‚¯ï¼ˆå„ªå…ˆï¼‰
        for (const pattern of nonRetryablePatterns) {
            if (pattern.test(lowerErrorMessage)) {
                console.log(`ğŸš« Non-retryable error detected: ${errorMessage}`);
                return false;
            }
        }

        // ãƒªãƒˆãƒ©ã‚¤å¯¾è±¡ã®ã‚¨ãƒ©ãƒ¼ã‚’ãƒã‚§ãƒƒã‚¯
        for (const pattern of retryablePatterns) {
            if (pattern.test(lowerErrorMessage)) {
                console.log(`ğŸ”„ Retryable error detected: ${errorMessage}`);
                return true;
            }
        }

        // ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã¯ãƒªãƒˆãƒ©ã‚¤å¯¾è±¡å¤–
        console.log(`â“ Unknown error type (not retrying): ${errorMessage}`);
        return false;
    }

    /**
     * ã‚³ãƒ³ãƒˆãƒ­ãƒ¼ãƒ©ãƒ¼ã®ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—
     */
    private async cleanupController(): Promise<void> {
        if (this.currentController) {
            try {
                // LLMFlowControllerã«cleanupãƒ¡ã‚½ãƒƒãƒ‰ãŒã‚ã‚‹å ´åˆã®ã¿å‘¼ã³å‡ºã—
                if (typeof (this.currentController as any).cleanup === 'function') {
                    await (this.currentController as any).cleanup();
                }
            } catch (cleanupError) {
                console.warn('âš ï¸ Controller cleanup failed:', cleanupError);
            }
        }
        this.currentController = null;

        // ã‚¬ãƒ™ãƒ¼ã‚¸ã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³
        if (this.options.enableGarbageCollection && global.gc) {
            global.gc();
        }
    }

    /**
     * ã‚µãƒ¼ãƒ“ã‚¹ã®ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—
     */
    async cleanup(): Promise<void> {
        await this.cleanupController();
    }
}
