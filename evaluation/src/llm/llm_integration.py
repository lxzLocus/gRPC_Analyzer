"""
LLMçµ±åˆã‚·ã‚¹ãƒ†ãƒ 
OpenAIã€Anthropicã€ãã®ä»–ã®LLMãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ã¨ã®çµ±åˆ
"""

import json
import os
import time
import asyncio
from typing import Dict, Any, Optional, List
from dataclasses import dataclass
from abc import ABC, abstractmethod
from pathlib import Path


@dataclass
class LLMResponse:
    """LLMå¿œç­”ãƒ‡ãƒ¼ã‚¿ã‚¯ãƒ©ã‚¹"""
    content: str
    usage: Dict[str, Any]
    model: str
    provider: str
    success: bool
    error_message: Optional[str] = None


class BaseLLMProvider(ABC):
    """LLMãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ã®åŸºåº•ã‚¯ãƒ©ã‚¹"""
    
    def __init__(self, model_name: str, api_key: Optional[str] = None):
        self.model_name = model_name
        self.api_key = api_key
        self.rate_limit_delay = 1.0  # ç§’
        
    @abstractmethod
    async def generate(self, prompt: str, **kwargs) -> LLMResponse:
        """ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã«å¯¾ã—ã¦LLMå¿œç­”ã‚’ç”Ÿæˆ"""
        pass
    
    def _handle_rate_limit(self, retry_count: int = 0) -> float:
        """ãƒ¬ãƒ¼ãƒˆåˆ¶é™ã®ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°"""
        delay = self.rate_limit_delay * (2 ** retry_count)  # æŒ‡æ•°ãƒãƒƒã‚¯ã‚ªãƒ•
        return min(delay, 60.0)  # æœ€å¤§60ç§’


class OpenAIProvider(BaseLLMProvider):
    """OpenAI LLMãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼"""
    
    def __init__(self, model_name: str = "gpt-4", api_key: Optional[str] = None):
        super().__init__(model_name, api_key or os.getenv("OPENAI_API_KEY"))
        self.rate_limit_delay = 1.0
        
        if not self.api_key:
            raise ValueError("OpenAI API key is required")
    
    async def generate(self, prompt: str, max_retries: int = 3, **kwargs) -> LLMResponse:
        """OpenAI APIã‚’ä½¿ç”¨ã—ã¦LLMå¿œç­”ã‚’ç”Ÿæˆ"""
        
        for attempt in range(max_retries + 1):
            try:
                # æ–°ã—ã„OpenAI APIã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã®åˆæœŸåŒ–
                from openai import AsyncOpenAI
                client = AsyncOpenAI(api_key=self.api_key)
                
                response = await client.chat.completions.create(
                    model=self.model_name,
                    messages=[
                        {"role": "system", "content": "You are a helpful AI assistant specializing in code analysis and evaluation. You must respond with valid JSON only."},
                        {"role": "user", "content": prompt}
                    ],
                    max_tokens=kwargs.get("max_tokens", 4000),
                    temperature=kwargs.get("temperature", 0.1),
                    response_format={"type": "json_object"}
                )
                
                return LLMResponse(
                    content=response.choices[0].message.content,
                    usage=response.usage.model_dump() if response.usage else {},
                    model=self.model_name,
                    provider="openai",
                    success=True
                )
                
            except Exception as e:
                if "rate_limit" in str(e).lower() and attempt < max_retries:
                    delay = self._handle_rate_limit(attempt)
                    print(f"â³ OpenAI ãƒ¬ãƒ¼ãƒˆåˆ¶é™ - {delay:.1f}ç§’å¾…æ©Ÿä¸­...")
                    await asyncio.sleep(delay)
                    continue
                else:
                    return LLMResponse(
                        content="",
                        usage={},
                        model=self.model_name,
                        provider="openai",
                        success=False,
                        error_message=str(e)
                    )
        
        return LLMResponse(
            content="",
            usage={},
            model=self.model_name,
            provider="openai",
            success=False,
            error_message="Max retries exceeded"
        )


class AnthropicProvider(BaseLLMProvider):
    """Anthropic Claude LLMãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼"""
    
    def __init__(self, model_name: str = "claude-3-sonnet-20240229", api_key: Optional[str] = None):
        super().__init__(model_name, api_key or os.getenv("ANTHROPIC_API_KEY"))
        self.rate_limit_delay = 2.0
        
        if not self.api_key:
            raise ValueError("Anthropic API key is required")
    
    async def generate(self, prompt: str, max_retries: int = 3, **kwargs) -> LLMResponse:
        """Anthropic APIã‚’ä½¿ç”¨ã—ã¦LLMå¿œç­”ã‚’ç”Ÿæˆ"""
        
        for attempt in range(max_retries + 1):
            try:
                # Anthropic APIã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã®åˆæœŸåŒ–ï¼ˆä»®æƒ³å®Ÿè£…ï¼‰
                import anthropic
                client = anthropic.AsyncAnthropic(api_key=self.api_key)
                
                response = await client.messages.create(
                    model=self.model_name,
                    max_tokens=kwargs.get("max_tokens", 4000),
                    temperature=kwargs.get("temperature", 0.1),
                    messages=[
                        {"role": "user", "content": prompt}
                    ]
                )
                
                return LLMResponse(
                    content=response.content[0].text,
                    usage={"input_tokens": response.usage.input_tokens, "output_tokens": response.usage.output_tokens},
                    model=self.model_name,
                    provider="anthropic",
                    success=True
                )
                
            except Exception as e:
                if "rate_limit" in str(e).lower() and attempt < max_retries:
                    delay = self._handle_rate_limit(attempt)
                    print(f"â³ Anthropic ãƒ¬ãƒ¼ãƒˆåˆ¶é™ - {delay:.1f}ç§’å¾…æ©Ÿä¸­...")
                    await asyncio.sleep(delay)
                    continue
                else:
                    return LLMResponse(
                        content="",
                        usage={},
                        model=self.model_name,
                        provider="anthropic",
                        success=False,
                        error_message=str(e)
                    )
        
        return LLMResponse(
            content="",
            usage={},
            model=self.model_name,
            provider="anthropic",
            success=False,
            error_message="Max retries exceeded"
        )


class MockLLMProvider(BaseLLMProvider):
    """ãƒ†ã‚¹ãƒˆç”¨ã®ãƒ¢ãƒƒã‚¯LLMãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼"""
    
    def __init__(self, model_name: str = "mock-gpt"):
        super().__init__(model_name, "mock_api_key")
        self.rate_limit_delay = 0.1
    
    async def generate(self, prompt: str, **kwargs) -> LLMResponse:
        """ãƒ¢ãƒƒã‚¯å¿œç­”ã‚’ç”Ÿæˆ"""
        
        # ç°¡å˜ãªé…å»¶ã‚’ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ãƒˆ
        await asyncio.sleep(0.1)
        
        # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆå†…å®¹ã«åŸºã¥ã„ã¦ãƒ¢ãƒƒã‚¯å¿œç­”ã‚’ç”Ÿæˆ
        if "parser_evaluation" in prompt.lower():
            # ã‚ˆã‚Šç¾å®Ÿçš„ã§è©³ç´°ãªè‹±èªè©•ä¾¡å¿œç­”ã‚’ç”Ÿæˆ
            mock_response = {
                "parser_evaluation": [
                    {
                        "turn": 1,
                        "status": "PASS",
                        "reasoning": "JSON structure is well-formed with complete metadata fields. Parser successfully extracted experiment ID, timestamps, and token usage data. No structural anomalies detected in turn 1."
                    },
                    {
                        "turn": 2,
                        "status": "PASS", 
                        "reasoning": "Continued parsing shows consistent data structure. Turn-to-turn data integrity maintained. All required fields present with valid data types."
                    }
                ],
                "workflow_evaluation": {
                    "is_compliant": True,
                    "reasoning": "System demonstrates proper Think->Plan->Act workflow adherence. Clear phase transitions observed with appropriate data flow between stages. Logical progression from problem analysis through strategy formulation to implementation execution."
                },
                "overall_assessment": {
                    "system_health": "GOOD",
                    "critical_issues": ["Minor performance optimization opportunities in token usage"],
                    "recommendations": [
                        "Consider implementing caching for repetitive analysis patterns",
                        "Add performance monitoring for turn processing times",
                        "Enhance error logging granularity for better debugging"
                    ]
                }
            }
            content = json.dumps(mock_response, indent=2, ensure_ascii=False)
        else:
            content = "Mock LLM Professional Response: Comprehensive system analysis complete. All evaluation criteria have been assessed using industry-standard methodologies."
        
        return LLMResponse(
            content=content,
            usage={"prompt_tokens": len(prompt) // 4, "completion_tokens": len(content) // 4, "total_tokens": (len(prompt) + len(content)) // 4},
            model=self.model_name,
            provider="mock",
            success=True
        )


class LLMProviderFactory:
    """LLMãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ãƒ•ã‚¡ã‚¯ãƒˆãƒªãƒ¼"""
    
    @staticmethod
    def create_provider(provider_name: str, model_name: Optional[str] = None, api_key: Optional[str] = None) -> BaseLLMProvider:
        """ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼åã«åŸºã¥ã„ã¦LLMãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ã‚’ä½œæˆ"""
        
        provider_name = provider_name.lower()
        
        if provider_name == "openai":
            model = model_name or "gpt-4"
            return OpenAIProvider(model, api_key)
        
        elif provider_name == "anthropic":
            model = model_name or "claude-3-sonnet-20240229"
            return AnthropicProvider(model, api_key)
        
        elif provider_name == "mock":
            model = model_name or "mock-gpt"
            return MockLLMProvider(model)
        
        else:
            raise ValueError(f"Unsupported provider: {provider_name}")


class LLMEvaluationManager:
    """LLMè©•ä¾¡ãƒãƒãƒ¼ã‚¸ãƒ£ãƒ¼"""
    
    def __init__(self, provider: BaseLLMProvider):
        self.provider = provider
        self.evaluation_cache = {}
    
    async def evaluate_system_compliance(self, prompt: str, cache_key: Optional[str] = None) -> LLMResponse:
        """ã‚·ã‚¹ãƒ†ãƒ ä»•æ§˜æº–æ‹ æ€§è©•ä¾¡ã‚’å®Ÿè¡Œ"""
        
        # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒã‚§ãƒƒã‚¯
        if cache_key and cache_key in self.evaluation_cache:
            cached_response = self.evaluation_cache[cache_key]
            print(f"ğŸ“‹ ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‹ã‚‰å–å¾—: {cache_key}")
            return cached_response
        
        print(f"ğŸ¤– LLMè©•ä¾¡å®Ÿè¡Œä¸­ - ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼: {self.provider.__class__.__name__}")
        print(f"ğŸ“ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆé•·: {len(prompt):,}æ–‡å­—")
        
        start_time = time.time()
        response = await self.provider.generate(prompt)
        end_time = time.time()
        
        if response.success:
            print(f"âœ… è©•ä¾¡å®Œäº† - å®Ÿè¡Œæ™‚é–“: {end_time - start_time:.2f}ç§’")
            print(f"ğŸ“Š ä½¿ç”¨é‡: {response.usage}")
            
            # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã«ä¿å­˜
            if cache_key:
                self.evaluation_cache[cache_key] = response
        else:
            print(f"âŒ è©•ä¾¡å¤±æ•—: {response.error_message}")
        
        return response
    
    def parse_evaluation_result(self, response_content: str) -> Optional[Dict[str, Any]]:
        """LLMè©•ä¾¡çµæœã‚’JSONã¨ã—ã¦è§£æ"""
        try:
            # JSONéƒ¨åˆ†ã‚’æŠ½å‡ºï¼ˆã‚³ãƒ¼ãƒ‰ãƒ–ãƒ­ãƒƒã‚¯å†…ã«ã‚ã‚‹å ´åˆï¼‰
            import re
            json_match = re.search(r'```json\s*(.*?)\s*```', response_content, re.DOTALL)
            if json_match:
                json_content = json_match.group(1)
            else:
                # å…¨ä½“ãŒJSONã®å ´åˆ
                json_content = response_content
            
            return json.loads(json_content)
            
        except (json.JSONDecodeError, AttributeError) as e:
            print(f"âš ï¸  JSONè§£æã‚¨ãƒ©ãƒ¼: {e}")
            return None
    
    async def batch_evaluate(self, prompts: List[str], concurrent_limit: int = 3) -> List[LLMResponse]:
        """è¤‡æ•°ã®ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’ä¸¦åˆ—è©•ä¾¡"""
        
        semaphore = asyncio.Semaphore(concurrent_limit)
        
        async def evaluate_single(prompt: str, index: int) -> LLMResponse:
            async with semaphore:
                print(f"ğŸ“ è©•ä¾¡ {index + 1}/{len(prompts)} é–‹å§‹...")
                return await self.provider.generate(prompt)
        
        tasks = [evaluate_single(prompt, i) for i, prompt in enumerate(prompts)]
        responses = await asyncio.gather(*tasks)
        
        successful = sum(1 for r in responses if r.success)
        print(f"ğŸ¯ ãƒãƒƒãƒè©•ä¾¡å®Œäº†: {successful}/{len(responses)} æˆåŠŸ")
        
        return responses


async def demo_llm_integration():
    """LLMçµ±åˆã®ãƒ‡ãƒ¢ãƒ³ã‚¹ãƒˆãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³"""
    print("ğŸ¤– LLMçµ±åˆã‚·ã‚¹ãƒ†ãƒ ãƒ‡ãƒ¢")
    print("=" * 50)
    
    # ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ã®ä½œæˆï¼ˆãƒ¢ãƒƒã‚¯ã‚’ä½¿ç”¨ï¼‰
    provider = LLMProviderFactory.create_provider("mock")
    manager = LLMEvaluationManager(provider)
    
    # ã‚µãƒ³ãƒ—ãƒ«ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ
    sample_prompt = """
    ## Role and Goal ##
    You are a QA Engineer evaluating system compliance.
    
    ## Log Data ##
    Turn 1: %_Thought_% Analysis complete %_Plan_% Step 1: Review code
    
    Provide parser_evaluation and workflow_evaluation in JSON format.
    """
    
    # è©•ä¾¡å®Ÿè¡Œ
    response = await manager.evaluate_system_compliance(sample_prompt, "demo_cache_key")
    
    if response.success:
        print("ğŸ¯ LLMè©•ä¾¡æˆåŠŸ!")
        print(f"ğŸ“„ å¿œç­”å†…å®¹ã®æœ€åˆã®300æ–‡å­—:")
        print(response.content[:300] + "...")
        
        # JSONè§£æ
        parsed_result = manager.parse_evaluation_result(response.content)
        if parsed_result:
            print("âœ… JSONè§£ææˆåŠŸ")
            print(f"ğŸ“Š ãƒ‘ãƒ¼ã‚µãƒ¼è©•ä¾¡çµæœæ•°: {len(parsed_result.get('parser_evaluation', []))}")
        
    else:
        print(f"âŒ LLMè©•ä¾¡å¤±æ•—: {response.error_message}")
    
    # ãƒãƒƒãƒè©•ä¾¡ã®ãƒ‡ãƒ¢
    print("\nğŸ”„ ãƒãƒƒãƒè©•ä¾¡ãƒ‡ãƒ¢...")
    batch_prompts = [sample_prompt] * 3
    batch_responses = await manager.batch_evaluate(batch_prompts, concurrent_limit=2)
    
    print(f"âœ… ãƒãƒƒãƒè©•ä¾¡å®Œäº†: {len(batch_responses)}ä»¶")


if __name__ == "__main__":
    asyncio.run(demo_llm_integration())
