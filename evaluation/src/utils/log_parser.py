"""
APRãƒ­ã‚°ãƒ‘ãƒ¼ã‚µãƒ¼ - LLMè©•ä¾¡ç”¨ãƒ‡ãƒ¼ã‚¿æŠ½å‡º
ãƒ­ã‚°ãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰ã‚·ã‚¹ãƒ†ãƒ ä»•æ§˜æº–æ‹ æ€§è©•ä¾¡ã®ãŸã‚ã®æƒ…å ±ã‚’æŠ½å‡ºã™ã‚‹
"""

import json
import re
from typing import Dict, List, Any, Optional
from dataclasses import dataclass
from pathlib import Path


@dataclass
class ParsedTurn:
    """ãƒ‘ãƒ¼ã‚¹ã•ã‚ŒãŸã‚¿ãƒ¼ãƒ³æƒ…å ±"""
    turn_number: int
    timestamp: str
    raw_llm_response: str
    system_parsed_content: Dict[str, Any]
    action_summary: str
    parsing_success: bool
    workflow_tags_found: List[str]


@dataclass
class ParsedExperiment:
    """ãƒ‘ãƒ¼ã‚¹ã•ã‚ŒãŸå®Ÿé¨“æƒ…å ±"""
    experiment_id: str
    total_turns: int
    start_time: str
    end_time: str
    status: str
    total_tokens: Dict[str, int]
    turns: List[ParsedTurn]


class APRLogParser:
    """APRãƒ­ã‚°ãƒ‘ãƒ¼ã‚µãƒ¼"""
    
    def __init__(self):
        # åˆ¶å¾¡ã‚¿ã‚°ã®å®šç¾©
        self.workflow_tags = {
            '%_Thought_%': 'Thought',
            '%_Plan_%': 'Plan',
            '%_Reply Required_%': 'Reply Required',
            '%_Modified_%': 'Modified',
            '%%_Fin_%%': 'Fin'
        }
        
        # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆç”Ÿæˆå™¨ã®åˆæœŸåŒ–
        self.prompt_generator = SystemCompliancePromptGenerator()
        
    def parse_log_file(self, log_file_path: Path) -> Optional[ParsedExperiment]:
        """ãƒ­ã‚°ãƒ•ã‚¡ã‚¤ãƒ«ã‚’è§£æã—ã¦ParsedExperimentã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã‚’è¿”ã™"""
        try:
            with open(log_file_path, 'r', encoding='utf-8') as f:
                log_data = json.load(f)
            
            return self._parse_log_data(log_data)
            
        except Exception as e:
            print(f"âš ï¸  ãƒ­ã‚°ãƒ•ã‚¡ã‚¤ãƒ«è§£æã‚¨ãƒ©ãƒ¼: {log_file_path} - {e}")
            return None
    
    def _parse_log_data(self, log_data: Dict) -> ParsedExperiment:
        """ãƒ­ã‚°ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰ParsedExperimentã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã‚’ç”Ÿæˆ"""
        
        # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã®æŠ½å‡º
        metadata = log_data.get('experiment_metadata', {})
        experiment_id = metadata.get('experiment_id', 'unknown')
        total_turns = metadata.get('total_turns', 0)
        start_time = metadata.get('start_time', '')
        end_time = metadata.get('end_time', '')
        status = metadata.get('status', 'unknown')
        total_tokens = metadata.get('total_tokens', {})
        
        # å„ã‚¿ãƒ¼ãƒ³ã®è§£æ
        turns = []
        for turn_data in log_data.get('interaction_log', []):
            parsed_turn = self._parse_turn(turn_data)
            if parsed_turn:
                turns.append(parsed_turn)
        
        return ParsedExperiment(
            experiment_id=experiment_id,
            total_turns=total_turns,
            start_time=start_time,
            end_time=end_time,
            status=status,
            total_tokens=total_tokens,
            turns=turns
        )
    
    def _parse_turn(self, turn_data: Dict) -> Optional[ParsedTurn]:
        """å˜ä¸€ã‚¿ãƒ¼ãƒ³ã®ãƒ‡ãƒ¼ã‚¿ã‚’è§£æ"""
        try:
            turn_number = turn_data.get('turn', 0)
            timestamp = turn_data.get('timestamp', '')
            
            # LLMå¿œç­”ã®å–å¾—
            llm_response = turn_data.get('llm_response', {})
            raw_content = llm_response.get('raw_content', '')
            parsed_content = llm_response.get('parsed_content', {})
            
            # ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼ã‚¿ã‚°ã®æ¤œå‡º
            workflow_tags_found = self._detect_workflow_tags(raw_content)
            action_summary = ' -> '.join(workflow_tags_found) if workflow_tags_found else 'No tags detected'
            
            # ãƒ‘ãƒ¼ã‚·ãƒ³ã‚°æˆåŠŸã®åˆ¤å®š
            parsing_success = self._evaluate_parsing_success(raw_content, parsed_content)
            
            return ParsedTurn(
                turn_number=turn_number,
                timestamp=timestamp,
                raw_llm_response=raw_content,
                system_parsed_content=parsed_content,
                action_summary=action_summary,
                parsing_success=parsing_success,
                workflow_tags_found=workflow_tags_found
            )
            
        except Exception as e:
            print(f"âš ï¸  ã‚¿ãƒ¼ãƒ³è§£æã‚¨ãƒ©ãƒ¼: {e}")
            return None
    
    def _detect_workflow_tags(self, raw_content: str) -> List[str]:
        """ç”Ÿã®LLMå¿œç­”ã‹ã‚‰ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼ã‚¿ã‚°ã‚’æ¤œå‡º"""
        found_tags = []
        
        for tag_pattern, tag_name in self.workflow_tags.items():
            # ã‚¨ã‚¹ã‚±ãƒ¼ãƒ—ã—ã¦ãƒ‘ã‚¿ãƒ¼ãƒ³ãƒãƒƒãƒãƒ³ã‚°
            escaped_pattern = re.escape(tag_pattern)
            if re.search(escaped_pattern, raw_content):
                found_tags.append(tag_name)
        
        return found_tags
    
    def _evaluate_parsing_success(self, raw_content: str, parsed_content: Dict) -> bool:
        """ãƒ‘ãƒ¼ã‚·ãƒ³ã‚°æˆåŠŸã‚’è©•ä¾¡"""
        if not raw_content:
            return False
        
        # ãƒ‘ãƒ¼ã‚¹ã•ã‚ŒãŸã‚³ãƒ³ãƒ†ãƒ³ãƒ„ãŒç©ºã§ãªã„ã‹ãƒã‚§ãƒƒã‚¯
        if not parsed_content:
            return False
        
        # ä¸»è¦ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ã®å­˜åœ¨ãƒã‚§ãƒƒã‚¯
        key_fields = ['thought', 'plan', 'reply_required', 'modified_diff']
        parsed_fields = sum(1 for field in key_fields if parsed_content.get(field) is not None)
        
        # ç”Ÿã®ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã«ã‚¿ã‚°ãŒå«ã¾ã‚Œã¦ã„ã‚‹ã‹ãƒã‚§ãƒƒã‚¯
        tags_in_raw = len(self._detect_workflow_tags(raw_content))
        
        # ãƒ‘ãƒ¼ã‚·ãƒ³ã‚°ã•ã‚ŒãŸãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰æ•°ãŒã‚¿ã‚°æ•°ã«æ¯”ä¾‹ã—ã¦ã„ã‚‹ã‹
        if tags_in_raw > 0:
            return parsed_fields > 0 and parsed_fields >= (tags_in_raw * 0.5)  # 50%ä»¥ä¸Šã®ã‚¿ã‚°ãŒãƒ‘ãƒ¼ã‚¹ã•ã‚Œã¦ã„ã‚Œã°æˆåŠŸ
        
        return parsed_fields > 0
    
    def extract_evaluation_data(self, parsed_experiment: ParsedExperiment) -> Dict[str, Any]:
        """è©•ä¾¡ç”¨LLMã«é€ä¿¡ã™ã‚‹ãŸã‚ã®ãƒ‡ãƒ¼ã‚¿ã‚’æŠ½å‡º"""
        evaluation_data = {
            "experiment_id": parsed_experiment.experiment_id,
            "total_turns": parsed_experiment.total_turns,
            "start_time": parsed_experiment.start_time,
            "end_time": parsed_experiment.end_time,
            "status": parsed_experiment.status,
            "turns": []
        }
        
        for turn in parsed_experiment.turns:
            turn_data = {
                "turn_number": turn.turn_number,
                "timestamp": turn.timestamp,
                "action_summary": turn.action_summary,
                "raw_llm_response": turn.raw_llm_response,
                "system_parsed_content": turn.system_parsed_content,
                "workflow_tags_found": turn.workflow_tags_found,
                "parsing_success": turn.parsing_success
            }
            evaluation_data["turns"].append(turn_data)
        
        return evaluation_data


class SystemCompliancePromptGenerator:
    """ã‚·ã‚¹ãƒ†ãƒ ä»•æ§˜æº–æ‹ æ€§è©•ä¾¡ç”¨ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆç”Ÿæˆå™¨"""
    
    def __init__(self):
        self.prompt_template = self._load_prompt_template()
    
    def _load_prompt_template(self) -> str:
        """ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆã‚’å®šç¾©"""
        return """## Role and Goal ##
You are a meticulous Quality Assurance (QA) Engineer. Your task is to evaluate the operational integrity of an AI agent system based on its interaction logs. You will perform two specific checks:
1. **Parser Integrity Check**: Verify if the system's internal parser correctly interpreted the raw response from the LLM.
2. **Workflow Compliance Check**: Assess if the agent's sequence of actions adheres to the documented operational workflow.

Provide your evaluation in a structured JSON format.

---
## System Specifications ##

### 1. Workflow Rules ###
The agent MUST follow a "Think -> Plan -> Act" cycle.
- **`%_Thought_%`**: The agent must first analyze the situation.
- **`%_Plan_%`**: Based on the thought, the agent must create a step-by-step plan.
- **Act**: After planning, the agent can act by requesting information (`%_Reply Required_%`) or proposing a patch (`%_Modified_%`).
- **`%%_Fin_%%`**: The agent must use this tag to signal the completion of all tasks. An agent can finish in any turn if it deems the task complete.

### 2. Parser Rules ###
The system's parser is responsible for extracting content from tags in the "Raw LLM Response" and structuring it into the "System's Parsed Content" JSON object. For example, text within `%_Thought_%` should populate the `thought` field in the JSON.

---
## Log Data to Evaluate ##

**Experiment ID**: `{experiment_id}`
**Total Turns**: `{total_turns}`
**Start Time**: `{start_time}`
**End Time**: `{end_time}`
**Status**: `{status}`

---
{turns_content}

---

## Your Task: Provide Evaluation in JSON Format

Based on the specifications and log data, provide your evaluation.

```json
{
  "parser_evaluation": [
    {
      "turn": 1,
      "status": "PASS/FAIL",
      "reasoning": "Detailed explanation of parser performance for this turn"
    }
  ],
  "workflow_evaluation": {
    "is_compliant": true/false,
    "reasoning": "Detailed explanation of workflow compliance"
  },
  "overall_assessment": {
    "system_health": "EXCELLENT/GOOD/POOR/CRITICAL",
    "critical_issues": ["List of critical issues if any"],
    "recommendations": ["List of improvement recommendations"]
  }
}
```

Please analyze each turn carefully and provide a comprehensive evaluation."""

    def generate_prompt(self, evaluation_data: Dict[str, Any]) -> str:
        """è©•ä¾¡ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’ç”Ÿæˆ"""
        
        # ã‚¿ãƒ¼ãƒ³æƒ…å ±ã®ç”Ÿæˆ
        turns_content = []
        for i, turn in enumerate(evaluation_data["turns"], 1):
            turn_section = f"""### Turn {turn["turn_number"]} ###

**Timestamp**: `{turn["timestamp"]}`
**Action Summary**: `{turn["action_summary"]}`

**Raw LLM Response**:
```
{turn["raw_llm_response"]}
```

**System's Parsed Content**:
```json
{json.dumps(turn["system_parsed_content"], indent=2, ensure_ascii=False)}
```

**Tags Found in Raw Response**: {', '.join(turn["workflow_tags_found"]) if turn["workflow_tags_found"] else 'None'}
**Parser Success**: {'âœ… SUCCESS' if turn["parsing_success"] else 'âŒ FAILED'}

---"""
            turns_content.append(turn_section)
        
        # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆã«å€¤ã‚’æŒ¿å…¥
        prompt = self.prompt_template.format(
            experiment_id=evaluation_data["experiment_id"],
            total_turns=evaluation_data["total_turns"],
            start_time=evaluation_data["start_time"],
            end_time=evaluation_data["end_time"],
            status=evaluation_data["status"],
            turns_content="\n".join(turns_content)
        )
        
        return prompt


def demo_log_parsing():
    """ãƒ­ã‚°è§£æã®ãƒ‡ãƒ¢ãƒ³ã‚¹ãƒˆãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³"""
    print("ğŸ” APRãƒ­ã‚°è§£æãƒ‡ãƒ¢")
    print("=" * 50)
    
    # ãƒ­ã‚°ãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‘ãƒ¼ã‚¹
    log_file = Path("/app/apr-logs/servantes/pullrequest/add_Secrets_service-_global_yaml/2025-07-20_10-23-50_JST.log")
    
    parser = APRLogParser()
    parsed_experiment = parser.parse_log_file(log_file)
    
    if not parsed_experiment:
        print("âŒ ãƒ­ã‚°ãƒ•ã‚¡ã‚¤ãƒ«ã®è§£æã«å¤±æ•—ã—ã¾ã—ãŸ")
        return
    
    print(f"ğŸ“Š å®Ÿé¨“æƒ…å ±:")
    print(f"  - ID: {parsed_experiment.experiment_id}")
    print(f"  - ã‚¿ãƒ¼ãƒ³æ•°: {parsed_experiment.total_turns}")
    print(f"  - ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹: {parsed_experiment.status}")
    print(f"  - ãƒˆãƒ¼ã‚¯ãƒ³æ•°: {parsed_experiment.total_tokens}")
    print()
    
    print("ğŸ”„ ã‚¿ãƒ¼ãƒ³åˆ¥åˆ†æ:")
    for turn in parsed_experiment.turns:
        print(f"  Turn {turn.turn_number}:")
        print(f"    - ã‚¢ã‚¯ã‚·ãƒ§ãƒ³: {turn.action_summary}")
        print(f"    - ãƒ‘ãƒ¼ã‚·ãƒ³ã‚°æˆåŠŸ: {'âœ…' if turn.parsing_success else 'âŒ'}")
        print(f"    - æ¤œå‡ºã‚¿ã‚°: {turn.workflow_tags_found}")
        print()
    
    # è©•ä¾¡ç”¨ãƒ‡ãƒ¼ã‚¿ã®ç”Ÿæˆ
    evaluation_data = parser.extract_evaluation_data(parsed_experiment)
    
    # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã®ç”Ÿæˆ
    prompt_generator = SystemCompliancePromptGenerator()
    evaluation_prompt = prompt_generator.generate_prompt(evaluation_data)
    
    print(f"ğŸ“ ç”Ÿæˆã•ã‚ŒãŸãƒ—ãƒ­ãƒ³ãƒ—ãƒˆé•·: {len(evaluation_prompt):,}æ–‡å­—")
    print(f"ğŸ¯ è©•ä¾¡æº–å‚™å®Œäº† - LLMã«ãƒªã‚¯ã‚¨ã‚¹ãƒˆå¯èƒ½")
    
    # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã®ä¸€éƒ¨ã‚’è¡¨ç¤º
    print("\nğŸ“„ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆä¾‹ (æœ€åˆã®500æ–‡å­—):")
    print("-" * 50)
    print(evaluation_prompt[:500] + "...")
    
    return evaluation_data, evaluation_prompt


if __name__ == "__main__":
    demo_log_parsing()
