"""
ã‚·ã‚¹ãƒ†ãƒ é©åˆæ€§è©•ä¾¡å™¨ - LLMçµ±åˆç‰ˆ
APRã‚·ã‚¹ãƒ†ãƒ ã®ãƒ‘ãƒ¼ã‚µæ•´åˆæ€§ã¨ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼é©åˆæ€§ã‚’è©•ä¾¡
"""

import json
import re
import asyncio
from pathlib import Path
from typing import Dict, List, Any, Optional
from dataclasses import dataclass, asdict
from datetime import datetime

from ..utils.log_iterator import APRLogIterator, LogEntry
from ..utils.log_parser import APRLogParser, ParsedExperiment
from ..utils.template_manager import APRPromptTemplates
from ..llm.llm_integration import LLMEvaluationManager, LLMResponse, OpenAIProvider, AnthropicProvider, MockLLMProvider


@dataclass
class ComplianceMetrics:
    """ã‚·ã‚¹ãƒ†ãƒ é©åˆæ€§ãƒ¡ãƒˆãƒªã‚¯ã‚¹"""
    control_flow_accuracy: float  # åˆ¶å¾¡ãƒ•ãƒ­ãƒ¼è§£æç²¾åº¦
    parser_success_rate: float    # ãƒ‘ãƒ¼ã‚µãƒ¼æˆåŠŸç‡
    file_processing_rate: float   # ãƒ•ã‚¡ã‚¤ãƒ«å‡¦ç†æˆåŠŸç‡
    error_handling_score: float   # ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°ã‚¹ã‚³ã‚¢
    overall_compliance: float     # ç·åˆé©åˆæ€§ã‚¹ã‚³ã‚¢


@dataclass
class LLMEvaluationResult:
    """LLMè©•ä¾¡çµæœ"""
    log_path: str
    experiment_id: str
    llm_response: LLMResponse
    parsed_evaluation: Optional[Dict[str, Any]]
    
    # æŠ½å‡ºã•ã‚ŒãŸãƒ¡ãƒˆãƒªã‚¯ã‚¹
    parser_success_rate: float
    workflow_compliance: bool
    system_health: str
    critical_issues: List[str]
    recommendations: List[str]


class SystemComplianceEvaluator:
    """ã‚·ã‚¹ãƒ†ãƒ é©åˆæ€§è©•ä¾¡å™¨ï¼ˆLLMçµ±åˆç‰ˆï¼‰"""
    
    def __init__(self, workspace_path: str = "/app", llm_provider: str = "mock", llm_model: str = None, 
                 prompt_template_style: str = "default"):
        """
        åˆæœŸåŒ–
        
        Args:
            workspace_path: ãƒ¯ãƒ¼ã‚¯ã‚¹ãƒšãƒ¼ã‚¹ãƒ‘ã‚¹
            llm_provider: LLMãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ (openai, anthropic, mock)
            llm_model: LLMãƒ¢ãƒ‡ãƒ«å
            prompt_template_style: ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆã‚¹ã‚¿ã‚¤ãƒ« (default, japanese, simple)
        """
        self.workspace_path = workspace_path
        self.log_iterator = APRLogIterator(workspace_path)  # ãƒ¯ãƒ¼ã‚¯ã‚¹ãƒšãƒ¼ã‚¹ãƒ‘ã‚¹ã‚’æ¸¡ã™
        self.log_parser = APRLogParser()
        
        # OpenAIåˆ©ç”¨å¯èƒ½ãƒ¢ãƒ‡ãƒ«ä¸€è¦§
        openai_models = [
            "gpt-4.1",                    # æœ€æ–°ã®é«˜æ€§èƒ½ãƒ¢ãƒ‡ãƒ« (2024å¹´5æœˆãƒªãƒªãƒ¼ã‚¹)
            "gpt-4.1-mini",              # è»½é‡ç‰ˆ
            "gpt-4-turbo",              # é«˜é€Ÿç‰ˆ (æœ€æ–°ã®Turbo)
            "gpt-4-turbo-preview",      # Turbo ãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼ç‰ˆ
            "gpt-4-1106-preview",       # GPT-4 Turbo (2023å¹´11æœˆç‰ˆ)
            "gpt-4-0125-preview",       # GPT-4 Turbo (2024å¹´1æœˆç‰ˆ)
            "gpt-4",                    # æ¨™æº–GPT-4
            "gpt-3.5-turbo"             # è»½é‡é«˜é€Ÿ
        ]
        
        # LLMãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ã‚’ä½œæˆ
        if llm_provider == "openai":
            # ãƒ¢ãƒ‡ãƒ«åãŒæŒ‡å®šã•ã‚Œã¦ã„ã‚Œã°ãã‚Œã‚’ä½¿ç”¨ã€ãªã‘ã‚Œã°ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ
            model_name = llm_model or "gpt-4-turbo"  # gpt-4-turboã‚’ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã«
            
            # æŒ‡å®šã•ã‚ŒãŸãƒ¢ãƒ‡ãƒ«ãŒåˆ©ç”¨å¯èƒ½ãƒªã‚¹ãƒˆã«ã‚ã‚‹ã‹ãƒã‚§ãƒƒã‚¯
            if llm_model and llm_model not in openai_models:
                print(f"âš ï¸  è­¦å‘Š: æŒ‡å®šã•ã‚ŒãŸãƒ¢ãƒ‡ãƒ« '{llm_model}' ã¯æ¨å¥¨ãƒªã‚¹ãƒˆã«ã‚ã‚Šã¾ã›ã‚“")
                print(f"ğŸ“‹ æ¨å¥¨ãƒ¢ãƒ‡ãƒ«: {', '.join(openai_models)}")
            
            provider = OpenAIProvider(model_name=model_name)
            print(f"ğŸ¤– OpenAI ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼åˆæœŸåŒ–å®Œäº† - ãƒ¢ãƒ‡ãƒ«: {model_name}")
            
        elif llm_provider == "anthropic":
            # Anthropicã®ãƒ¢ãƒ‡ãƒ«åã‚‚æŒ‡å®šå¯èƒ½
            anthropic_models = [
                "claude-3-opus-20240229",     # æœ€é«˜æ€§èƒ½
                "claude-3-sonnet-20240229",   # ãƒãƒ©ãƒ³ã‚¹å‹
                "claude-3-haiku-20240307"     # é«˜é€Ÿè»½é‡
            ]
            model_name = llm_model or "claude-3-sonnet-20240229"
            
            if llm_model and llm_model not in anthropic_models:
                print(f"âš ï¸  è­¦å‘Š: æŒ‡å®šã•ã‚ŒãŸãƒ¢ãƒ‡ãƒ« '{llm_model}' ã¯æ¨å¥¨ãƒªã‚¹ãƒˆã«ã‚ã‚Šã¾ã›ã‚“")
                print(f"ğŸ“‹ æ¨å¥¨ãƒ¢ãƒ‡ãƒ«: {', '.join(anthropic_models)}")
            
            provider = AnthropicProvider(model_name=model_name)
            print(f"ğŸ¤– Anthropic ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼åˆæœŸåŒ–å®Œäº† - ãƒ¢ãƒ‡ãƒ«: {model_name}")
            
        else:  # "mock" ã¾ãŸã¯ä»–ã®å ´åˆã¯Mockãƒ—ãƒ­ãƒã‚¤ãƒ€ã‚’ä½¿ç”¨
            provider = MockLLMProvider()
            print(f"ğŸ­ Mock ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼åˆæœŸåŒ–å®Œäº† - ãƒ†ã‚¹ãƒˆãƒ¢ãƒ¼ãƒ‰")
        
        self.llm_manager = LLMEvaluationManager(provider)
        self.results: List[LLMEvaluationResult] = []
        
        # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆç®¡ç†ã‚’åˆæœŸåŒ–
        self.prompt_templates = APRPromptTemplates(template_style=prompt_template_style)
        
        # ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆã®å¦¥å½“æ€§ã‚’ãƒã‚§ãƒƒã‚¯
        is_valid, missing_vars = self.prompt_templates.validate_evaluation_template()
        if not is_valid:
            print(f"âš ï¸  è­¦å‘Š: è©•ä¾¡ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆã«ä¸è¶³å¤‰æ•°ãŒã‚ã‚Šã¾ã™: {missing_vars}")
        else:
            print("âœ… è©•ä¾¡ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆã®å¦¥å½“æ€§ç¢ºèªå®Œäº†")
    
    async def evaluate_single_repository(self, repository_name: str, max_logs: int = 5) -> Dict[str, Any]:
        """ç‰¹å®šã®ãƒªãƒã‚¸ãƒˆãƒªã®ãƒ­ã‚°ã‚’è©•ä¾¡ï¼ˆå‹•ä½œæ¤œè¨¼ç”¨ï¼‰"""
        print(f"ğŸ” å˜ä¸€ãƒªãƒã‚¸ãƒˆãƒªè©•ä¾¡é–‹å§‹: {repository_name}")
        print(f"ğŸ“Š æœ€å¤§å‡¦ç†ãƒ­ã‚°æ•°: {max_logs}ä»¶")
        print("=" * 50)
        
        start_time = datetime.now()
        self.results = []
        
        # æŒ‡å®šãƒªãƒã‚¸ãƒˆãƒªã®ãƒ­ã‚°ã‚’å·¡å›ã—ã¦è©•ä¾¡
        processed_count = 0
        evaluation_tasks = []
        
        for log_entry in self.log_iterator.iterate_all_logs():
            # æŒ‡å®šã•ã‚ŒãŸãƒªãƒã‚¸ãƒˆãƒªã®ãƒ­ã‚°ã®ã¿å‡¦ç†
            if log_entry.project_name != repository_name:
                continue
                
            if max_logs is not None and processed_count >= max_logs:  # æŒ‡å®šä»¶æ•°ã¾ã§åˆ¶é™
                break
                
            try:
                print(f"ğŸ”„ å‡¦ç†ä¸­ ({processed_count + 1}/{max_logs if max_logs is not None else 'å…¨ä»¶'}): {log_entry.log_path}")
                
                # ãƒ­ã‚°ãƒ•ã‚¡ã‚¤ãƒ«ã‚’è§£æ
                parsed_experiment = self.log_parser.parse_log_file(log_entry.log_path)
                if not parsed_experiment:
                    print(f"âš ï¸  ãƒ‘ãƒ¼ã‚¹å¤±æ•—: {log_entry.log_path}")
                    continue
                
                print(f"âœ… ãƒ‘ãƒ¼ã‚¹æˆåŠŸ: ID={parsed_experiment.experiment_id}, Turns={len(parsed_experiment.turns)}")
                
                # è©•ä¾¡ã‚¿ã‚¹ã‚¯ã‚’ä½œæˆï¼ˆéåŒæœŸå‡¦ç†ç”¨ï¼‰
                task = self._evaluate_single_log_async(log_entry, parsed_experiment)
                evaluation_tasks.append(task)
                processed_count += 1
                    
            except Exception as e:
                print(f"âš ï¸  ãƒ­ã‚°è©•ä¾¡ã‚¨ãƒ©ãƒ¼: {log_entry.log_path} - {e}")
                continue
        
        if processed_count == 0:
            print(f"âŒ æŒ‡å®šã•ã‚ŒãŸãƒªãƒã‚¸ãƒˆãƒª '{repository_name}' ã®ãƒ­ã‚°ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸ")
            print("ğŸ“‹ åˆ©ç”¨å¯èƒ½ãªãƒªãƒã‚¸ãƒˆãƒª:")
            for project in self.log_iterator.get_project_names():
                print(f"  - {project}")
            return self._create_empty_evaluation_result("no_logs_found")
        
        # è©•ä¾¡ã‚¿ã‚¹ã‚¯ã‚’å®Ÿè¡Œ
        if evaluation_tasks:
            print(f"ğŸš€ {len(evaluation_tasks)}ä»¶ã®ãƒ­ã‚°ã‚’ä¸¦åˆ—è©•ä¾¡ä¸­...")
            batch_results = await asyncio.gather(*evaluation_tasks, return_exceptions=True)
            self._process_batch_results(batch_results)
        
        end_time = datetime.now()
        processing_time = (end_time - start_time).total_seconds()
        
        # çµæœé›†è¨ˆ
        metrics = self._calculate_llm_metrics()
        
        evaluation_result = {
            "evaluation_type": "single_repository_compliance",
            "repository_name": repository_name,
            "timestamp": end_time.isoformat(),
            "processing_time_seconds": processing_time,
            "llm_provider": self.llm_manager.provider.__class__.__name__,
            "llm_model": self.llm_manager.provider.model_name,
            "summary": {
                "total_logs_processed": len(self.results),
                "successful_evaluations": len([r for r in self.results if r.llm_response.success]),
                "failed_evaluations": len([r for r in self.results if not r.llm_response.success]),
                "parser_issues_detected": len([r for r in self.results if r.parser_success_rate < 0.5]),
                "workflow_violations": len([r for r in self.results if not r.workflow_compliance]),
                "critical_systems": len([r for r in self.results if r.system_health == "CRITICAL"])
            },
            "compliance_metrics": asdict(metrics),
            "detailed_results": [self._serialize_result(result) for result in self.results]
        }
        
        print(f"âœ… å˜ä¸€ãƒªãƒã‚¸ãƒˆãƒªè©•ä¾¡å®Œäº†: {repository_name}")
        print(f"  - å‡¦ç†æ™‚é–“: {processing_time:.2f}ç§’")
        print(f"  - å‡¦ç†ãƒ­ã‚°æ•°: {len(self.results)}")
        print(f"  - LLMæˆåŠŸç‡: {evaluation_result['summary']['successful_evaluations']}/{len(self.results)}")
        print(f"  - ç·åˆé©åˆæ€§ã‚¹ã‚³ã‚¢: {metrics.overall_compliance:.3f}")
        
        return evaluation_result
    
    def _create_empty_evaluation_result(self, reason: str) -> Dict[str, Any]:
        """ç©ºã®è©•ä¾¡çµæœã‚’ä½œæˆ"""
        return {
            "evaluation_type": "single_repository_compliance",
            "repository_name": "unknown",
            "timestamp": datetime.now().isoformat(),
            "processing_time_seconds": 0.0,
            "llm_provider": self.llm_manager.provider.__class__.__name__,
            "llm_model": self.llm_manager.provider.model_name,
            "summary": {
                "total_logs_processed": 0,
                "successful_evaluations": 0,
                "failed_evaluations": 0,
                "parser_issues_detected": 0,
                "workflow_violations": 0,
                "critical_systems": 0
            },
            "compliance_metrics": asdict(ComplianceMetrics(0.0, 0.0, 0.0, 0.0, 0.0)),
            "detailed_results": [],
            "error_reason": reason
        }
    
    async def evaluate_all_projects(self) -> Dict[str, Any]:
        """å…¨ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã®ãƒ­ã‚°ã‚’è©•ä¾¡"""
        print("ğŸ” Step 1: ã‚·ã‚¹ãƒ†ãƒ é©åˆæ€§è©•ä¾¡é–‹å§‹ï¼ˆLLMçµ±åˆç‰ˆï¼‰")
        print("=" * 50)
        
        start_time = datetime.now()
        self.results = []
        
        # å…¨ãƒ­ã‚°ã‚’å·¡å›ã—ã¦è©•ä¾¡ï¼ˆæœ€åˆã®5ä»¶ã®ã¿ãƒ†ã‚¹ãƒˆç”¨ï¼‰
        processed_count = 0
        evaluation_tasks = []
        
        for log_entry in self.log_iterator.iterate_all_logs():
            if processed_count >= 5:  # ãƒ†ã‚¹ãƒˆç”¨ã«åˆ¶é™
                break
                
            try:
                # ãƒ­ã‚°ãƒ•ã‚¡ã‚¤ãƒ«ã‚’è§£æ
                parsed_experiment = self.log_parser.parse_log_file(log_entry.log_path)
                if not parsed_experiment:
                    continue
                
                # è©•ä¾¡ã‚¿ã‚¹ã‚¯ã‚’ä½œæˆï¼ˆéåŒæœŸå‡¦ç†ç”¨ï¼‰
                task = self._evaluate_single_log_async(log_entry, parsed_experiment)
                evaluation_tasks.append(task)
                processed_count += 1
                    
            except Exception as e:
                print(f"âš ï¸  ãƒ­ã‚°è©•ä¾¡ã‚¨ãƒ©ãƒ¼: {log_entry.log_path} - {e}")
                continue
        
        # è©•ä¾¡ã‚¿ã‚¹ã‚¯ã‚’å®Ÿè¡Œ
        if evaluation_tasks:
            print(f"ğŸš€ {len(evaluation_tasks)}ä»¶ã®ãƒ­ã‚°ã‚’ä¸¦åˆ—è©•ä¾¡ä¸­...")
            batch_results = await asyncio.gather(*evaluation_tasks, return_exceptions=True)
            self._process_batch_results(batch_results)
        
        end_time = datetime.now()
        processing_time = (end_time - start_time).total_seconds()
        
        # çµæœé›†è¨ˆ
        metrics = self._calculate_llm_metrics()
        
        evaluation_result = {
            "evaluation_type": "system_compliance_llm",
            "timestamp": end_time.isoformat(),
            "processing_time_seconds": processing_time,
            "llm_provider": self.llm_manager.provider.__class__.__name__,
            "llm_model": self.llm_manager.provider.model_name,
            "summary": {
                "total_logs_processed": len(self.results),
                "successful_evaluations": len([r for r in self.results if r.llm_response.success]),
                "failed_evaluations": len([r for r in self.results if not r.llm_response.success]),
                "parser_issues_detected": len([r for r in self.results if r.parser_success_rate < 0.5]),
                "workflow_violations": len([r for r in self.results if not r.workflow_compliance]),
                "critical_systems": len([r for r in self.results if r.system_health == "CRITICAL"])
            },
            "compliance_metrics": asdict(metrics),
            "project_breakdown": self._get_project_breakdown(),
            "detailed_results": [self._serialize_result(result) for result in self.results]
        }
        
        print(f"âœ… ã‚·ã‚¹ãƒ†ãƒ é©åˆæ€§è©•ä¾¡å®Œäº†ï¼ˆLLMçµ±åˆç‰ˆï¼‰")
        print(f"  - å‡¦ç†æ™‚é–“: {processing_time:.2f}ç§’")
        print(f"  - å‡¦ç†ãƒ­ã‚°æ•°: {len(self.results)}")
        print(f"  - LLMæˆåŠŸç‡: {evaluation_result['summary']['successful_evaluations']}/{len(self.results)}")
        print(f"  - ç·åˆé©åˆæ€§ã‚¹ã‚³ã‚¢: {metrics.overall_compliance:.3f}")
        
        return evaluation_result
    
    async def _evaluate_single_log_async(self, log_entry: LogEntry, 
                                       parsed_experiment: ParsedExperiment) -> LLMEvaluationResult:
        """å˜ä¸€ãƒ­ã‚°ã®éåŒæœŸè©•ä¾¡"""
        
        try:
            # è©•ä¾¡ç”¨ãƒ‡ãƒ¼ã‚¿ã‚’æŠ½å‡º
            evaluation_data = self.log_parser.extract_evaluation_data(parsed_experiment)
            
            # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’ç”Ÿæˆ
            prompt = self._generate_evaluation_prompt(evaluation_data)
            
            # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚­ãƒ¼ã‚’ç”Ÿæˆ
            cache_key = f"{parsed_experiment.experiment_id}_{len(parsed_experiment.turns)}"
            
            # LLMè©•ä¾¡ã‚’å®Ÿè¡Œ
            llm_response = await self.llm_manager.evaluate_system_compliance(prompt, cache_key)
            
            # è©•ä¾¡çµæœã‚’è§£æ
            parsed_evaluation = None
            if llm_response.success:
                parsed_evaluation = self.llm_manager.parse_evaluation_result(llm_response.content)
            
            # ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã‚’æŠ½å‡º
            parser_success_rate, workflow_compliance, system_health, critical_issues, recommendations = \
                self._extract_metrics_from_evaluation(parsed_evaluation)
            
            return LLMEvaluationResult(
                log_path=str(log_entry.log_path),
                experiment_id=parsed_experiment.experiment_id,
                llm_response=llm_response,
                parsed_evaluation=parsed_evaluation,
                parser_success_rate=parser_success_rate,
                workflow_compliance=workflow_compliance,
                system_health=system_health,
                critical_issues=critical_issues,
                recommendations=recommendations
            )
            
        except Exception as e:
            # ã‚¨ãƒ©ãƒ¼ã®å ´åˆã¯ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤ã§çµæœã‚’ä½œæˆ
            return LLMEvaluationResult(
                log_path=str(log_entry.log_path),
                experiment_id=parsed_experiment.experiment_id if parsed_experiment else "unknown",
                llm_response=LLMResponse("", {}, "", "", False, str(e)),
                parsed_evaluation=None,
                parser_success_rate=0.0,
                workflow_compliance=False,
                system_health="CRITICAL",
                critical_issues=[f"Evaluation error: {str(e)}"],
                recommendations=["Fix evaluation pipeline"]
            )
    
    def _generate_evaluation_prompt(self, evaluation_data: Dict[str, Any]) -> str:
        """ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰è©•ä¾¡ç”¨ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’ç”Ÿæˆ"""
        try:
            return self.prompt_templates.get_evaluation_prompt(evaluation_data)
        except Exception as e:
            print(f"âš ï¸  ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆãƒ—ãƒ­ãƒ³ãƒ—ãƒˆç”Ÿæˆã‚¨ãƒ©ãƒ¼: {e}")
            print("ğŸ’¡ ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’ä½¿ç”¨ã—ã¾ã™")
            return self._generate_fallback_prompt(evaluation_data)
    
    def _generate_fallback_prompt(self, evaluation_data: Dict[str, Any]) -> str:
        """ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ç”¨ã®ãƒãƒ¼ãƒ‰ã‚³ãƒ¼ãƒ‰ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ"""
        return f"""## APR System Compliance Evaluation ##
You are evaluating an Automated Program Repair (APR) system.

## Analysis Data ##
Experiment ID: {evaluation_data.get('experiment_id', 'N/A')}
Total Processing Turns: {evaluation_data.get('turn_count', 0)}
Overall Status: {evaluation_data.get('overall_status', 'N/A')}

## Raw Data ##
{json.dumps(evaluation_data.get('turns_data', [])[:2], indent=2, ensure_ascii=False)}

## Task ##
Evaluate parser integrity, workflow compliance, and system reliability.
Provide assessment in JSON format with parser_evaluation, workflow_evaluation, and overall_assessment sections.

Focus on identifying technical issues and providing actionable recommendations."""
    
    def _process_batch_results(self, batch_results: List):
        """ãƒãƒƒãƒçµæœã‚’å‡¦ç†"""
        for result in batch_results:
            if isinstance(result, Exception):
                print(f"âš ï¸  ãƒãƒƒãƒå‡¦ç†ã‚¨ãƒ©ãƒ¼: {result}")
                continue
            
            if isinstance(result, LLMEvaluationResult):
                self.results.append(result)
    
    def _extract_metrics_from_evaluation(self, parsed_evaluation: Optional[Dict[str, Any]]) -> tuple:
        """LLMè©•ä¾¡çµæœã‹ã‚‰ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã‚’æŠ½å‡º"""
        
        if not parsed_evaluation:
            return 0.0, False, "CRITICAL", ["No evaluation available"], ["Fix evaluation system"]
        
        # ãƒ‘ãƒ¼ã‚µãƒ¼æˆåŠŸç‡ã®è¨ˆç®—
        parser_evals = parsed_evaluation.get("parser_evaluation", [])
        if parser_evals:
            successful_parses = sum(1 for eval_item in parser_evals if eval_item.get("status") == "PASS")
            parser_success_rate = successful_parses / len(parser_evals)
        else:
            parser_success_rate = 0.0
        
        # ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼é©åˆæ€§
        workflow_eval = parsed_evaluation.get("workflow_evaluation", {})
        workflow_compliance = workflow_eval.get("is_compliant", False)
        
        # ã‚·ã‚¹ãƒ†ãƒ å¥å…¨æ€§
        overall_assessment = parsed_evaluation.get("overall_assessment", {})
        system_health = overall_assessment.get("system_health", "POOR")
        critical_issues = overall_assessment.get("critical_issues", [])
        recommendations = overall_assessment.get("recommendations", [])
        
        return parser_success_rate, workflow_compliance, system_health, critical_issues, recommendations
    
    def _calculate_llm_metrics(self) -> ComplianceMetrics:
        """LLMè©•ä¾¡çµæœã‹ã‚‰å…¨ä½“ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã‚’è¨ˆç®—"""
        
        if not self.results:
            return ComplianceMetrics(0.0, 0.0, 0.0, 0.0, 0.0)
        
        successful_results = [r for r in self.results if r.llm_response.success]
        
        if not successful_results:
            return ComplianceMetrics(0.0, 0.0, 0.0, 0.0, 0.0)
        
        # ãƒ‘ãƒ¼ã‚µãƒ¼æˆåŠŸç‡ã®å¹³å‡
        parser_success_rate = sum(r.parser_success_rate for r in successful_results) / len(successful_results)
        
        # ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼é©åˆç‡
        workflow_compliance_rate = sum(1 for r in successful_results if r.workflow_compliance) / len(successful_results)
        
        # ã‚·ã‚¹ãƒ†ãƒ å¥å…¨æ€§ã‚¹ã‚³ã‚¢
        health_scores = {"EXCELLENT": 1.0, "GOOD": 0.8, "POOR": 0.4, "CRITICAL": 0.0}
        system_health_score = sum(health_scores.get(r.system_health, 0.0) for r in successful_results) / len(successful_results)
        
        # ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°ã‚¹ã‚³ã‚¢ï¼ˆã‚¯ãƒªãƒ†ã‚£ã‚«ãƒ«å•é¡ŒãŒå°‘ãªã„ã»ã©é«˜ã„ï¼‰
        avg_critical_issues = sum(len(r.critical_issues) for r in successful_results) / len(successful_results)
        error_handling_score = max(0.0, 1.0 - (avg_critical_issues / 5.0))  # 5å€‹ä»¥ä¸Šã§0ç‚¹
        
        # ç·åˆé©åˆæ€§ã‚¹ã‚³ã‚¢
        overall_compliance = (
            parser_success_rate * 0.3 +
            workflow_compliance_rate * 0.25 +
            system_health_score * 0.25 +
            error_handling_score * 0.2
        )
        
        return ComplianceMetrics(
            control_flow_accuracy=workflow_compliance_rate,
            parser_success_rate=parser_success_rate,
            file_processing_rate=system_health_score,
            error_handling_score=error_handling_score,
            overall_compliance=overall_compliance
        )
    
    def _get_project_breakdown(self) -> Dict[str, Dict]:
        """ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆåˆ¥å†…è¨³ã®å–å¾—"""
        project_results = {}
        
        for result in self.results:
            project_name = result.experiment_id.split('/')[0] if '/' in result.experiment_id else 'unknown'
            
            if project_name not in project_results:
                project_results[project_name] = {
                    "logs_count": 0,
                    "successful_evaluations": 0,
                    "parser_success_rate": 0.0,
                    "workflow_compliance_rate": 0.0,
                    "critical_issues_count": 0,
                    "avg_system_health": "UNKNOWN"
                }
            
            proj_stats = project_results[project_name]
            proj_stats["logs_count"] += 1
            proj_stats["successful_evaluations"] += 1 if result.llm_response.success else 0
            proj_stats["critical_issues_count"] += len(result.critical_issues)
        
        # ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆåˆ¥å¹³å‡å€¤ã‚’è¨ˆç®—
        for project_name, stats in project_results.items():
            project_results_list = [r for r in self.results 
                                  if (r.experiment_id.split('/')[0] if '/' in r.experiment_id else 'unknown') == project_name]
            
            if project_results_list:
                successful_results = [r for r in project_results_list if r.llm_response.success]
                
                if successful_results:
                    stats["parser_success_rate"] = sum(r.parser_success_rate for r in successful_results) / len(successful_results)
                    stats["workflow_compliance_rate"] = sum(1 for r in successful_results if r.workflow_compliance) / len(successful_results)
                    
                    # æœ€ã‚‚ä¸€èˆ¬çš„ãªã‚·ã‚¹ãƒ†ãƒ å¥å…¨æ€§ãƒ¬ãƒ™ãƒ«
                    health_counts = {}
                    for r in successful_results:
                        health_counts[r.system_health] = health_counts.get(r.system_health, 0) + 1
                    
                    if health_counts:
                        stats["avg_system_health"] = max(health_counts.items(), key=lambda x: x[1])[0]
        
        return project_results
    
    def _serialize_result(self, result: LLMEvaluationResult) -> Dict[str, Any]:
        """è©•ä¾¡çµæœã‚’ã‚·ãƒªã‚¢ãƒ©ã‚¤ã‚º"""
        return {
            "log_path": result.log_path,
            "experiment_id": result.experiment_id,
            "llm_success": result.llm_response.success,
            "llm_error": result.llm_response.error_message,
            "parser_success_rate": result.parser_success_rate,
            "workflow_compliance": result.workflow_compliance,
            "system_health": result.system_health,
            "critical_issues": result.critical_issues,
            "recommendations": result.recommendations,
            "llm_usage": result.llm_response.usage
        }


async def demo_compliance_evaluation():
    """é©åˆæ€§è©•ä¾¡ã®ãƒ‡ãƒ¢"""
    print("ğŸ” ã‚·ã‚¹ãƒ†ãƒ é©åˆæ€§è©•ä¾¡ãƒ‡ãƒ¢ï¼ˆLLMçµ±åˆç‰ˆï¼‰")
    print("=" * 60)
    
    # ãƒ‡ãƒ¢ç”¨ã«è¤‡æ•°ã®ãƒ¢ãƒ‡ãƒ«ã‚’è©¦è¡Œ
    demo_configs = [
        {"provider": "openai", "model": "gpt-4-turbo", "description": "OpenAI GPT-4 Turbo (æ¨å¥¨ãƒ»é«˜æ€§èƒ½)"},
        {"provider": "openai", "model": "gpt-4-1106-preview", "description": "OpenAI GPT-4 Turbo (2023å¹´11æœˆç‰ˆ)"},
        {"provider": "openai", "model": "gpt-4.1", "description": "OpenAI gpt-4.1 (æœ€æ–°)"},
        {"provider": "openai", "model": "gpt-4.1-mini", "description": "OpenAI gpt-4.1 Mini (è»½é‡)"},
        {"provider": "mock", "model": None, "description": "Mock LLM (ãƒ†ã‚¹ãƒˆç”¨)"}
    ]
    
    print("ğŸ“‹ åˆ©ç”¨å¯èƒ½ãªè¨­å®š:")
    for i, config in enumerate(demo_configs, 1):
        print(f"  {i}. {config['description']}")
    
    # ãƒ‡ãƒ¢ç”¨ã«Mockã‚’ä½¿ç”¨ï¼ˆæœ¬ç•ªã§ã¯é©åˆ‡ãªãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ã‚’é¸æŠï¼‰
    selected_config = demo_configs[3]  # Mockã‚’é¸æŠ
    
    print(f"\nğŸš€ å®Ÿè¡Œè¨­å®š: {selected_config['description']}")
    print("-" * 60)
    
    # è©•ä¾¡å™¨ã‚’åˆæœŸåŒ–
    evaluator = SystemComplianceEvaluator(
        llm_provider=selected_config["provider"], 
        llm_model=selected_config["model"]
    )
    
    # è©•ä¾¡å®Ÿè¡Œ
    results = await evaluator.evaluate_all_projects()
    
    print("\nğŸ“Š è©•ä¾¡çµæœã‚µãƒãƒªãƒ¼:")
    print(f"  - ç·åˆé©åˆæ€§ã‚¹ã‚³ã‚¢: {results['compliance_metrics']['overall_compliance']:.3f}")
    print(f"  - ãƒ‘ãƒ¼ã‚µãƒ¼æˆåŠŸç‡: {results['compliance_metrics']['parser_success_rate']:.3f}")
    print(f"  - ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼é©åˆç‡: {results['compliance_metrics']['control_flow_accuracy']:.3f}")
    print(f"  - LLMè©•ä¾¡æˆåŠŸç‡: {results['summary']['successful_evaluations']}/{results['summary']['total_logs_processed']}")
    print(f"  - ä½¿ç”¨ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼: {results['llm_provider']}")
    print(f"  - ä½¿ç”¨ãƒ¢ãƒ‡ãƒ«: {results['llm_model']}")
    
    print("\nğŸ’¡ ä»–ã®OpenAIãƒ¢ãƒ‡ãƒ«ã‚’è©¦ã™ã«ã¯:")
    print('  evaluator = SystemComplianceEvaluator(llm_provider="openai", llm_model="gpt-4.1-mini")')
    print('  evaluator = SystemComplianceEvaluator(llm_provider="openai", llm_model="gpt-4-turbo")')
    print('  evaluator = SystemComplianceEvaluator(llm_provider="openai", llm_model="gpt-3.5-turbo")')
    

# å€‹åˆ¥ãƒ¢ãƒ‡ãƒ«å®Ÿè¡Œç”¨ã®ãƒ˜ãƒ«ãƒ‘ãƒ¼é–¢æ•°ã‚‚è¿½åŠ 
async def run_with_specific_model(provider: str = "openai", model: str = "gpt-4.1"):
    """æŒ‡å®šã•ã‚ŒãŸãƒ¢ãƒ‡ãƒ«ã§è©•ä¾¡ã‚’å®Ÿè¡Œ"""
    print(f"ğŸ¯ æŒ‡å®šãƒ¢ãƒ‡ãƒ«ã§ã®è©•ä¾¡: {provider}/{model}")
    
    evaluator = SystemComplianceEvaluator(
        llm_provider=provider,
        llm_model=model
    )
    
    results = await evaluator.evaluate_all_projects()
    return results


if __name__ == "__main__":
    asyncio.run(demo_compliance_evaluation())
