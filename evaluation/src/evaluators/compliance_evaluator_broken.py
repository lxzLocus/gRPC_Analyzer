"""
Step 1: ã‚·ã‚¹ãƒ†ãƒ é©åˆæ€§è©•ä¾¡å™¨
APRãƒ­ã‚°ã‚’è§£æã—ã¦ã‚·ã‚¹ãƒ†ãƒ ã®å‹•ä½œé©åˆæ€§ã‚’è©•ä¾¡ã™ã‚‹
"""

import json
import re
import asyncio
from pathlib import Path
from typing import Dict, List, Any, Optional
from dataclasses import dataclass, asdict
from datetime import datetime

from ..utils.log_iterator import APRLogIterator, LogEntry, ProjectLogs
from ..utils.log_parser import APRLogParser, ParsedExperiment, SystemCompliancePromptGenerator
from ..llm.llm_integration import LLMProviderFactory, LLMEvaluationManager, LLMResponse


@dataclass
class ComplianceMetrics:
    """ã‚·ã‚¹ãƒ†ãƒ é©åˆæ€§ãƒ¡ãƒˆãƒªã‚¯ã‚¹"""
    control_flow_accuracy: float  # åˆ¶å¾¡ãƒ•ãƒ­ãƒ¼è§£æç²¾åº¦
    parser_success_rate: float    # ãƒ‘ãƒ¼ã‚µãƒ¼æˆåŠŸç‡
    file_processing_rate: float   # ãƒ•ã‚¡ã‚¤ãƒ«å‡¦ç†æˆåŠŸç‡
    error_handling_score: float   # ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°ã‚¹ã‚³ã‚¢
    overall_compliance: float     # ç·åˆé©åˆæ€§ã‚¹ã‚³ã‚¢


@dataclass
class LLMEvaluationResult:
    """LLMè©•ä¾¡çµæœ"""
    log_path: str
    experiment_id: str
    llm_response: LLMResponse
    parsed_evaluation: Optional[Dict[str, Any]]
    
    # æŠ½å‡ºã•ã‚ŒãŸãƒ¡ãƒˆãƒªã‚¯ã‚¹
    parser_success_rate: float
    workflow_compliance: bool
    system_health: str
    critical_issues: List[str]
    recommendations: List[str]


class SystemComplianceEvaluator:
    """ã‚·ã‚¹ãƒ†ãƒ é©åˆæ€§è©•ä¾¡å™¨ï¼ˆLLMçµ±åˆç‰ˆï¼‰"""
    
    def __init__(self, apr_logs_path: str = "/app/apr-logs", 
                 llm_provider: str = "mock", 
                 llm_model: Optional[str] = None):
        self.apr_logs_path = Path(apr_logs_path)
        self.log_iterator = APRLogIterator(apr_logs_path)
        
        # LLMçµ±åˆã®ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—
        provider = LLMProviderFactory.create_provider(llm_provider, llm_model)
        self.llm_manager = LLMEvaluationManager(provider)
        
        # ãƒ­ã‚°ãƒ‘ãƒ¼ã‚µãƒ¼ã¨ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆç”Ÿæˆå™¨
        self.log_parser = APRLogParser()
        self.prompt_generator = SystemCompliancePromptGenerator()
        
        self.results: List[LLMEvaluationResult] = []
    
    async def evaluate_all_projects(self) -> Dict[str, Any]:
        """å…¨ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã®ãƒ­ã‚°ã‚’è©•ä¾¡"""
        print("ğŸ” Step 1: ã‚·ã‚¹ãƒ†ãƒ é©åˆæ€§è©•ä¾¡é–‹å§‹ï¼ˆLLMçµ±åˆç‰ˆï¼‰")
        print("=" * 50)
        
        start_time = datetime.now()
        self.results = []
        
        # å…¨ãƒ­ã‚°ã‚’å·¡å›ã—ã¦è©•ä¾¡ï¼ˆæœ€åˆã®5ä»¶ã®ã¿ãƒ†ã‚¹ãƒˆç”¨ï¼‰
        processed_count = 0
        evaluation_tasks = []
        
        for log_entry in self.log_iterator.iterate_all_logs():
            if processed_count >= 5:  # ãƒ†ã‚¹ãƒˆç”¨ã«åˆ¶é™
                break
                
            try:
                # ãƒ­ã‚°ãƒ•ã‚¡ã‚¤ãƒ«ã‚’è§£æ
                parsed_experiment = self.log_parser.parse_log_file(log_entry.log_path)
                if not parsed_experiment:
                    continue
                
                # è©•ä¾¡ã‚¿ã‚¹ã‚¯ã‚’ä½œæˆï¼ˆéåŒæœŸå‡¦ç†ç”¨ï¼‰
                task = self._evaluate_single_log_async(log_entry, parsed_experiment)
                evaluation_tasks.append(task)
                processed_count += 1
                    
            except Exception as e:
                print(f"âš ï¸  ãƒ­ã‚°è©•ä¾¡ã‚¨ãƒ©ãƒ¼: {log_entry.log_path} - {e}")
                continue
        
        # è©•ä¾¡ã‚¿ã‚¹ã‚¯ã‚’å®Ÿè¡Œ
        if evaluation_tasks:
            print(f"ğŸš€ {len(evaluation_tasks)}ä»¶ã®ãƒ­ã‚°ã‚’ä¸¦åˆ—è©•ä¾¡ä¸­...")
            batch_results = await asyncio.gather(*evaluation_tasks, return_exceptions=True)
            self._process_batch_results(batch_results)
        
        end_time = datetime.now()
        processing_time = (end_time - start_time).total_seconds()
        
        # çµæœé›†è¨ˆ
        metrics = self._calculate_llm_metrics()
        
        evaluation_result = {
            "evaluation_type": "system_compliance_llm",
            "timestamp": end_time.isoformat(),
            "processing_time_seconds": processing_time,
            "llm_provider": self.llm_manager.provider.__class__.__name__,
            "llm_model": self.llm_manager.provider.model_name,
            "summary": {
                "total_logs_processed": len(self.results),
                "successful_evaluations": len([r for r in self.results if r.llm_response.success]),
                "failed_evaluations": len([r for r in self.results if not r.llm_response.success]),
                "parser_issues_detected": len([r for r in self.results if r.parser_success_rate < 0.5]),
                "workflow_violations": len([r for r in self.results if not r.workflow_compliance]),
                "critical_systems": len([r for r in self.results if r.system_health == "CRITICAL"])
            },
            "compliance_metrics": asdict(metrics),
            "project_breakdown": self._get_project_breakdown(),
            "detailed_results": [self._serialize_result(result) for result in self.results]
        }
        
        print(f"âœ… ã‚·ã‚¹ãƒ†ãƒ é©åˆæ€§è©•ä¾¡å®Œäº†ï¼ˆLLMçµ±åˆç‰ˆï¼‰")
        print(f"  - å‡¦ç†æ™‚é–“: {processing_time:.2f}ç§’")
        print(f"  - å‡¦ç†ãƒ­ã‚°æ•°: {len(self.results)}")
        print(f"  - LLMæˆåŠŸç‡: {evaluation_result['summary']['successful_evaluations']}/{len(self.results)}")
        print(f"  - ç·åˆé©åˆæ€§ã‚¹ã‚³ã‚¢: {metrics.overall_compliance:.3f}")
        
        return evaluation_result
    
    async def _evaluate_single_log_async(self, log_entry: LogEntry, 
                                       parsed_experiment: ParsedExperiment) -> LLMEvaluationResult:
        """å˜ä¸€ãƒ­ã‚°ã®éåŒæœŸè©•ä¾¡"""
        
        try:
            # è©•ä¾¡ç”¨ãƒ‡ãƒ¼ã‚¿ã‚’æŠ½å‡º
            evaluation_data = self.log_parser.extract_evaluation_data(parsed_experiment)
            
            # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’ç”Ÿæˆ
            prompt = self.prompt_generator.generate_prompt(evaluation_data)
            
            # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚­ãƒ¼ã‚’ç”Ÿæˆ
            cache_key = f"{parsed_experiment.experiment_id}_{len(parsed_experiment.turns)}"
            
            # LLMè©•ä¾¡ã‚’å®Ÿè¡Œ
            llm_response = await self.llm_manager.evaluate_system_compliance(prompt, cache_key)
            
            # è©•ä¾¡çµæœã‚’è§£æ
            parsed_evaluation = None
            if llm_response.success:
                parsed_evaluation = self.llm_manager.parse_evaluation_result(llm_response.content)
            
            # ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã‚’æŠ½å‡º
            parser_success_rate, workflow_compliance, system_health, critical_issues, recommendations = \
                self._extract_metrics_from_evaluation(parsed_evaluation)
            
            return LLMEvaluationResult(
                log_path=str(log_entry.log_path),
                experiment_id=parsed_experiment.experiment_id,
                llm_response=llm_response,
                parsed_evaluation=parsed_evaluation,
                parser_success_rate=parser_success_rate,
                workflow_compliance=workflow_compliance,
                system_health=system_health,
                critical_issues=critical_issues,
                recommendations=recommendations
            )
            
        except Exception as e:
            # ã‚¨ãƒ©ãƒ¼ã®å ´åˆã¯ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤ã§çµæœã‚’ä½œæˆ
            return LLMEvaluationResult(
                log_path=str(log_entry.log_path),
                experiment_id=parsed_experiment.experiment_id if parsed_experiment else "unknown",
                llm_response=LLMResponse("", {}, "", "", False, str(e)),
                parsed_evaluation=None,
                parser_success_rate=0.0,
                workflow_compliance=False,
                system_health="CRITICAL",
                critical_issues=[f"Evaluation error: {str(e)}"],
                recommendations=["Fix evaluation pipeline"]
            )
    
    def _process_batch_results(self, batch_results: List):
        """ãƒãƒƒãƒçµæœã‚’å‡¦ç†"""
        for result in batch_results:
            if isinstance(result, Exception):
                print(f"âš ï¸  ãƒãƒƒãƒå‡¦ç†ã‚¨ãƒ©ãƒ¼: {result}")
                continue
            
            if isinstance(result, LLMEvaluationResult):
                self.results.append(result)
    
    def _extract_metrics_from_evaluation(self, parsed_evaluation: Optional[Dict[str, Any]]) -> tuple:
        """LLMè©•ä¾¡çµæœã‹ã‚‰ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã‚’æŠ½å‡º"""
        
        if not parsed_evaluation:
            return 0.0, False, "CRITICAL", ["No evaluation available"], ["Fix evaluation system"]
        
        # ãƒ‘ãƒ¼ã‚µãƒ¼æˆåŠŸç‡ã®è¨ˆç®—
        parser_evals = parsed_evaluation.get("parser_evaluation", [])
        if parser_evals:
            successful_parses = sum(1 for eval_item in parser_evals if eval_item.get("status") == "PASS")
            parser_success_rate = successful_parses / len(parser_evals)
        else:
            parser_success_rate = 0.0
        
        # ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼é©åˆæ€§
        workflow_eval = parsed_evaluation.get("workflow_evaluation", {})
        workflow_compliance = workflow_eval.get("is_compliant", False)
        
        # ã‚·ã‚¹ãƒ†ãƒ å¥å…¨æ€§
        overall_assessment = parsed_evaluation.get("overall_assessment", {})
        system_health = overall_assessment.get("system_health", "POOR")
        critical_issues = overall_assessment.get("critical_issues", [])
        recommendations = overall_assessment.get("recommendations", [])
        
        return parser_success_rate, workflow_compliance, system_health, critical_issues, recommendations
    
    def _calculate_llm_metrics(self) -> ComplianceMetrics:
        """LLMè©•ä¾¡çµæœã‹ã‚‰å…¨ä½“ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã‚’è¨ˆç®—"""
        
        if not self.results:
            return ComplianceMetrics(0.0, 0.0, 0.0, 0.0, 0.0)
        
        successful_results = [r for r in self.results if r.llm_response.success]
        
        if not successful_results:
            return ComplianceMetrics(0.0, 0.0, 0.0, 0.0, 0.0)
        
        # ãƒ‘ãƒ¼ã‚µãƒ¼æˆåŠŸç‡ã®å¹³å‡
        parser_success_rate = sum(r.parser_success_rate for r in successful_results) / len(successful_results)
        
        # ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼é©åˆç‡
        workflow_compliance_rate = sum(1 for r in successful_results if r.workflow_compliance) / len(successful_results)
        
        # ã‚·ã‚¹ãƒ†ãƒ å¥å…¨æ€§ã‚¹ã‚³ã‚¢
        health_scores = {"EXCELLENT": 1.0, "GOOD": 0.8, "POOR": 0.4, "CRITICAL": 0.0}
        system_health_score = sum(health_scores.get(r.system_health, 0.0) for r in successful_results) / len(successful_results)
        
        # ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°ã‚¹ã‚³ã‚¢ï¼ˆã‚¯ãƒªãƒ†ã‚£ã‚«ãƒ«å•é¡ŒãŒå°‘ãªã„ã»ã©é«˜ã„ï¼‰
        avg_critical_issues = sum(len(r.critical_issues) for r in successful_results) / len(successful_results)
        error_handling_score = max(0.0, 1.0 - (avg_critical_issues / 5.0))  # 5å€‹ä»¥ä¸Šã§0ç‚¹
        
        # ç·åˆé©åˆæ€§ã‚¹ã‚³ã‚¢
        overall_compliance = (
            parser_success_rate * 0.3 +
            workflow_compliance_rate * 0.25 +
            system_health_score * 0.25 +
            error_handling_score * 0.2
        )
        
        return ComplianceMetrics(
            control_flow_accuracy=workflow_compliance_rate,
            parser_success_rate=parser_success_rate,
            file_processing_rate=system_health_score,
            error_handling_score=error_handling_score,
            overall_compliance=overall_compliance
        )
    
    def _get_project_breakdown(self) -> Dict[str, Dict]:
        """ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆåˆ¥å†…è¨³ã®å–å¾—"""
        project_results = {}
        
        for result in self.results:
            project_name = result.experiment_id.split('/')[0] if '/' in result.experiment_id else 'unknown'
            
            if project_name not in project_results:
                project_results[project_name] = {
                    "logs_count": 0,
                    "successful_evaluations": 0,
                    "parser_success_rate": 0.0,
                    "workflow_compliance_rate": 0.0,
                    "critical_issues_count": 0,
                    "avg_system_health": "UNKNOWN"
                }
            
            proj_stats = project_results[project_name]
            proj_stats["logs_count"] += 1
            proj_stats["successful_evaluations"] += 1 if result.llm_response.success else 0
            proj_stats["critical_issues_count"] += len(result.critical_issues)
        
        # ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆåˆ¥å¹³å‡å€¤ã‚’è¨ˆç®—
        for project_name, stats in project_results.items():
            project_results_list = [r for r in self.results 
                                  if (r.experiment_id.split('/')[0] if '/' in r.experiment_id else 'unknown') == project_name]
            
            if project_results_list:
                successful_results = [r for r in project_results_list if r.llm_response.success]
                
                if successful_results:
                    stats["parser_success_rate"] = sum(r.parser_success_rate for r in successful_results) / len(successful_results)
                    stats["workflow_compliance_rate"] = sum(1 for r in successful_results if r.workflow_compliance) / len(successful_results)
                    
                    # æœ€ã‚‚ä¸€èˆ¬çš„ãªã‚·ã‚¹ãƒ†ãƒ å¥å…¨æ€§ãƒ¬ãƒ™ãƒ«
                    health_counts = {}
                    for r in successful_results:
                        health_counts[r.system_health] = health_counts.get(r.system_health, 0) + 1
                    
                    if health_counts:
                        stats["avg_system_health"] = max(health_counts.items(), key=lambda x: x[1])[0]
        
        return project_results
    
    def _serialize_result(self, result: LLMEvaluationResult) -> Dict[str, Any]:
        """è©•ä¾¡çµæœã‚’ã‚·ãƒªã‚¢ãƒ©ã‚¤ã‚º"""
        return {
            "log_path": result.log_path,
            "experiment_id": result.experiment_id,
            "llm_success": result.llm_response.success,
            "llm_error": result.llm_response.error_message,
            "parser_success_rate": result.parser_success_rate,
            "workflow_compliance": result.workflow_compliance,
            "system_health": result.system_health,
            "critical_issues": result.critical_issues,
            "recommendations": result.recommendations,
            "llm_usage": result.llm_response.usage
        }


async def demo_compliance_evaluation():
    """é©åˆæ€§è©•ä¾¡ã®ãƒ‡ãƒ¢"""
    print("ğŸ” ã‚·ã‚¹ãƒ†ãƒ é©åˆæ€§è©•ä¾¡ãƒ‡ãƒ¢ï¼ˆLLMçµ±åˆç‰ˆï¼‰")
    
    evaluator = SystemComplianceEvaluator(llm_provider="mock", llm_model="mock-gpt")
    results = await evaluator.evaluate_all_projects()
    
    print("\nğŸ“Š è©•ä¾¡çµæœã‚µãƒãƒªãƒ¼:")
    print(f"  - ç·åˆé©åˆæ€§ã‚¹ã‚³ã‚¢: {results['compliance_metrics']['overall_compliance']:.3f}")
    print(f"  - ãƒ‘ãƒ¼ã‚µãƒ¼æˆåŠŸç‡: {results['compliance_metrics']['parser_success_rate']:.3f}")
    print(f"  - ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼é©åˆç‡: {results['compliance_metrics']['control_flow_accuracy']:.3f}")
    print(f"  - LLMè©•ä¾¡æˆåŠŸç‡: {results['summary']['successful_evaluations']}/{results['summary']['total_logs_processed']}")
    

if __name__ == "__main__":
    asyncio.run(demo_compliance_evaluation())
        """å…¨ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã®ãƒ­ã‚°ã‚’è©•ä¾¡"""
        print("ğŸ” Step 1: ã‚·ã‚¹ãƒ†ãƒ é©åˆæ€§è©•ä¾¡é–‹å§‹")
        print("=" * 50)
        
        start_time = datetime.now()
        self.results = []
        
        # å…¨ãƒ­ã‚°ã‚’å·¡å›ã—ã¦è©•ä¾¡
        processed_count = 0
        for log_entry in self.log_iterator.iterate_all_logs():
            try:
                result = self._evaluate_single_log(log_entry)
                self.results.append(result)
                processed_count += 1
                
                if processed_count % 10 == 0:
                    print(f"  å‡¦ç†æ¸ˆã¿: {processed_count} ãƒ­ã‚°")
                    
            except Exception as e:
                print(f"âš ï¸  ãƒ­ã‚°è©•ä¾¡ã‚¨ãƒ©ãƒ¼: {log_entry.log_path} - {e}")
                continue
        
        end_time = datetime.now()
        processing_time = (end_time - start_time).total_seconds()
        
        # çµæœé›†è¨ˆ
        metrics = self._calculate_overall_metrics()
        
        evaluation_result = {
            "evaluation_type": "system_compliance",
            "timestamp": end_time.isoformat(),
            "processing_time_seconds": processing_time,
            "summary": {
                "total_logs_processed": len(self.results),
                "successful_evaluations": len([r for r in self.results if r.parsing_success]),
                "failed_evaluations": len([r for r in self.results if not r.parsing_success]),
                "average_files_per_log": sum(r.files_processed for r in self.results) / len(self.results) if self.results else 0,
                "total_errors": sum(r.errors_count for r in self.results)
            },
            "compliance_metrics": asdict(metrics),
            "project_breakdown": self._get_project_breakdown(),
            "detailed_results": [asdict(result) for result in self.results[:100]]  # æœ€åˆã®100ä»¶ã®ã¿ä¿å­˜
        }
        
        print(f"âœ… ã‚·ã‚¹ãƒ†ãƒ é©åˆæ€§è©•ä¾¡å®Œäº†")
        print(f"  - å‡¦ç†æ™‚é–“: {processing_time:.2f}ç§’")
        print(f"  - å‡¦ç†ãƒ­ã‚°æ•°: {len(self.results)}")
        print(f"  - ç·åˆé©åˆæ€§ã‚¹ã‚³ã‚¢: {metrics.overall_compliance:.3f}")
        
        return evaluation_result
    
    def _evaluate_single_log(self, log_entry: LogEntry) -> LogAnalysisResult:
        """å˜ä¸€ãƒ­ã‚°ã®è©•ä¾¡"""
        analysis_start = datetime.now()
        
        # ãƒ­ã‚°ãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã¿
        try:
            with open(log_entry.log_path, 'r', encoding='utf-8') as f:
                log_data = json.load(f)
        except Exception as e:
            # JSONèª­ã¿è¾¼ã¿å¤±æ•—ã®å ´åˆã¯ãƒ†ã‚­ã‚¹ãƒˆã¨ã—ã¦èª­ã¿è¾¼ã¿
            with open(log_entry.log_path, 'r', encoding='utf-8', errors='ignore') as f:
                log_content = f.read()
            log_data = {"raw_content": log_content, "parsing_error": str(e)}
        
        analysis_end = datetime.now()
        processing_time = (analysis_end - analysis_start).total_seconds() * 1000
        
        # ãƒ­ã‚°å†…å®¹ã‚’è§£æ
        parsing_success = self._check_parsing_success(log_data)
        has_control_flow = self._check_control_flow(log_data)
        files_processed = self._count_processed_files(log_data)
        errors_count = self._count_errors(log_data)
        
        # ã‚¹ã‚³ã‚¢è¨ˆç®—
        control_flow_score = 1.0 if has_control_flow else 0.0
        parser_score = 1.0 if parsing_success else 0.0
        file_processing_score = min(files_processed / 10.0, 1.0)  # 10ãƒ•ã‚¡ã‚¤ãƒ«å‡¦ç†ã§æº€ç‚¹
        error_handling_score = max(0.0, 1.0 - (errors_count / 5.0))  # 5ã‚¨ãƒ©ãƒ¼ä»¥ä¸Šã§0ç‚¹
        
        return LogAnalysisResult(
            project_name=log_entry.project_name,
            log_type=log_entry.log_type,
            log_path=str(log_entry.log_path),
            
            parsing_success=parsing_success,
            has_control_flow=has_control_flow,
            files_processed=files_processed,
            errors_count=errors_count,
            
            control_flow_score=control_flow_score,
            parser_score=parser_score,
            file_processing_score=file_processing_score,
            error_handling_score=error_handling_score,
            
            timestamp=log_entry.timestamp,
            log_size=log_entry.size,
            processing_time_ms=processing_time
        )
    
    def _check_parsing_success(self, log_data: Dict) -> bool:
        """ãƒ‘ãƒ¼ã‚·ãƒ³ã‚°æˆåŠŸãƒã‚§ãƒƒã‚¯"""
        if "parsing_error" in log_data:
            return False
        
        # ä¸€èˆ¬çš„ãªæˆåŠŸæŒ‡æ¨™ã‚’ãƒã‚§ãƒƒã‚¯
        success_indicators = [
            "status" in log_data and log_data.get("status") == "success",
            "parsing_success" in log_data and log_data.get("parsing_success"),
            "files" in log_data or "processed_files" in log_data,
            not ("error" in log_data or "exception" in log_data)
        ]
        
        return any(success_indicators)
    
    def _check_control_flow(self, log_data: Dict) -> bool:
        """åˆ¶å¾¡ãƒ•ãƒ­ãƒ¼æ¤œå‡ºãƒã‚§ãƒƒã‚¯"""
        # åˆ¶å¾¡ãƒ•ãƒ­ãƒ¼é–¢é€£ã®ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’æ¤œç´¢
        control_flow_keywords = [
            "control_flow", "control-flow", "controlflow",
            "method_calls", "function_calls", "call_graph",
            "ast", "syntax_tree", "program_flow"
        ]
        
        log_str = json.dumps(log_data).lower()
        return any(keyword in log_str for keyword in control_flow_keywords)
    
    def _count_processed_files(self, log_data: Dict) -> int:
        """å‡¦ç†ãƒ•ã‚¡ã‚¤ãƒ«æ•°ã®ã‚«ã‚¦ãƒ³ãƒˆ"""
        # æ§˜ã€…ãªãƒ•ã‚¡ã‚¤ãƒ«æ•°æŒ‡æ¨™ã‚’è©¦è¡Œ
        file_count_candidates = [
            log_data.get("files_processed", 0),
            log_data.get("file_count", 0),
            len(log_data.get("files", [])),
            len(log_data.get("processed_files", [])),
            log_data.get("num_files", 0)
        ]
        
        return max(file_count_candidates)
    
    def _count_errors(self, log_data: Dict) -> int:
        """ã‚¨ãƒ©ãƒ¼æ•°ã®ã‚«ã‚¦ãƒ³ãƒˆ"""
        error_count = 0
        
        # ç›´æ¥çš„ãªã‚¨ãƒ©ãƒ¼æ•°
        if "error_count" in log_data:
            error_count += log_data["error_count"]
        
        # ã‚¨ãƒ©ãƒ¼é…åˆ—
        if "errors" in log_data:
            error_count += len(log_data["errors"])
        
        # ãƒ­ã‚°å†…å®¹ã‹ã‚‰ã‚¨ãƒ©ãƒ¼ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’æ¤œç´¢
        log_str = json.dumps(log_data).lower()
        error_keywords = ["error", "exception", "failed", "failure"]
        for keyword in error_keywords:
            error_count += log_str.count(keyword)
        
        return error_count
    
    def _calculate_overall_metrics(self) -> ComplianceMetrics:
        """å…¨ä½“ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã®è¨ˆç®—"""
        if not self.results:
            return ComplianceMetrics(0.0, 0.0, 0.0, 0.0, 0.0)
        
        control_flow_accuracy = sum(r.control_flow_score for r in self.results) / len(self.results)
        parser_success_rate = sum(r.parser_score for r in self.results) / len(self.results)
        file_processing_rate = sum(r.file_processing_score for r in self.results) / len(self.results)
        error_handling_score = sum(r.error_handling_score for r in self.results) / len(self.results)
        
        # ç·åˆã‚¹ã‚³ã‚¢ã¯å„è¦ç´ ã®é‡ã¿ä»˜ã‘å¹³å‡
        overall_compliance = (
            control_flow_accuracy * 0.3 +
            parser_success_rate * 0.25 +
            file_processing_rate * 0.25 +
            error_handling_score * 0.2
        )
        
        return ComplianceMetrics(
            control_flow_accuracy=control_flow_accuracy,
            parser_success_rate=parser_success_rate,
            file_processing_rate=file_processing_rate,
            error_handling_score=error_handling_score,
            overall_compliance=overall_compliance
        )
    
    def _get_project_breakdown(self) -> Dict[str, Dict]:
        """ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆåˆ¥å†…è¨³ã®å–å¾—"""
        project_results = {}
        
        for result in self.results:
            if result.project_name not in project_results:
                project_results[result.project_name] = {
                    "logs_count": 0,
                    "successful_parsing": 0,
                    "control_flow_detected": 0,
                    "total_files_processed": 0,
                    "total_errors": 0,
                    "avg_control_flow_score": 0.0,
                    "avg_parser_score": 0.0
                }
            
            proj_stats = project_results[result.project_name]
            proj_stats["logs_count"] += 1
            proj_stats["successful_parsing"] += 1 if result.parsing_success else 0
            proj_stats["control_flow_detected"] += 1 if result.has_control_flow else 0
            proj_stats["total_files_processed"] += result.files_processed
            proj_stats["total_errors"] += result.errors_count
        
        # å¹³å‡å€¤è¨ˆç®—
        for project_name, stats in project_results.items():
            project_logs = [r for r in self.results if r.project_name == project_name]
            stats["avg_control_flow_score"] = sum(r.control_flow_score for r in project_logs) / len(project_logs)
            stats["avg_parser_score"] = sum(r.parser_score for r in project_logs) / len(project_logs)
        
        return project_results


def demo_compliance_evaluation():
    """é©åˆæ€§è©•ä¾¡ã®ãƒ‡ãƒ¢"""
    print("ğŸ” ã‚·ã‚¹ãƒ†ãƒ é©åˆæ€§è©•ä¾¡ãƒ‡ãƒ¢")
    
    evaluator = SystemComplianceEvaluator()
    results = evaluator.evaluate_all_projects()
    
    print("\nğŸ“Š è©•ä¾¡çµæœã‚µãƒãƒªãƒ¼:")
    print(f"  - ç·åˆé©åˆæ€§ã‚¹ã‚³ã‚¢: {results['compliance_metrics']['overall_compliance']:.3f}")
    print(f"  - ãƒ‘ãƒ¼ã‚µãƒ¼æˆåŠŸç‡: {results['compliance_metrics']['parser_success_rate']:.3f}")
    print(f"  - åˆ¶å¾¡ãƒ•ãƒ­ãƒ¼ç²¾åº¦: {results['compliance_metrics']['control_flow_accuracy']:.3f}")
    

if __name__ == "__main__":
    demo_compliance_evaluation()
