"""
Step 2: ãƒ‘ãƒƒãƒå“è³ªè©•ä¾¡å™¨
APRãŒç”Ÿæˆã—ãŸãƒ‘ãƒƒãƒã¨ã‚°ãƒ©ã‚¦ãƒ³ãƒ‰ãƒˆã‚¥ãƒ«ãƒ¼ã‚¹ã‚’æ¯”è¼ƒã—ã¦å“è³ªã‚’è©•ä¾¡ã™ã‚‹
"""

import json
import csv
from pathlib import Path
from typing import Dict, List, Any, Optional, Tuple
from dataclasses import dataclass, asdict
from datetime import datetime
import difflib
import re


@dataclass
class QualityMetrics:
    """ãƒ‘ãƒƒãƒå“è³ªãƒ¡ãƒˆãƒªã‚¯ã‚¹"""
    plausibility_score: float     # å¦¥å½“æ€§ã‚¹ã‚³ã‚¢
    correctness_r1: float         # æ­£ç¢ºæ€§@1
    correctness_r5: float         # æ­£ç¢ºæ€§@5  
    correctness_r10: float        # æ­£ç¢ºæ€§@10
    reasoning_quality: float      # æ¨è«–å“è³ª
    semantic_similarity: float    # æ„å‘³çš„é¡ä¼¼åº¦
    overall_quality: float        # ç·åˆå“è³ªã‚¹ã‚³ã‚¢


@dataclass
class PatchComparison:
    """ãƒ‘ãƒƒãƒæ¯”è¼ƒçµæœ"""
    project_name: str
    commit_hash: str
    file_path: str
    
    # å…¥åŠ›ãƒ‡ãƒ¼ã‚¿
    ground_truth: str
    generated_patch: Optional[str]
    
    # è©•ä¾¡çµæœ
    has_generated_patch: bool
    exact_match: bool
    similarity_score: float
    plausibility_score: float
    reasoning_score: float
    
    # è©³ç´°åˆ†æ
    diff_analysis: Dict[str, Any]
    semantic_analysis: Dict[str, Any]
    
    timestamp: datetime


class PatchQualityEvaluator:
    """ãƒ‘ãƒƒãƒå“è³ªè©•ä¾¡å™¨"""
    
    def __init__(self, 
                 dataset_path: str = "/app/dataset", 
                 apr_output_path: str = "/app/apr-output"):
        self.dataset_path = Path(dataset_path)
        self.apr_output_path = Path(apr_output_path)
        self.dataset_entries: List[Dict] = []
        self.results: List[PatchComparison] = []
    
    def load_dataset(self) -> None:
        """ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã¿"""
        dataset_file = self.dataset_path / "P.U_merged_filtered - Final_merged_only_not_excluded_yes_ms_unarchived_commit_hash v2.0.csv"
        
        if not dataset_file.exists():
            raise FileNotFoundError(f"Dataset file not found: {dataset_file}")
        
        print(f"ğŸ“Š ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆèª­ã¿è¾¼ã¿ä¸­: {dataset_file}")
        
        with open(dataset_file, 'r', encoding='utf-8') as f:
            reader = csv.DictReader(f)
            self.dataset_entries = list(reader)
        
        print(f"âœ… ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆèª­ã¿è¾¼ã¿å®Œäº†: {len(self.dataset_entries)}ä»¶")
    
    def evaluate_all_patches(self) -> Dict[str, Any]:
        """å…¨ãƒ‘ãƒƒãƒã®å“è³ªè©•ä¾¡"""
        print("ğŸ¯ Step 2: ãƒ‘ãƒƒãƒå“è³ªè©•ä¾¡é–‹å§‹")
        print("=" * 50)
        
        if not self.dataset_entries:
            self.load_dataset()
        
        start_time = datetime.now()
        self.results = []
        
        # ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®å„ã‚¨ãƒ³ãƒˆãƒªãƒ¼ã‚’å‡¦ç†
        processed_count = 0
        for entry in self.dataset_entries:
            try:
                result = self._evaluate_single_patch(entry)
                if result:  # çµæœãŒã‚ã‚‹å ´åˆã®ã¿è¿½åŠ 
                    self.results.append(result)
                processed_count += 1
                
                if processed_count % 50 == 0:
                    print(f"  å‡¦ç†æ¸ˆã¿: {processed_count}/{len(self.dataset_entries)} ã‚¨ãƒ³ãƒˆãƒªãƒ¼")
                    
            except Exception as e:
                print(f"âš ï¸  ãƒ‘ãƒƒãƒè©•ä¾¡ã‚¨ãƒ©ãƒ¼: {entry.get('commit_hash', 'unknown')} - {e}")
                continue
        
        end_time = datetime.now()
        processing_time = (end_time - start_time).total_seconds()
        
        # çµæœé›†è¨ˆ
        metrics = self._calculate_overall_metrics()
        
        evaluation_result = {
            "evaluation_type": "patch_quality",
            "timestamp": end_time.isoformat(),
            "processing_time_seconds": processing_time,
            "summary": {
                "total_dataset_entries": len(self.dataset_entries),
                "evaluations_completed": len(self.results),
                "patches_generated": len([r for r in self.results if r.has_generated_patch]),
                "exact_matches": len([r for r in self.results if r.exact_match]),
                "average_similarity": sum(r.similarity_score for r in self.results) / len(self.results) if self.results else 0
            },
            "quality_metrics": asdict(metrics),
            "project_breakdown": self._get_project_breakdown(),
            "detailed_results": [asdict(result) for result in self.results[:100]]  # æœ€åˆã®100ä»¶ã®ã¿ä¿å­˜
        }
        
        print(f"âœ… ãƒ‘ãƒƒãƒå“è³ªè©•ä¾¡å®Œäº†")
        print(f"  - å‡¦ç†æ™‚é–“: {processing_time:.2f}ç§’")
        print(f"  - è©•ä¾¡å®Œäº†: {len(self.results)}/{len(self.dataset_entries)}")
        print(f"  - ç·åˆå“è³ªã‚¹ã‚³ã‚¢: {metrics.overall_quality:.3f}")
        
        return evaluation_result
    
    def _evaluate_single_patch(self, dataset_entry: Dict) -> Optional[PatchComparison]:
        """å˜ä¸€ãƒ‘ãƒƒãƒã®è©•ä¾¡"""
        commit_hash = dataset_entry.get('commit_hash', '')
        project = dataset_entry.get('project', '')
        file_path = dataset_entry.get('file_path', '')
        ground_truth = dataset_entry.get('ground_truth', '')
        
        if not all([commit_hash, project, ground_truth]):
            return None
        
        # ç”Ÿæˆã•ã‚ŒãŸãƒ‘ãƒƒãƒã‚’æ¤œç´¢
        generated_patch = self._find_generated_patch(project, commit_hash, file_path)
        
        # ãƒ‘ãƒƒãƒãŒç”Ÿæˆã•ã‚Œã¦ã„ãªã„å ´åˆ
        if generated_patch is None:
            return PatchComparison(
                project_name=project,
                commit_hash=commit_hash,
                file_path=file_path,
                ground_truth=ground_truth,
                generated_patch=None,
                has_generated_patch=False,
                exact_match=False,
                similarity_score=0.0,
                plausibility_score=0.0,
                reasoning_score=0.0,
                diff_analysis={"status": "no_patch_generated"},
                semantic_analysis={"status": "no_patch_generated"},
                timestamp=datetime.now()
            )
        
        # ãƒ‘ãƒƒãƒæ¯”è¼ƒåˆ†æ
        exact_match = ground_truth.strip() == generated_patch.strip()
        similarity_score = self._calculate_similarity(ground_truth, generated_patch)
        plausibility_score = self._evaluate_plausibility(generated_patch)
        reasoning_score = self._evaluate_reasoning_quality(generated_patch, ground_truth)
        
        diff_analysis = self._analyze_diff(ground_truth, generated_patch)
        semantic_analysis = self._analyze_semantic_similarity(ground_truth, generated_patch)
        
        return PatchComparison(
            project_name=project,
            commit_hash=commit_hash,
            file_path=file_path,
            ground_truth=ground_truth,
            generated_patch=generated_patch,
            has_generated_patch=True,
            exact_match=exact_match,
            similarity_score=similarity_score,
            plausibility_score=plausibility_score,
            reasoning_score=reasoning_score,
            diff_analysis=diff_analysis,
            semantic_analysis=semantic_analysis,
            timestamp=datetime.now()
        )
    
    def _find_generated_patch(self, project: str, commit_hash: str, file_path: str) -> Optional[str]:
        """ç”Ÿæˆã•ã‚ŒãŸãƒ‘ãƒƒãƒã‚’æ¤œç´¢"""
        # APRå‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã§ãƒ‘ãƒƒãƒãƒ•ã‚¡ã‚¤ãƒ«ã‚’æ¤œç´¢
        # è¤‡æ•°ã®å¯èƒ½ãªå ´æ‰€ã‚’ç¢ºèª
        
        possible_locations = [
            self.apr_output_path / f"{project}_{commit_hash}.patch",
            self.apr_output_path / f"{commit_hash}.patch", 
            self.apr_output_path / "tmp_restoredDiff.txt",
            self.apr_output_path / f"{project}.patch"
        ]
        
        for patch_file in possible_locations:
            if patch_file.exists():
                try:
                    with open(patch_file, 'r', encoding='utf-8') as f:
                        content = f.read()
                        if content.strip():  # ç©ºã§ãªã„å ´åˆ
                            return content
                except Exception:
                    continue
        
        # processing_summary ã‹ã‚‰ãƒ‘ãƒƒãƒæƒ…å ±ã‚’æ¤œç´¢
        for summary_file in self.apr_output_path.glob("processing_summary_*.json"):
            try:
                with open(summary_file, 'r', encoding='utf-8') as f:
                    data = json.load(f)
                    
                # ã‚µãƒãƒªãƒ¼ãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰è©²å½“ã™ã‚‹ãƒ‘ãƒƒãƒã‚’æ¤œç´¢
                if self._matches_entry(data, project, commit_hash, file_path):
                    patch_content = self._extract_patch_from_summary(data)
                    if patch_content:
                        return patch_content
                        
            except Exception:
                continue
        
        return None
    
    def _matches_entry(self, summary_data: Dict, project: str, commit_hash: str, file_path: str) -> bool:
        """ã‚µãƒãƒªãƒ¼ãƒ‡ãƒ¼ã‚¿ãŒãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚¨ãƒ³ãƒˆãƒªãƒ¼ã¨ä¸€è‡´ã™ã‚‹ã‹ãƒã‚§ãƒƒã‚¯"""
        # ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆåã¨ã‚³ãƒŸãƒƒãƒˆãƒãƒƒã‚·ãƒ¥ã®ä¸€è‡´ã‚’ãƒã‚§ãƒƒã‚¯
        summary_project = summary_data.get('project', '')
        summary_commit = summary_data.get('commit_hash', '')
        
        return (project.lower() in summary_project.lower() or summary_project.lower() in project.lower()) and \
               (commit_hash in summary_commit or summary_commit in commit_hash)
    
    def _extract_patch_from_summary(self, summary_data: Dict) -> Optional[str]:
        """ã‚µãƒãƒªãƒ¼ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰ãƒ‘ãƒƒãƒå†…å®¹ã‚’æŠ½å‡º"""
        # æ§˜ã€…ãªå¯èƒ½ãªãƒ‘ãƒƒãƒãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ã‚’è©¦è¡Œ
        patch_fields = ['patch', 'diff', 'changes', 'generated_patch', 'result']
        
        for field in patch_fields:
            if field in summary_data and summary_data[field]:
                return str(summary_data[field])
        
        return None
    
    def _calculate_similarity(self, ground_truth: str, generated_patch: str) -> float:
        """æ–‡å­—åˆ—é¡ä¼¼åº¦è¨ˆç®—"""
        if not ground_truth or not generated_patch:
            return 0.0
        
        # difflib ã‚’ä½¿ç”¨ã—ãŸé¡ä¼¼åº¦è¨ˆç®—
        matcher = difflib.SequenceMatcher(None, ground_truth, generated_patch)
        return matcher.ratio()
    
    def _evaluate_plausibility(self, generated_patch: str) -> float:
        """ãƒ‘ãƒƒãƒã®å¦¥å½“æ€§è©•ä¾¡"""
        if not generated_patch:
            return 0.0
        
        score = 0.0
        
        # åŸºæœ¬çš„ãªå¦¥å½“æ€§ãƒã‚§ãƒƒã‚¯
        # 1. ç©ºã§ãªã„ã“ã¨
        if generated_patch.strip():
            score += 0.2
        
        # 2. æ§‹æ–‡çš„ã«å¦¥å½“ãã†ãªã‚³ãƒ¼ãƒ‰å¤‰æ›´ã§ã‚ã‚‹ã“ã¨
        if any(keyword in generated_patch.lower() for keyword in [
            'function', 'method', 'class', 'if', 'for', 'while', 'return', 
            '+', '-', 'add', 'remove', 'fix', 'change'
        ]):
            score += 0.3
        
        # 3. diffå½¢å¼ã¾ãŸã¯ã‚³ãƒ¼ãƒ‰å½¢å¼ã§ã‚ã‚‹ã“ã¨
        if generated_patch.startswith(('---', '+++', 'diff', '@')) or \
           any(line.startswith(('+', '-')) for line in generated_patch.split('\n')):
            score += 0.3
        
        # 4. é©åˆ‡ãªé•·ã•ã§ã‚ã‚‹ã“ã¨ï¼ˆæ¥µç«¯ã«çŸ­ã„ãƒ»é•·ã„ã¯ä½è©•ä¾¡ï¼‰
        length = len(generated_patch)
        if 10 < length < 10000:
            score += 0.2
        
        return min(score, 1.0)
    
    def _evaluate_reasoning_quality(self, generated_patch: str, ground_truth: str) -> float:
        """æ¨è«–å“è³ªã®è©•ä¾¡"""
        if not generated_patch:
            return 0.0
        
        score = 0.0
        
        # 1. å¤‰æ›´ã®æ–¹å‘æ€§ãŒæ­£ã—ã„ã‹
        gt_lines = set(ground_truth.split('\n'))
        gen_lines = set(generated_patch.split('\n'))
        
        common_lines = gt_lines.intersection(gen_lines)
        if common_lines:
            score += 0.4
        
        # 2. ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã®ä¸€è‡´
        gt_keywords = set(re.findall(r'\b\w+\b', ground_truth.lower()))
        gen_keywords = set(re.findall(r'\b\w+\b', generated_patch.lower()))
        
        if gt_keywords and gen_keywords:
            keyword_overlap = len(gt_keywords.intersection(gen_keywords)) / len(gt_keywords.union(gen_keywords))
            score += keyword_overlap * 0.6
        
        return min(score, 1.0)
    
    def _analyze_diff(self, ground_truth: str, generated_patch: str) -> Dict[str, Any]:
        """å·®åˆ†åˆ†æ"""
        if not generated_patch:
            return {"status": "no_patch"}
        
        # åŸºæœ¬çš„ãªå·®åˆ†çµ±è¨ˆ
        gt_lines = ground_truth.split('\n')
        gen_lines = generated_patch.split('\n')
        
        return {
            "ground_truth_lines": len(gt_lines),
            "generated_lines": len(gen_lines),
            "line_difference": abs(len(gt_lines) - len(gen_lines)),
            "common_lines": len(set(gt_lines).intersection(set(gen_lines))),
            "unique_gt_lines": len(set(gt_lines) - set(gen_lines)),
            "unique_gen_lines": len(set(gen_lines) - set(gt_lines))
        }
    
    def _analyze_semantic_similarity(self, ground_truth: str, generated_patch: str) -> Dict[str, Any]:
        """æ„å‘³çš„é¡ä¼¼åº¦åˆ†æ"""
        if not generated_patch:
            return {"status": "no_patch"}
        
        # ç°¡å˜ãªæ„å‘³çš„é¡ä¼¼åº¦åˆ†æ
        gt_tokens = set(re.findall(r'\b\w+\b', ground_truth.lower()))
        gen_tokens = set(re.findall(r'\b\w+\b', generated_patch.lower()))
        
        if not gt_tokens:
            return {"token_similarity": 0.0, "total_tokens_gt": 0, "total_tokens_gen": len(gen_tokens)}
        
        intersection = gt_tokens.intersection(gen_tokens)
        union = gt_tokens.union(gen_tokens)
        
        return {
            "token_similarity": len(intersection) / len(union) if union else 0.0,
            "total_tokens_gt": len(gt_tokens),
            "total_tokens_gen": len(gen_tokens),
            "common_tokens": len(intersection),
            "unique_gt_tokens": len(gt_tokens - gen_tokens),
            "unique_gen_tokens": len(gen_tokens - gt_tokens)
        }
    
    def _calculate_overall_metrics(self) -> QualityMetrics:
        """å…¨ä½“å“è³ªãƒ¡ãƒˆãƒªã‚¯ã‚¹ã®è¨ˆç®—"""
        if not self.results:
            return QualityMetrics(0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0)
        
        # åŸºæœ¬ãƒ¡ãƒˆãƒªã‚¯ã‚¹
        plausibility_score = sum(r.plausibility_score for r in self.results) / len(self.results)
        reasoning_quality = sum(r.reasoning_score for r in self.results) / len(self.results)
        
        # æ­£ç¢ºæ€§ãƒ¡ãƒˆãƒªã‚¯ã‚¹ï¼ˆå®Œå…¨ä¸€è‡´ã®å‰²åˆï¼‰
        exact_matches = [r for r in self.results if r.exact_match]
        correctness_r1 = len(exact_matches) / len(self.results)
        
        # R5, R10ã¯é¡ä¼¼åº¦ä¸Šä½5ä»¶ã€10ä»¶ã§ã®ä¸€è‡´ç‡ï¼ˆç°¡å˜ãªå®Ÿè£…ï¼‰
        similarity_sorted = sorted(self.results, key=lambda x: x.similarity_score, reverse=True)
        top_5_percent = int(len(similarity_sorted) * 0.05) or 1
        top_10_percent = int(len(similarity_sorted) * 0.10) or 1
        
        correctness_r5 = sum(1 for r in similarity_sorted[:top_5_percent] if r.similarity_score > 0.8) / len(self.results)
        correctness_r10 = sum(1 for r in similarity_sorted[:top_10_percent] if r.similarity_score > 0.7) / len(self.results)
        
        # æ„å‘³çš„é¡ä¼¼åº¦
        semantic_similarity = sum(r.similarity_score for r in self.results) / len(self.results)
        
        # ç·åˆå“è³ªã‚¹ã‚³ã‚¢
        overall_quality = (
            plausibility_score * 0.25 +
            correctness_r1 * 0.30 +
            correctness_r10 * 0.20 +
            reasoning_quality * 0.15 +
            semantic_similarity * 0.10
        )
        
        return QualityMetrics(
            plausibility_score=plausibility_score,
            correctness_r1=correctness_r1,
            correctness_r5=correctness_r5,
            correctness_r10=correctness_r10,
            reasoning_quality=reasoning_quality,
            semantic_similarity=semantic_similarity,
            overall_quality=overall_quality
        )
    
    def _get_project_breakdown(self) -> Dict[str, Dict]:
        """ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆåˆ¥å†…è¨³ã®å–å¾—"""
        project_results = {}
        
        for result in self.results:
            if result.project_name not in project_results:
                project_results[result.project_name] = {
                    "total_evaluations": 0,
                    "patches_generated": 0,
                    "exact_matches": 0,
                    "avg_similarity": 0.0,
                    "avg_plausibility": 0.0
                }
            
            proj_stats = project_results[result.project_name]
            proj_stats["total_evaluations"] += 1
            proj_stats["patches_generated"] += 1 if result.has_generated_patch else 0
            proj_stats["exact_matches"] += 1 if result.exact_match else 0
        
        # å¹³å‡å€¤è¨ˆç®—
        for project_name, stats in project_results.items():
            project_results_list = [r for r in self.results if r.project_name == project_name]
            if project_results_list:
                stats["avg_similarity"] = sum(r.similarity_score for r in project_results_list) / len(project_results_list)
                stats["avg_plausibility"] = sum(r.plausibility_score for r in project_results_list) / len(project_results_list)
        
        return project_results


def demo_quality_evaluation():
    """å“è³ªè©•ä¾¡ã®ãƒ‡ãƒ¢"""
    print("ğŸ¯ ãƒ‘ãƒƒãƒå“è³ªè©•ä¾¡ãƒ‡ãƒ¢")
    
    evaluator = PatchQualityEvaluator()
    results = evaluator.evaluate_all_patches()
    
    print("\nğŸ“Š è©•ä¾¡çµæœã‚µãƒãƒªãƒ¼:")
    print(f"  - ç·åˆå“è³ªã‚¹ã‚³ã‚¢: {results['quality_metrics']['overall_quality']:.3f}")
    print(f"  - å¦¥å½“æ€§ã‚¹ã‚³ã‚¢: {results['quality_metrics']['plausibility_score']:.3f}")
    print(f"  - æ­£ç¢ºæ€§@1: {results['quality_metrics']['correctness_r1']:.3f}")


if __name__ == "__main__":
    demo_quality_evaluation()
