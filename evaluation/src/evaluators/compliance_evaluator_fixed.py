#!/usr/bin/env python3
"""
ã‚·ã‚¹ãƒ†ãƒ é©åˆæ€§è©•ä¾¡å™¨ - LLMçµ±åˆç‰ˆ
APRã‚·ã‚¹ãƒ†ãƒ ã®ãƒ‘ãƒ¼ã‚µæ•´åˆæ€§ã¨ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼é©åˆæ€§ã‚’è©•ä¾¡
"""
import json
import asyncio
from datetime import datetime
from typing import List, Dict, Any, Optional
from dataclasses import dataclass, asdict

from utils.log_iterator import APRLogIterator
from utils.log_parser import APRLogParser
from llm.llm_integration import LLMEvaluationManager, LLMResponse

@dataclass
class ComplianceMetrics:
    """é©åˆæ€§ãƒ¡ãƒˆãƒªã‚¯ã‚¹"""
    overall_compliance: float
    parser_success_avg: float
    workflow_compliance_rate: float
    system_health_score: float
    llm_evaluation_success_rate: float

@dataclass
class LogEvaluationResult:
    """å€‹åˆ¥ãƒ­ã‚°ã®è©•ä¾¡çµæœ"""
    log_path: str
    project_name: str
    parser_success_rate: float
    workflow_compliance: bool
    system_health: str  # "GOOD", "WARNING", "CRITICAL"
    llm_response: LLMResponse
    workflow_tags_detected: List[str]
    evaluation_timestamp: str

class SystemComplianceEvaluator:
    """ã‚·ã‚¹ãƒ†ãƒ é©åˆæ€§è©•ä¾¡å™¨ï¼ˆLLMçµ±åˆç‰ˆï¼‰"""
    
    def __init__(self, workspace_path: str = "/app", llm_provider: str = "mock"):
        self.workspace_path = workspace_path
        self.log_iterator = APRLogIterator(workspace_path)
        self.log_parser = APRLogParser()
        self.llm_manager = LLMEvaluationManager(provider_type=llm_provider)
        self.results: List[LogEvaluationResult] = []
    
    async def evaluate_all_projects(self) -> Dict[str, Any]:
        """å…¨ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã®ãƒ­ã‚°ã‚’è©•ä¾¡"""
        print("ğŸ” Step 1: ã‚·ã‚¹ãƒ†ãƒ é©åˆæ€§è©•ä¾¡é–‹å§‹ï¼ˆLLMçµ±åˆç‰ˆï¼‰")
        print("=" * 50)
        
        start_time = datetime.now()
        self.results = []
        
        # å…¨ãƒ­ã‚°ã‚’å·¡å›ã—ã¦è©•ä¾¡ï¼ˆæœ€åˆã®5ä»¶ã®ã¿ãƒ†ã‚¹ãƒˆç”¨ï¼‰
        processed_count = 0
        evaluation_tasks = []
        
        for log_entry in self.log_iterator.iterate_all_logs():
            if processed_count >= 5:  # ãƒ†ã‚¹ãƒˆç”¨ã«åˆ¶é™
                break
                
            try:
                # ãƒ­ã‚°ãƒ•ã‚¡ã‚¤ãƒ«ã‚’è§£æ
                parsed_experiment = self.log_parser.parse_log_file(log_entry.log_path)
                if not parsed_experiment:
                    continue
                
                # è©•ä¾¡ã‚¿ã‚¹ã‚¯ã‚’ä½œæˆï¼ˆéåŒæœŸå‡¦ç†ç”¨ï¼‰
                task = self._evaluate_single_log_async(log_entry, parsed_experiment)
                evaluation_tasks.append(task)
                processed_count += 1
                    
            except Exception as e:
                print(f"âš ï¸  ãƒ­ã‚°è©•ä¾¡ã‚¨ãƒ©ãƒ¼: {log_entry.log_path} - {e}")
                continue
        
        # è©•ä¾¡ã‚¿ã‚¹ã‚¯ã‚’å®Ÿè¡Œ
        if evaluation_tasks:
            print(f"ğŸš€ {len(evaluation_tasks)}ä»¶ã®ãƒ­ã‚°ã‚’ä¸¦åˆ—è©•ä¾¡ä¸­...")
            batch_results = await asyncio.gather(*evaluation_tasks, return_exceptions=True)
            self._process_batch_results(batch_results)
        
        end_time = datetime.now()
        processing_time = (end_time - start_time).total_seconds()
        
        # çµæœé›†è¨ˆ
        metrics = self._calculate_llm_metrics()
        
        evaluation_result = {
            "evaluation_type": "system_compliance_llm",
            "timestamp": end_time.isoformat(),
            "processing_time_seconds": processing_time,
            "llm_provider": self.llm_manager.provider.__class__.__name__,
            "llm_model": self.llm_manager.provider.model_name,
            "summary": {
                "total_logs_processed": len(self.results),
                "successful_evaluations": len([r for r in self.results if r.llm_response.success]),
                "failed_evaluations": len([r for r in self.results if not r.llm_response.success]),
                "parser_issues_detected": len([r for r in self.results if r.parser_success_rate < 0.5]),
                "workflow_violations": len([r for r in self.results if not r.workflow_compliance]),
                "critical_systems": len([r for r in self.results if r.system_health == "CRITICAL"])
            },
            "compliance_metrics": asdict(metrics),
            "project_breakdown": self._get_project_breakdown(),
            "detailed_results": [self._serialize_result(result) for result in self.results]
        }
        
        print(f"âœ… ã‚·ã‚¹ãƒ†ãƒ é©åˆæ€§è©•ä¾¡å®Œäº†ï¼ˆLLMçµ±åˆç‰ˆï¼‰")
        print(f"  - å‡¦ç†æ™‚é–“: {processing_time:.2f}ç§’")
        print(f"  - å‡¦ç†ãƒ­ã‚°æ•°: {len(self.results)}")
        print(f"  - LLMæˆåŠŸç‡: {evaluation_result['summary']['successful_evaluations']}/{len(self.results)}")
        print(f"  - ç·åˆé©åˆæ€§ã‚¹ã‚³ã‚¢: {metrics.overall_compliance:.3f}")
        
        return evaluation_result
    
    async def _evaluate_single_log_async(self, log_entry, parsed_experiment) -> LogEvaluationResult:
        """å˜ä¸€ãƒ­ã‚°ã®éåŒæœŸè©•ä¾¡"""
        try:
            # LLMãƒ—ãƒ­ãƒ³ãƒ—ãƒˆç”Ÿæˆ
            prompt = self.log_parser.prompt_generator.generate_system_compliance_prompt(parsed_experiment)
            
            # LLMè©•ä¾¡å®Ÿè¡Œ
            llm_response = await self.llm_manager.evaluate_async(
                prompt=prompt,
                experiment_id=f"{log_entry.project}_{log_entry.log_path.stem}"
            )
            
            # è©•ä¾¡çµæœã‚’è§£æ
            parser_success_rate = self._calculate_parser_success_rate(parsed_experiment)
            workflow_compliance = self._check_workflow_compliance(parsed_experiment)
            system_health = self._assess_system_health(parser_success_rate, workflow_compliance, llm_response)
            
            return LogEvaluationResult(
                log_path=str(log_entry.log_path),
                project_name=log_entry.project,
                parser_success_rate=parser_success_rate,
                workflow_compliance=workflow_compliance,
                system_health=system_health,
                llm_response=llm_response,
                workflow_tags_detected=parsed_experiment.get('workflow_tags', []),
                evaluation_timestamp=datetime.now().isoformat()
            )
            
        except Exception as e:
            # ã‚¨ãƒ©ãƒ¼æ™‚ã¯ãƒ€ãƒŸãƒ¼çµæœã‚’è¿”ã™
            return LogEvaluationResult(
                log_path=str(log_entry.log_path),
                project_name=log_entry.project,
                parser_success_rate=0.0,
                workflow_compliance=False,
                system_health="CRITICAL",
                llm_response=LLMResponse(success=False, content="", error=str(e), evaluation_score=0.0),
                workflow_tags_detected=[],
                evaluation_timestamp=datetime.now().isoformat()
            )
    
    def _process_batch_results(self, batch_results: List):
        """ãƒãƒƒãƒå‡¦ç†çµæœã‚’å‡¦ç†"""
        for result in batch_results:
            if isinstance(result, Exception):
                print(f"âš ï¸  ãƒãƒƒãƒå‡¦ç†ã‚¨ãƒ©ãƒ¼: {result}")
                continue
            if isinstance(result, LogEvaluationResult):
                self.results.append(result)
    
    def _calculate_parser_success_rate(self, parsed_experiment: Dict) -> float:
        """ãƒ‘ãƒ¼ã‚µæˆåŠŸç‡ã‚’è¨ˆç®—"""
        total_operations = parsed_experiment.get('total_parsing_operations', 1)
        successful_operations = parsed_experiment.get('successful_parsing_operations', 0)
        return successful_operations / total_operations if total_operations > 0 else 0.0
    
    def _check_workflow_compliance(self, parsed_experiment: Dict) -> bool:
        """ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼é©åˆæ€§ã‚’ãƒã‚§ãƒƒã‚¯"""
        required_tags = ['%_Thought_%', '%_Plan_%', '%_Reply Required_%']
        detected_tags = parsed_experiment.get('workflow_tags', [])
        return len([tag for tag in required_tags if tag in detected_tags]) >= 2
    
    def _assess_system_health(self, parser_success_rate: float, workflow_compliance: bool, llm_response: LLMResponse) -> str:
        """ã‚·ã‚¹ãƒ†ãƒ å¥å…¨æ€§ã‚’è©•ä¾¡"""
        if parser_success_rate >= 0.8 and workflow_compliance and llm_response.success:
            return "GOOD"
        elif parser_success_rate >= 0.5 and (workflow_compliance or llm_response.success):
            return "WARNING"
        else:
            return "CRITICAL"
    
    def _calculate_llm_metrics(self) -> ComplianceMetrics:
        """LLMçµ±åˆãƒ¡ãƒˆãƒªã‚¯ã‚¹ã‚’è¨ˆç®—"""
        if not self.results:
            return ComplianceMetrics(0.0, 0.0, 0.0, 0.0, 0.0)
        
        # åŸºæœ¬çµ±è¨ˆ
        parser_success_avg = sum(r.parser_success_rate for r in self.results) / len(self.results)
        workflow_compliance_rate = len([r for r in self.results if r.workflow_compliance]) / len(self.results)
        llm_success_rate = len([r for r in self.results if r.llm_response.success]) / len(self.results)
        
        # ã‚·ã‚¹ãƒ†ãƒ å¥å…¨æ€§ã‚¹ã‚³ã‚¢
        health_scores = {"GOOD": 1.0, "WARNING": 0.5, "CRITICAL": 0.0}
        system_health_score = sum(health_scores.get(r.system_health, 0.0) for r in self.results) / len(self.results)
        
        # ç·åˆé©åˆæ€§ï¼ˆé‡ã¿ä»˜ãå¹³å‡ï¼‰
        overall_compliance = (
            parser_success_avg * 0.3 +
            workflow_compliance_rate * 0.3 +
            system_health_score * 0.2 +
            llm_success_rate * 0.2
        )
        
        return ComplianceMetrics(
            overall_compliance=overall_compliance,
            parser_success_avg=parser_success_avg,
            workflow_compliance_rate=workflow_compliance_rate,
            system_health_score=system_health_score,
            llm_evaluation_success_rate=llm_success_rate
        )
    
    def _get_project_breakdown(self) -> Dict[str, Any]:
        """ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆåˆ¥å†…è¨³ã‚’å–å¾—"""
        projects = {}
        for result in self.results:
            if result.project_name not in projects:
                projects[result.project_name] = {
                    "log_count": 0,
                    "avg_parser_success": 0.0,
                    "workflow_compliance_count": 0,
                    "health_distribution": {"GOOD": 0, "WARNING": 0, "CRITICAL": 0}
                }
            
            project = projects[result.project_name]
            project["log_count"] += 1
            project["avg_parser_success"] += result.parser_success_rate
            if result.workflow_compliance:
                project["workflow_compliance_count"] += 1
            project["health_distribution"][result.system_health] += 1
        
        # å¹³å‡å€¤ã‚’è¨ˆç®—
        for project in projects.values():
            if project["log_count"] > 0:
                project["avg_parser_success"] /= project["log_count"]
        
        return projects
    
    def _serialize_result(self, result: LogEvaluationResult) -> Dict[str, Any]:
        """è©•ä¾¡çµæœã‚’ã‚·ãƒªã‚¢ãƒ©ã‚¤ã‚º"""
        return {
            "log_file_name": result.log_path.split('/')[-1],
            "project_name": result.project_name,
            "parser_success_rate": result.parser_success_rate,
            "workflow_compliance": result.workflow_compliance,
            "system_health": result.system_health,
            "llm_evaluation_success": result.llm_response.success,
            "llm_evaluation_score": result.llm_response.evaluation_score,
            "workflow_tags_count": len(result.workflow_tags_detected),
            "evaluation_timestamp": result.evaluation_timestamp
        }

# ãƒ‡ãƒ¢å®Ÿè¡Œé–¢æ•°
async def demo_compliance_evaluation():
    """ãƒ‡ãƒ¢ç”¨ã®ã‚·ã‚¹ãƒ†ãƒ é©åˆæ€§è©•ä¾¡ã‚’å®Ÿè¡Œ"""
    print("ğŸ§ª ã‚·ã‚¹ãƒ†ãƒ é©åˆæ€§è©•ä¾¡å™¨ ãƒ‡ãƒ¢å®Ÿè¡Œ")
    print("=" * 40)
    
    evaluator = SystemComplianceEvaluator(llm_provider="mock")
    result = await evaluator.evaluate_all_projects()
    
    # çµæœã‚’ãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜
    output_file = f"/app/apr-output/llm_compliance_evaluation_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
    with open(output_file, 'w', encoding='utf-8') as f:
        json.dump(result, f, indent=2, ensure_ascii=False)
    
    print(f"ğŸ“ è©•ä¾¡çµæœã‚’ä¿å­˜: {output_file}")
    return result

if __name__ == "__main__":
    asyncio.run(demo_compliance_evaluation())
