"""
çµ±åˆè©•ä¾¡ã‚·ã‚¹ãƒ†ãƒ 
Step 1ã¨Step 2ã®è©•ä¾¡ã‚’çµ±åˆã—ã¦åŒ…æ‹¬çš„ãªåˆ†æã‚’å®Ÿè¡Œ
"""

import json
from pathlib import Path
from typing import Dict, Any
from datetime import datetime
from dataclasses import dataclass, asdict

from .compliance_evaluator import SystemComplianceEvaluator
from .quality_evaluator import PatchQualityEvaluator


@dataclass
class IntegratedMetrics:
    """çµ±åˆè©•ä¾¡ãƒ¡ãƒˆãƒªã‚¯ã‚¹"""
    # Step 1ãƒ¡ãƒˆãƒªã‚¯ã‚¹
    control_flow_accuracy: float
    parser_success_rate: float
    file_processing_rate: float
    error_handling_score: float
    overall_compliance: float
    
    # Step 2ãƒ¡ãƒˆãƒªã‚¯ã‚¹
    plausibility_score: float
    correctness_r1: float
    correctness_r5: float
    correctness_r10: float
    reasoning_quality: float
    semantic_similarity: float
    overall_quality: float
    
    # çµ±åˆãƒ¡ãƒˆãƒªã‚¯ã‚¹
    system_health_score: float      # ã‚·ã‚¹ãƒ†ãƒ å…¨ä½“ã®å¥å…¨æ€§
    ai_capability_score: float      # AIèƒ½åŠ›ã‚¹ã‚³ã‚¢
    overall_apr_score: float        # ç·åˆAPRã‚¹ã‚³ã‚¢
    
    # æ¨å¥¨äº‹é …
    recommendations: list


class IntegratedEvaluator:
    """çµ±åˆè©•ä¾¡ã‚·ã‚¹ãƒ†ãƒ """
    
    def __init__(self):
        self.compliance_evaluator = SystemComplianceEvaluator()
        self.quality_evaluator = PatchQualityEvaluator()
        self.results = {}
    
    def run_full_evaluation(self, include_step1: bool = True, include_step2: bool = True) -> Dict[str, Any]:
        """å®Œå…¨ãªè©•ä¾¡ã‚’å®Ÿè¡Œ"""
        print("ğŸš€ çµ±åˆAPRè©•ä¾¡ã‚·ã‚¹ãƒ†ãƒ å®Ÿè¡Œé–‹å§‹")
        print("=" * 60)
        
        start_time = datetime.now()
        evaluation_results = {
            "evaluation_metadata": {
                "timestamp": start_time.isoformat(),
                "step1_enabled": include_step1,
                "step2_enabled": include_step2,
                "evaluator_version": "1.0"
            }
        }
        
        # Step 1: ã‚·ã‚¹ãƒ†ãƒ é©åˆæ€§è©•ä¾¡
        step1_results = None
        if include_step1:
            print("ğŸ” Step 1: ã‚·ã‚¹ãƒ†ãƒ é©åˆæ€§è©•ä¾¡å®Ÿè¡Œä¸­...")
            step1_results = self.compliance_evaluator.evaluate_all_projects()
            evaluation_results["step1_results"] = step1_results
            print("âœ… Step 1å®Œäº†\n")
        
        # Step 2: ãƒ‘ãƒƒãƒå“è³ªè©•ä¾¡
        step2_results = None
        if include_step2:
            print("ğŸ¯ Step 2: ãƒ‘ãƒƒãƒå“è³ªè©•ä¾¡å®Ÿè¡Œä¸­...")
            step2_results = self.quality_evaluator.evaluate_all_patches()
            evaluation_results["step2_results"] = step2_results
            print("âœ… Step 2å®Œäº†\n")
        
        # çµ±åˆåˆ†æ
        print("ğŸ“Š çµ±åˆåˆ†æå®Ÿè¡Œä¸­...")
        integrated_analysis = self._perform_integrated_analysis(step1_results, step2_results)
        evaluation_results["integrated_analysis"] = integrated_analysis
        
        # çµ‚äº†å‡¦ç†
        end_time = datetime.now()
        total_processing_time = (end_time - start_time).total_seconds()
        
        evaluation_results["evaluation_metadata"]["end_timestamp"] = end_time.isoformat()
        evaluation_results["evaluation_metadata"]["total_processing_time_seconds"] = total_processing_time
        
        print(f"ğŸ‰ çµ±åˆè©•ä¾¡å®Œäº† - å‡¦ç†æ™‚é–“: {total_processing_time:.2f}ç§’")
        print(f"ğŸ“ˆ ç·åˆAPRã‚¹ã‚³ã‚¢: {integrated_analysis['metrics']['overall_apr_score']:.3f}")
        
        return evaluation_results
    
    def _perform_integrated_analysis(self, step1_results: Dict, step2_results: Dict) -> Dict[str, Any]:
        """çµ±åˆåˆ†æã®å®Ÿè¡Œ"""
        
        # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤ã§åˆæœŸåŒ–
        compliance_metrics = {
            "control_flow_accuracy": 0.0,
            "parser_success_rate": 0.0,
            "file_processing_rate": 0.0,
            "error_handling_score": 0.0,
            "overall_compliance": 0.0
        }
        
        quality_metrics = {
            "plausibility_score": 0.0,
            "correctness_r1": 0.0,
            "correctness_r5": 0.0,
            "correctness_r10": 0.0,
            "reasoning_quality": 0.0,
            "semantic_similarity": 0.0,
            "overall_quality": 0.0
        }
        
        # Step 1çµæœã®å–å¾—
        if step1_results:
            compliance_metrics.update(step1_results.get("compliance_metrics", {}))
        
        # Step 2çµæœã®å–å¾—
        if step2_results:
            quality_metrics.update(step2_results.get("quality_metrics", {}))
        
        # çµ±åˆãƒ¡ãƒˆãƒªã‚¯ã‚¹ã®è¨ˆç®—
        integrated_metrics = self._calculate_integrated_metrics(compliance_metrics, quality_metrics)
        
        # è©³ç´°åˆ†æ
        detailed_analysis = self._generate_detailed_analysis(step1_results, step2_results, integrated_metrics)
        
        # æ¨å¥¨äº‹é …ã®ç”Ÿæˆ
        recommendations = self._generate_recommendations(integrated_metrics)
        
        return {
            "metrics": asdict(integrated_metrics),
            "detailed_analysis": detailed_analysis,
            "recommendations": recommendations,
            "cross_step_correlations": self._analyze_correlations(step1_results, step2_results),
            "performance_summary": self._generate_performance_summary(step1_results, step2_results)
        }
    
    def _calculate_integrated_metrics(self, compliance_metrics: Dict, quality_metrics: Dict) -> IntegratedMetrics:
        """çµ±åˆãƒ¡ãƒˆãƒªã‚¯ã‚¹ã®è¨ˆç®—"""
        
        # ã‚·ã‚¹ãƒ†ãƒ å¥å…¨æ€§ã‚¹ã‚³ã‚¢ï¼ˆStep 1ã®çµæœã‹ã‚‰ï¼‰
        system_health_score = (
            compliance_metrics["control_flow_accuracy"] * 0.3 +
            compliance_metrics["parser_success_rate"] * 0.3 +
            compliance_metrics["file_processing_rate"] * 0.2 +
            compliance_metrics["error_handling_score"] * 0.2
        )
        
        # AIèƒ½åŠ›ã‚¹ã‚³ã‚¢ï¼ˆStep 2ã®çµæœã‹ã‚‰ï¼‰
        ai_capability_score = (
            quality_metrics["plausibility_score"] * 0.25 +
            quality_metrics["correctness_r1"] * 0.30 +
            quality_metrics["correctness_r10"] * 0.25 +
            quality_metrics["reasoning_quality"] * 0.20
        )
        
        # ç·åˆAPRã‚¹ã‚³ã‚¢ï¼ˆã‚·ã‚¹ãƒ†ãƒ å¥å…¨æ€§ã¨AIèƒ½åŠ›ã®çµ„ã¿åˆã‚ã›ï¼‰
        overall_apr_score = (system_health_score * 0.4 + ai_capability_score * 0.6)
        
        # æ¨å¥¨äº‹é …ã®ç”Ÿæˆ
        recommendations = []
        if system_health_score < 0.7:
            recommendations.append("ã‚·ã‚¹ãƒ†ãƒ ã®åŸºæœ¬æ©Ÿèƒ½ï¼ˆãƒ‘ãƒ¼ã‚µãƒ¼ã€åˆ¶å¾¡ãƒ•ãƒ­ãƒ¼è§£æï¼‰ã®æ”¹å–„ãŒå¿…è¦")
        if ai_capability_score < 0.5:
            recommendations.append("AIæ¨è«–èƒ½åŠ›ã®å‘ä¸Šã¨ãƒ‘ãƒƒãƒç”Ÿæˆå“è³ªã®æ”¹å–„ãŒå¿…è¦")
        if compliance_metrics["error_handling_score"] < 0.6:
            recommendations.append("ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°ã®æ”¹å–„ãŒæ€¥å‹™")
        if quality_metrics["correctness_r1"] < 0.2:
            recommendations.append("æ­£ç¢ºãªãƒ‘ãƒƒãƒç”Ÿæˆèƒ½åŠ›ã®å¤§å¹…ãªæ”¹å–„ãŒå¿…è¦")
        
        return IntegratedMetrics(
            # Step 1ãƒ¡ãƒˆãƒªã‚¯ã‚¹
            control_flow_accuracy=compliance_metrics["control_flow_accuracy"],
            parser_success_rate=compliance_metrics["parser_success_rate"],
            file_processing_rate=compliance_metrics["file_processing_rate"],
            error_handling_score=compliance_metrics["error_handling_score"],
            overall_compliance=compliance_metrics["overall_compliance"],
            
            # Step 2ãƒ¡ãƒˆãƒªã‚¯ã‚¹
            plausibility_score=quality_metrics["plausibility_score"],
            correctness_r1=quality_metrics["correctness_r1"],
            correctness_r5=quality_metrics["correctness_r5"],
            correctness_r10=quality_metrics["correctness_r10"],
            reasoning_quality=quality_metrics["reasoning_quality"],
            semantic_similarity=quality_metrics["semantic_similarity"],
            overall_quality=quality_metrics["overall_quality"],
            
            # çµ±åˆãƒ¡ãƒˆãƒªã‚¯ã‚¹
            system_health_score=system_health_score,
            ai_capability_score=ai_capability_score,
            overall_apr_score=overall_apr_score,
            recommendations=recommendations
        )
    
    def _generate_detailed_analysis(self, step1_results: Dict, step2_results: Dict, 
                                  integrated_metrics: IntegratedMetrics) -> Dict[str, Any]:
        """è©³ç´°åˆ†æã®ç”Ÿæˆ"""
        
        analysis = {
            "system_performance": {
                "strengths": [],
                "weaknesses": [],
                "critical_issues": []
            },
            "quality_assessment": {
                "patch_generation_capability": "unknown",
                "reasoning_effectiveness": "unknown",
                "accuracy_level": "unknown"
            },
            "comparative_analysis": {
                "best_performing_projects": [],
                "worst_performing_projects": [],
                "performance_variance": 0.0
            }
        }
        
        # ã‚·ã‚¹ãƒ†ãƒ æ€§èƒ½åˆ†æ
        if integrated_metrics.system_health_score > 0.8:
            analysis["system_performance"]["strengths"].append("å„ªç§€ãªã‚·ã‚¹ãƒ†ãƒ åŸºç›¤æ€§èƒ½")
        elif integrated_metrics.system_health_score < 0.5:
            analysis["system_performance"]["critical_issues"].append("ã‚·ã‚¹ãƒ†ãƒ åŸºç›¤ã«é‡å¤§ãªå•é¡Œ")
        
        if integrated_metrics.parser_success_rate > 0.9:
            analysis["system_performance"]["strengths"].append("é«˜ã„ãƒ‘ãƒ¼ã‚µãƒ¼æˆåŠŸç‡")
        elif integrated_metrics.parser_success_rate < 0.7:
            analysis["system_performance"]["weaknesses"].append("ãƒ‘ãƒ¼ã‚µãƒ¼æˆåŠŸç‡ãŒä½ã„")
        
        # å“è³ªè©•ä¾¡åˆ†æ
        if integrated_metrics.correctness_r1 > 0.3:
            analysis["quality_assessment"]["accuracy_level"] = "é«˜ç²¾åº¦"
        elif integrated_metrics.correctness_r1 > 0.1:
            analysis["quality_assessment"]["accuracy_level"] = "ä¸­ç²¾åº¦"
        else:
            analysis["quality_assessment"]["accuracy_level"] = "ä½ç²¾åº¦"
        
        if integrated_metrics.reasoning_quality > 0.7:
            analysis["quality_assessment"]["reasoning_effectiveness"] = "åŠ¹æœçš„"
        elif integrated_metrics.reasoning_quality > 0.4:
            analysis["quality_assessment"]["reasoning_effectiveness"] = "æ™®é€š"
        else:
            analysis["quality_assessment"]["reasoning_effectiveness"] = "æ”¹å–„ãŒå¿…è¦"
        
        # ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆåˆ¥æ¯”è¼ƒåˆ†æ
        if step1_results and "project_breakdown" in step1_results:
            project_scores = {}
            for project, stats in step1_results["project_breakdown"].items():
                project_scores[project] = stats.get("avg_parser_score", 0.0)
            
            if project_scores:
                sorted_projects = sorted(project_scores.items(), key=lambda x: x[1], reverse=True)
                analysis["comparative_analysis"]["best_performing_projects"] = sorted_projects[:3]
                analysis["comparative_analysis"]["worst_performing_projects"] = sorted_projects[-3:]
                
                scores = list(project_scores.values())
                if len(scores) > 1:
                    variance = sum((x - sum(scores)/len(scores))**2 for x in scores) / len(scores)
                    analysis["comparative_analysis"]["performance_variance"] = variance
        
        return analysis
    
    def _generate_recommendations(self, metrics: IntegratedMetrics) -> List[Dict[str, Any]]:
        """æ¨å¥¨äº‹é …ã®ç”Ÿæˆ"""
        recommendations = []
        
        # é«˜å„ªå…ˆåº¦ã®æ¨å¥¨äº‹é …
        if metrics.overall_apr_score < 0.4:
            recommendations.append({
                "priority": "HIGH",
                "category": "critical_improvement",
                "title": "APRã‚·ã‚¹ãƒ†ãƒ å…¨ä½“ã®ç·Šæ€¥æ”¹å–„",
                "description": "ã‚·ã‚¹ãƒ†ãƒ åŸºç›¤ã¨AIèƒ½åŠ›ã®ä¸¡æ–¹ã«é‡å¤§ãªå•é¡ŒãŒã‚ã‚Šã€åŒ…æ‹¬çš„ãªè¦‹ç›´ã—ãŒå¿…è¦ã§ã™ã€‚",
                "action_items": [
                    "åŸºæœ¬çš„ãªãƒ‘ãƒ¼ã‚µãƒ¼æ©Ÿèƒ½ã®ä¿®æ­£",
                    "åˆ¶å¾¡ãƒ•ãƒ­ãƒ¼è§£æã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã®æ”¹å–„", 
                    "AIæ¨è«–ãƒ¢ãƒ‡ãƒ«ã®å†æ¤œè¨",
                    "å“è³ªä¿è¨¼ãƒ—ãƒ­ã‚»ã‚¹ã®å¼·åŒ–"
                ]
            })
        
        # ãƒ‘ãƒ¼ã‚µãƒ¼é–¢é€£ã®æ¨å¥¨äº‹é …
        if metrics.parser_success_rate < 0.7:
            recommendations.append({
                "priority": "HIGH",
                "category": "parser_improvement",
                "title": "ãƒ‘ãƒ¼ã‚µãƒ¼æ©Ÿèƒ½ã®æ”¹å–„",
                "description": f"ãƒ‘ãƒ¼ã‚µãƒ¼æˆåŠŸç‡ãŒ{metrics.parser_success_rate:.1%}ã¨ä½ãã€åŸºæœ¬çš„ãªã‚³ãƒ¼ãƒ‰è§£æã«å•é¡ŒãŒã‚ã‚Šã¾ã™ã€‚",
                "action_items": [
                    "ãƒ‘ãƒ¼ã‚µãƒ¼ã®ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°å¼·åŒ–",
                    "ã‚µãƒãƒ¼ãƒˆã™ã‚‹è¨€èªä»•æ§˜ã®æ‹¡å¼µ",
                    "æ§‹æ–‡ã‚¨ãƒ©ãƒ¼å›å¾©æ©Ÿèƒ½ã®å®Ÿè£…"
                ]
            })
        
        # å“è³ªé–¢é€£ã®æ¨å¥¨äº‹é …
        if metrics.correctness_r1 < 0.2:
            recommendations.append({
                "priority": "HIGH",
                "category": "quality_improvement", 
                "title": "ãƒ‘ãƒƒãƒå“è³ªã®å¤§å¹…æ”¹å–„",
                "description": f"æ­£ç¢ºæ€§@1ãŒ{metrics.correctness_r1:.1%}ã¨æ¥µã‚ã¦ä½ãã€ãƒ‘ãƒƒãƒç”Ÿæˆèƒ½åŠ›ã«é‡å¤§ãªå•é¡ŒãŒã‚ã‚Šã¾ã™ã€‚",
                "action_items": [
                    "LLMãƒ¢ãƒ‡ãƒ«ã®å†è©•ä¾¡ãƒ»å¤‰æ›´",
                    "ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚°ã®æ”¹å–„",
                    "ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ãƒ‡ãƒ¼ã‚¿ã®å“è³ªå‘ä¸Š",
                    "è©•ä¾¡åŸºæº–ã®è¦‹ç›´ã—"
                ]
            })
        
        # ä¸­å„ªå…ˆåº¦ã®æ¨å¥¨äº‹é …
        if metrics.reasoning_quality < 0.6:
            recommendations.append({
                "priority": "MEDIUM",
                "category": "reasoning_improvement",
                "title": "æ¨è«–å“è³ªã®å‘ä¸Š",
                "description": "AIæ¨è«–ãƒ—ãƒ­ã‚»ã‚¹ã®æ”¹å–„ã«ã‚ˆã‚Šã€ã‚ˆã‚Šé©åˆ‡ãªãƒ‘ãƒƒãƒç”ŸæˆãŒæœŸå¾…ã§ãã¾ã™ã€‚",
                "action_items": [
                    "æ¨è«–ã‚¹ãƒ†ãƒƒãƒ—ã®å¯è¦–åŒ–",
                    "ä¸­é–“çµæœã®å“è³ªãƒã‚§ãƒƒã‚¯", 
                    "æ¨è«–ãƒã‚§ãƒ¼ãƒ³ã®æœ€é©åŒ–"
                ]
            })
        
        # ã‚·ã‚¹ãƒ†ãƒ å®‰å®šæ€§ã®æ¨å¥¨äº‹é …
        if metrics.error_handling_score < 0.6:
            recommendations.append({
                "priority": "MEDIUM",
                "category": "stability_improvement",
                "title": "ã‚·ã‚¹ãƒ†ãƒ å®‰å®šæ€§ã®å‘ä¸Š",
                "description": "ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°ã®æ”¹å–„ã«ã‚ˆã‚Šã€ã‚·ã‚¹ãƒ†ãƒ ã®å®‰å®šæ€§ã‚’å‘ä¸Šã•ã›ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™ã€‚",
                "action_items": [
                    "ä¾‹å¤–å‡¦ç†ã®æ”¹å–„",
                    "ãƒ­ã‚°å“è³ªã®å‘ä¸Š",
                    "å›å¾©æ©Ÿèƒ½ã®å®Ÿè£…",
                    "ç›£è¦–ã‚·ã‚¹ãƒ†ãƒ ã®å°å…¥"
                ]
            })
        
        # ä½å„ªå…ˆåº¦ã®æ”¹å–„äº‹é …
        if metrics.overall_apr_score > 0.6:
            recommendations.append({
                "priority": "LOW",
                "category": "optimization",
                "title": "æœ€é©åŒ–ã¨å¾®èª¿æ•´",
                "description": "åŸºæœ¬çš„ãªæ€§èƒ½ã¯è‰¯å¥½ãªãŸã‚ã€ã•ã‚‰ãªã‚‹æœ€é©åŒ–ã«å–ã‚Šçµ„ã‚ã¾ã™ã€‚",
                "action_items": [
                    "å‡¦ç†é€Ÿåº¦ã®å‘ä¸Š",
                    "ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ã®æœ€é©åŒ–",
                    "ãƒ¦ãƒ¼ã‚¶ãƒ¼ã‚¤ãƒ³ã‚¿ãƒ¼ãƒ•ã‚§ãƒ¼ã‚¹ã®æ”¹å–„",
                    "è©³ç´°ãªåˆ†ææ©Ÿèƒ½ã®è¿½åŠ "
                ]
            })
        
        return recommendations
    
    def _analyze_correlations(self, step1_results: Dict, step2_results: Dict) -> Dict[str, Any]:
        """Stepé–“ã®ç›¸é–¢åˆ†æ"""
        correlations = {
            "parser_quality_correlation": "unknown",
            "system_health_impact": "unknown", 
            "error_impact_analysis": "unknown"
        }
        
        # ç°¡å˜ãªç›¸é–¢åˆ†æï¼ˆã‚ˆã‚Šè©³ç´°ãªåˆ†æã¯å°†æ¥ã®æ‹¡å¼µã§ï¼‰
        if step1_results and step2_results:
            compliance_score = step1_results.get("compliance_metrics", {}).get("overall_compliance", 0)
            quality_score = step2_results.get("quality_metrics", {}).get("overall_quality", 0)
            
            if compliance_score > 0.7 and quality_score > 0.5:
                correlations["parser_quality_correlation"] = "strong_positive"
            elif compliance_score > 0.5 and quality_score < 0.3:
                correlations["parser_quality_correlation"] = "weak_or_negative"
            else:
                correlations["parser_quality_correlation"] = "unclear"
        
        return correlations
    
    def _generate_performance_summary(self, step1_results: Dict, step2_results: Dict) -> Dict[str, Any]:
        """ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹è¦ç´„ã®ç”Ÿæˆ"""
        summary = {
            "processing_efficiency": "unknown",
            "resource_utilization": "unknown",
            "scalability_assessment": "unknown"
        }
        
        # å‡¦ç†æ™‚é–“ã®åˆ†æ
        step1_time = step1_results.get("processing_time_seconds", 0) if step1_results else 0
        step2_time = step2_results.get("processing_time_seconds", 0) if step2_results else 0
        total_time = step1_time + step2_time
        
        if total_time < 300:  # 5åˆ†æœªæº€
            summary["processing_efficiency"] = "excellent"
        elif total_time < 1800:  # 30åˆ†æœªæº€
            summary["processing_efficiency"] = "good"
        elif total_time < 3600:  # 60åˆ†æœªæº€
            summary["processing_efficiency"] = "acceptable"
        else:
            summary["processing_efficiency"] = "needs_improvement"
        
        return summary


def demo_integrated_evaluation():
    """çµ±åˆè©•ä¾¡ã®ãƒ‡ãƒ¢"""
    print("ğŸš€ çµ±åˆAPRè©•ä¾¡ã‚·ã‚¹ãƒ†ãƒ ãƒ‡ãƒ¢")
    
    evaluator = IntegratedEvaluator()
    results = evaluator.run_full_evaluation()
    
    # çµæœã®ä¿å­˜
    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
    output_file = Path(f"/app/results/integrated_evaluation_{timestamp}.json")
    output_file.parent.mkdir(parents=True, exist_ok=True)
    
    with open(output_file, 'w', encoding='utf-8') as f:
        json.dump(results, f, indent=2, ensure_ascii=False)
    
    print(f"âœ… çµ±åˆè©•ä¾¡çµæœã‚’ä¿å­˜: {output_file}")
    
    return results


if __name__ == "__main__":
    demo_integrated_evaluation()
