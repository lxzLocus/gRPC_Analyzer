#!/usr/bin/env python3
"""
å®ŸLLMè©•ä¾¡å®Ÿè¡Œãƒ„ãƒ¼ãƒ«
OpenAIã‚’ä½¿ç”¨ã—ãŸå®Ÿéš›ã®APRã‚·ã‚¹ãƒ†ãƒ è©•ä¾¡ã¨è©³ç´°ãƒ­ã‚°å‡ºåŠ›
"""

import asyncio
import json
import sys
import os
from datetime import datetime
from pathlib import Path
import logging
from typing import Dict, Any, List

# .envãƒ•ã‚¡ã‚¤ãƒ«ã‚’æ˜ç¤ºçš„ã«ãƒ­ãƒ¼ãƒ‰
try:
    from dotenv import load_dotenv
    load_dotenv("/app/.env")
    print(f"âœ… .envãƒ•ã‚¡ã‚¤ãƒ«ã‚’ãƒ­ãƒ¼ãƒ‰: OPENAI_API_KEY={'è¨­å®šæ¸ˆã¿' if os.getenv('OPENAI_API_KEY') else 'æœªè¨­å®š'}")
except ImportError:
    print("âš ï¸  python-dotenvãŒæœªã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ« - OSã®ç’°å¢ƒå¤‰æ•°ã‚’ä½¿ç”¨")

sys.path.append('/app')
sys.path.append('/app/src')

from src.evaluators.compliance_evaluator import SystemComplianceEvaluator


class DetailedLLMLogger:
    """è©³ç´°ãªLLMè©•ä¾¡ãƒ­ã‚°å‡ºåŠ›ã‚¯ãƒ©ã‚¹"""
    
    def __init__(self, output_dir: str = "/app/logs"):
        self.output_dir = Path(output_dir)
        self.output_dir.mkdir(exist_ok=True)
        
        # ãƒ­ã‚°ãƒ•ã‚¡ã‚¤ãƒ«åã‚’ç”Ÿæˆ
        self.timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        self.log_file = self.output_dir / f"llm_evaluation_detailed_{self.timestamp}.log"
        self.json_file = self.output_dir / f"llm_responses_{self.timestamp}.json"
        
        # ãƒ­ã‚°è¨­å®š
        logging.basicConfig(
            level=logging.INFO,
            format='%(asctime)s - %(levelname)s - %(message)s',
            handlers=[
                logging.FileHandler(self.log_file, encoding='utf-8'),
                logging.StreamHandler(sys.stdout)
            ]
        )
        self.logger = logging.getLogger(__name__)
        
        # LLMãƒ¬ã‚¹ãƒãƒ³ã‚¹è©³ç´°ãƒ‡ãƒ¼ã‚¿
        self.llm_responses = []
        
        self.logger.info(f"ğŸ” è©³ç´°LLMè©•ä¾¡ãƒ­ã‚°é–‹å§‹ - {self.timestamp}")
        self.logger.info(f"ğŸ“„ ãƒ­ã‚°ãƒ•ã‚¡ã‚¤ãƒ«: {self.log_file}")
        self.logger.info(f"ğŸ“Š JSONãƒ•ã‚¡ã‚¤ãƒ«: {self.json_file}")
    
    def log_evaluation_start(self, repository: str, max_logs: int, provider: str, model: str):
        """è©•ä¾¡é–‹å§‹ã®ãƒ­ã‚°"""
        self.logger.info("=" * 80)
        self.logger.info(f"ğŸ¯ å®ŸLLMè©•ä¾¡é–‹å§‹")
        self.logger.info(f"ğŸ“‚ å¯¾è±¡ãƒªãƒã‚¸ãƒˆãƒª: {repository}")
        self.logger.info(f"ğŸ“Š æœ€å¤§å‡¦ç†ãƒ­ã‚°æ•°: {max_logs}")
        self.logger.info(f"ğŸ¤– LLMãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼: {provider}")
        self.logger.info(f"ğŸ§  LLMãƒ¢ãƒ‡ãƒ«: {model}")
        self.logger.info("=" * 80)
    
    def log_prompt_and_response(self, log_path: str, experiment_id: str, 
                               prompt: str, response: str, usage: Dict, success: bool, 
                               error: str = None):
        """ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã¨ãƒ¬ã‚¹ãƒãƒ³ã‚¹ã®è©³ç´°ãƒ­ã‚°"""
        
        log_name = Path(log_path).name
        
        self.logger.info(f"\nğŸ“‹ ãƒ­ã‚°å‡¦ç†: {log_name}")
        self.logger.info(f"ğŸ†” å®Ÿé¨“ID: {experiment_id}")
        self.logger.info(f"ğŸ“ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆé•·: {len(prompt)} æ–‡å­—")
        self.logger.info(f"âœ… LLMæˆåŠŸ: {success}")
        
        if error:
            self.logger.error(f"âŒ LLMã‚¨ãƒ©ãƒ¼: {error}")
        
        if usage:
            total_tokens = usage.get('total_tokens', 0)
            prompt_tokens = usage.get('prompt_tokens', 0) 
            completion_tokens = usage.get('completion_tokens', 0)
            self.logger.info(f"ğŸ§® ãƒˆãƒ¼ã‚¯ãƒ³ä½¿ç”¨é‡: {total_tokens} (å…¥åŠ›:{prompt_tokens}, å‡ºåŠ›:{completion_tokens})")
        
        # è©³ç´°ãƒ‡ãƒ¼ã‚¿ã‚’ä¿å­˜
        response_data = {
            "timestamp": datetime.now().isoformat(),
            "log_path": log_path,
            "experiment_id": experiment_id,
            "prompt": prompt,
            "response": response,
            "usage": usage,
            "success": success,
            "error": error,
            "prompt_length": len(prompt),
            "response_length": len(response) if response else 0
        }
        
        self.llm_responses.append(response_data)
        
        # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆå†…å®¹ã‚’ä¸€éƒ¨è¡¨ç¤º
        self.logger.info(f"ğŸ“„ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆå†…å®¹ï¼ˆæœ€åˆã®200æ–‡å­—ï¼‰:")
        self.logger.info(prompt[:200] + "...")
        
        # ãƒ¬ã‚¹ãƒãƒ³ã‚¹å†…å®¹ã‚’è¡¨ç¤º
        if success and response:
            self.logger.info(f"ğŸ¤– LLMãƒ¬ã‚¹ãƒãƒ³ã‚¹å†…å®¹ï¼ˆæœ€åˆã®300æ–‡å­—ï¼‰:")
            self.logger.info(response[:300] + "...")
            
            # JSONãƒ¬ã‚¹ãƒãƒ³ã‚¹ã®å ´åˆã¯æ§‹é€ ã‚‚è§£æ
            try:
                parsed_response = json.loads(response)
                self.logger.info("ğŸ” ãƒ¬ã‚¹ãƒãƒ³ã‚¹æ§‹é€ è§£æ:")
                
                if "parser_evaluation" in parsed_response:
                    parser_evals = parsed_response["parser_evaluation"]
                    self.logger.info(f"  ğŸ“ˆ ãƒ‘ãƒ¼ã‚µãƒ¼è©•ä¾¡: {len(parser_evals)}ã‚¿ãƒ¼ãƒ³")
                    for i, eval_item in enumerate(parser_evals, 1):
                        status = eval_item.get("status", "N/A")
                        reasoning = eval_item.get("reasoning", "")[:60]
                        self.logger.info(f"    Turn {i}: {status} - {reasoning}...")
                
                if "workflow_evaluation" in parsed_response:
                    workflow = parsed_response["workflow_evaluation"]
                    compliance = workflow.get("is_compliant", "N/A")
                    self.logger.info(f"  ğŸ”„ ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼é©åˆ: {compliance}")
                
                if "overall_assessment" in parsed_response:
                    assessment = parsed_response["overall_assessment"]
                    health = assessment.get("system_health", "N/A")
                    issues_count = len(assessment.get("critical_issues", []))
                    recommendations_count = len(assessment.get("recommendations", []))
                    self.logger.info(f"  â¤ï¸  ã‚·ã‚¹ãƒ†ãƒ å¥å…¨æ€§: {health}")
                    self.logger.info(f"  âš ï¸  é‡å¤§å•é¡Œæ•°: {issues_count}")
                    self.logger.info(f"  ğŸ’¡ æ¨å¥¨äº‹é …æ•°: {recommendations_count}")
                    
            except json.JSONDecodeError:
                self.logger.info("  ğŸ“ éJSONå½¢å¼ã®ãƒ¬ã‚¹ãƒãƒ³ã‚¹")
        else:
            self.logger.warning("âš ï¸  ãƒ¬ã‚¹ãƒãƒ³ã‚¹ãŒç©ºã¾ãŸã¯å¤±æ•—")
    
    def log_evaluation_summary(self, result: Dict[str, Any]):
        """è©•ä¾¡ã‚µãƒãƒªãƒ¼ã®ãƒ­ã‚°"""
        self.logger.info("\n" + "=" * 80)
        self.logger.info("ğŸ“Š è©•ä¾¡çµæœã‚µãƒãƒªãƒ¼")
        self.logger.info("=" * 80)
        
        summary = result.get("summary", {})
        metrics = result.get("compliance_metrics", {})
        
        self.logger.info(f"ğŸ“‚ ãƒªãƒã‚¸ãƒˆãƒª: {result.get('repository_name', 'N/A')}")
        self.logger.info(f"â±ï¸  å‡¦ç†æ™‚é–“: {result.get('processing_time_seconds', 0):.2f}ç§’")
        self.logger.info(f"ğŸ¤– LLMãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼: {result.get('llm_provider', 'N/A')}")
        self.logger.info(f"ğŸ§  LLMãƒ¢ãƒ‡ãƒ«: {result.get('llm_model', 'N/A')}")
        
        self.logger.info("\nğŸ“ˆ å‡¦ç†çµ±è¨ˆ:")
        self.logger.info(f"  - ç·å‡¦ç†ãƒ­ã‚°æ•°: {summary.get('total_logs_processed', 0)}")
        self.logger.info(f"  - LLMæˆåŠŸè©•ä¾¡: {summary.get('successful_evaluations', 0)}")
        self.logger.info(f"  - LLMå¤±æ•—è©•ä¾¡: {summary.get('failed_evaluations', 0)}")
        self.logger.info(f"  - ãƒ‘ãƒ¼ã‚µãƒ¼å•é¡Œæ¤œå‡º: {summary.get('parser_issues_detected', 0)}")
        self.logger.info(f"  - ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼é•å: {summary.get('workflow_violations', 0)}")
        self.logger.info(f"  - é‡å¤§ã‚·ã‚¹ãƒ†ãƒ æ•°: {summary.get('critical_systems', 0)}")
        
        self.logger.info("\nğŸ¯ é©åˆæ€§ãƒ¡ãƒˆãƒªã‚¯ã‚¹:")
        self.logger.info(f"  - åˆ¶å¾¡ãƒ•ãƒ­ãƒ¼ç²¾åº¦: {metrics.get('control_flow_accuracy', 0):.3f}")
        self.logger.info(f"  - ãƒ‘ãƒ¼ã‚µãƒ¼æˆåŠŸç‡: {metrics.get('parser_success_rate', 0):.3f}")
        self.logger.info(f"  - ãƒ•ã‚¡ã‚¤ãƒ«å‡¦ç†ç‡: {metrics.get('file_processing_rate', 0):.3f}")
        self.logger.info(f"  - ã‚¨ãƒ©ãƒ¼å‡¦ç†ã‚¹ã‚³ã‚¢: {metrics.get('error_handling_score', 0):.3f}")
        self.logger.info(f"  - ç·åˆé©åˆæ€§: {metrics.get('overall_compliance', 0):.3f}")
        
        # ãƒˆãƒ¼ã‚¯ãƒ³ä½¿ç”¨é‡ã®é›†è¨ˆ
        total_tokens = 0
        total_cost_estimate = 0.0
        for response_data in self.llm_responses:
            if response_data.get("usage"):
                tokens = response_data["usage"].get("total_tokens", 0)
                total_tokens += tokens
                # GPT-4ã®æ¦‚ç®—ã‚³ã‚¹ãƒˆ (input: $0.03/1K, output: $0.06/1K tokens)
                input_tokens = response_data["usage"].get("prompt_tokens", 0)
                output_tokens = response_data["usage"].get("completion_tokens", 0)
                cost = (input_tokens * 0.03 + output_tokens * 0.06) / 1000
                total_cost_estimate += cost
        
        self.logger.info(f"\nğŸ’° ã‚³ã‚¹ãƒˆæƒ…å ±:")
        self.logger.info(f"  - ç·ãƒˆãƒ¼ã‚¯ãƒ³ä½¿ç”¨é‡: {total_tokens:,}")
        self.logger.info(f"  - æ¦‚ç®—ã‚³ã‚¹ãƒˆ: ${total_cost_estimate:.4f} USD")
        
    def save_detailed_responses(self):
        """è©³ç´°ãªãƒ¬ã‚¹ãƒãƒ³ã‚¹ãƒ‡ãƒ¼ã‚¿ã‚’JSONãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜"""
        detailed_data = {
            "evaluation_session": {
                "timestamp": self.timestamp,
                "total_responses": len(self.llm_responses),
                "log_file": str(self.log_file),
                "summary": {
                    "successful_responses": len([r for r in self.llm_responses if r.get("success")]),
                    "failed_responses": len([r for r in self.llm_responses if not r.get("success")]),
                    "total_tokens": sum(r.get("usage", {}).get("total_tokens", 0) for r in self.llm_responses),
                    "average_prompt_length": sum(r.get("prompt_length", 0) for r in self.llm_responses) / len(self.llm_responses) if self.llm_responses else 0,
                    "average_response_length": sum(r.get("response_length", 0) for r in self.llm_responses) / len(self.llm_responses) if self.llm_responses else 0
                }
            },
            "llm_responses": self.llm_responses
        }
        
        with open(self.json_file, 'w', encoding='utf-8') as f:
            json.dump(detailed_data, f, indent=2, ensure_ascii=False)
        
        self.logger.info(f"ğŸ’¾ è©³ç´°ãƒ¬ã‚¹ãƒãƒ³ã‚¹ãƒ‡ãƒ¼ã‚¿ã‚’ä¿å­˜: {self.json_file}")
        
    def finalize(self):
        """ãƒ­ã‚°çµ‚äº†å‡¦ç†"""
        self.save_detailed_responses()
        self.logger.info("=" * 80)
        self.logger.info(f"âœ… è©³ç´°LLMè©•ä¾¡ãƒ­ã‚°å®Œäº† - {self.timestamp}")
        self.logger.info(f"ğŸ“‹ ãƒ­ã‚°ãƒ•ã‚¡ã‚¤ãƒ«: {self.log_file}")
        self.logger.info(f"ğŸ“Š è©³ç´°ãƒ‡ãƒ¼ã‚¿: {self.json_file}")
        self.logger.info("=" * 80)


class EnhancedSystemComplianceEvaluator(SystemComplianceEvaluator):
    """ãƒ­ã‚°å‡ºåŠ›æ©Ÿèƒ½ä»˜ãã®ã‚·ã‚¹ãƒ†ãƒ é©åˆæ€§è©•ä¾¡å™¨"""
    
    def __init__(self, workspace_path: str = "/app", llm_provider: str = "openai", 
                 llm_model: str = "gpt-4-turbo", prompt_template_style: str = "default", 
                 detailed_logger: DetailedLLMLogger = None):
        super().__init__(workspace_path, llm_provider, llm_model, prompt_template_style)
        self.detailed_logger = detailed_logger
    
    async def _evaluate_single_log_async(self, log_entry, parsed_experiment):
        """å˜ä¸€ãƒ­ã‚°ã®éåŒæœŸè©•ä¾¡ï¼ˆãƒ­ã‚°å‡ºåŠ›ä»˜ãï¼‰"""
        
        try:
            # è©•ä¾¡ç”¨ãƒ‡ãƒ¼ã‚¿ã‚’æŠ½å‡º
            evaluation_data = self.log_parser.extract_evaluation_data(parsed_experiment)
            
            # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’ç”Ÿæˆ
            prompt = self._generate_evaluation_prompt(evaluation_data)
            
            # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚­ãƒ¼ã‚’ç”Ÿæˆ
            cache_key = f"{parsed_experiment.experiment_id}_{len(parsed_experiment.turns)}"
            
            # LLMè©•ä¾¡ã‚’å®Ÿè¡Œ
            llm_response = await self.llm_manager.evaluate_system_compliance(prompt, cache_key)
            
            # è©³ç´°ãƒ­ã‚°å‡ºåŠ›
            if self.detailed_logger:
                self.detailed_logger.log_prompt_and_response(
                    log_path=str(log_entry.log_path),
                    experiment_id=parsed_experiment.experiment_id,
                    prompt=prompt,
                    response=llm_response.content,
                    usage=llm_response.usage,
                    success=llm_response.success,
                    error=llm_response.error_message
                )
            
            # è©•ä¾¡çµæœã‚’è§£æ
            parsed_evaluation = None
            if llm_response.success:
                parsed_evaluation = self.llm_manager.parse_evaluation_result(llm_response.content)
            
            # ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã‚’æŠ½å‡º
            parser_success_rate, workflow_compliance, system_health, critical_issues, recommendations = \
                self._extract_metrics_from_evaluation(parsed_evaluation)
            
            from src.evaluators.compliance_evaluator import LLMEvaluationResult
            return LLMEvaluationResult(
                log_path=str(log_entry.log_path),
                experiment_id=parsed_experiment.experiment_id,
                llm_response=llm_response,
                parsed_evaluation=parsed_evaluation,
                parser_success_rate=parser_success_rate,
                workflow_compliance=workflow_compliance,
                system_health=system_health,
                critical_issues=critical_issues,
                recommendations=recommendations
            )
            
        except Exception as e:
            if self.detailed_logger:
                self.detailed_logger.logger.error(f"âŒ LLMè©•ä¾¡ã‚¨ãƒ©ãƒ¼: {log_entry.log_path} - {e}")
            
            # ã‚¨ãƒ©ãƒ¼ã®å ´åˆã¯ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤ã§çµæœã‚’ä½œæˆ
            from src.evaluators.compliance_evaluator import LLMEvaluationResult, LLMResponse
            return LLMEvaluationResult(
                log_path=str(log_entry.log_path),
                experiment_id=parsed_experiment.experiment_id if parsed_experiment else "unknown",
                llm_response=LLMResponse("", {}, "", "", False, str(e)),
                parsed_evaluation=None,
                parser_success_rate=0.0,
                workflow_compliance=False,
                system_health="CRITICAL",
                critical_issues=[f"Evaluation error: {str(e)}"],
                recommendations=["Fix evaluation pipeline"]
            )


async def run_real_llm_evaluation(repository: str = "servantes", max_logs: int = 3, 
                                llm_provider: str = "openai", llm_model: str = "gpt-4-turbo"):
    """å®ŸLLMã§ã®è©•ä¾¡å®Ÿè¡Œ"""
    
    # è©³ç´°ãƒ­ã‚°å‡ºåŠ›ã®åˆæœŸåŒ–
    logger = DetailedLLMLogger()
    logger.log_evaluation_start(repository, max_logs, llm_provider, llm_model)
    
    try:
        # æ‹¡å¼µè©•ä¾¡å™¨ã®åˆæœŸåŒ–
        evaluator = EnhancedSystemComplianceEvaluator(
            workspace_path="/app",
            llm_provider=llm_provider,
            llm_model=llm_model,
            prompt_template_style="default",
            detailed_logger=logger
        )
        
        # è©•ä¾¡å®Ÿè¡Œ
        result = await evaluator.evaluate_single_repository(repository, max_logs=max_logs)
        
        # çµæœã®ãƒ­ã‚°å‡ºåŠ›
        logger.log_evaluation_summary(result)
        
        # çµæœã‚’ä¿å­˜
        timestamp = logger.timestamp
        output_dir = Path("/app/output/verification_results")
        output_dir.mkdir(parents=True, exist_ok=True)
        output_file = output_dir / f"real_llm_analysis_{repository}_{timestamp}.json"
        
        output_data = {
            f"Real_{llm_provider.upper()}_Analysis": result
        }
        
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(output_data, f, indent=2, ensure_ascii=False)
        
        logger.logger.info(f"ğŸ’¾ è©•ä¾¡çµæœã‚’ä¿å­˜: {output_file}")
        
        return result, logger
        
    except Exception as e:
        logger.logger.error(f"âŒ è©•ä¾¡å®Ÿè¡Œã‚¨ãƒ©ãƒ¼: {e}")
        raise
    finally:
        logger.finalize()


if __name__ == "__main__":
    import argparse
    
    parser = argparse.ArgumentParser(description="å®ŸLLMè©•ä¾¡å®Ÿè¡Œãƒ„ãƒ¼ãƒ«")
    parser.add_argument("--repo", "-r", default="servantes", help="è©•ä¾¡å¯¾è±¡ãƒªãƒã‚¸ãƒˆãƒªå")
    parser.add_argument("--max-logs", "-n", type=int, default=3, help="æœ€å¤§å‡¦ç†ãƒ­ã‚°æ•°")
    parser.add_argument("--provider", "-p", default="openai", choices=["openai", "anthropic", "mock"], help="LLMãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼")
    parser.add_argument("--model", "-m", help="LLMãƒ¢ãƒ‡ãƒ«å (ä¾‹: gpt-4.1-mini, gpt-4.1, claude-3-sonnet-20240229)")
    
    args = parser.parse_args()
    
    # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆãƒ¢ãƒ‡ãƒ«è¨­å®š
    if not args.model:
        if args.provider == "openai":
            args.model = "gpt-4.1-mini"  # ã‚³ã‚¹ãƒˆåŠ¹ç‡ã®è‰¯ã„ãƒ¢ãƒ‡ãƒ«
        elif args.provider == "anthropic":
            args.model = "claude-3-sonnet-20240229"
        elif args.provider == "mock":
            args.model = "mock-gpt"
    
    print(f"ğŸ¯ å®ŸLLMè©•ä¾¡é–‹å§‹")
    print(f"ğŸ“‚ ãƒªãƒã‚¸ãƒˆãƒª: {args.repo}")
    print(f"ğŸ“Š æœ€å¤§ãƒ­ã‚°æ•°: {args.max_logs}")
    print(f"ğŸ¤– ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼: {args.provider}")
    print(f"ğŸ§  ãƒ¢ãƒ‡ãƒ«: {args.model}")
    print()
    
    try:
        result, logger = asyncio.run(run_real_llm_evaluation(
            repository=args.repo,
            max_logs=args.max_logs,
            llm_provider=args.provider,
            llm_model=args.model
        ))
        
        print("\nâœ… å®ŸLLMè©•ä¾¡ãŒå®Œäº†ã—ã¾ã—ãŸï¼")
        print(f"ğŸ“‹ è©³ç´°ãƒ­ã‚°: {logger.log_file}")
        print(f"ğŸ“Š è©³ç´°ãƒ‡ãƒ¼ã‚¿: {logger.json_file}")
        
    except KeyboardInterrupt:
        print("\nâ¹ï¸  ãƒ¦ãƒ¼ã‚¶ãƒ¼ã«ã‚ˆã£ã¦ä¸­æ–­ã•ã‚Œã¾ã—ãŸ")
    except Exception as e:
        print(f"\nâŒ å®Ÿè¡Œã‚¨ãƒ©ãƒ¼: {e}")
        sys.exit(1)
