#!/usr/bin/env python3
"""
ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆè©•ä¾¡å“è³ªãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆå™¨
APRã‚·ã‚¹ãƒ†ãƒ ã®è©•ä¾¡çµæœã‚’è©³ç´°ã«åˆ†æã—ã€å“è³ªæŒ‡æ¨™ã‚’æä¾›
"""

import json
import argparse
from pathlib import Path
from typing import Dict, List, Any
import statistics
import matplotlib.pyplot as plt
import numpy as np

class EvaluationQualityReporter:
    """è©•ä¾¡å“è³ªãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆå™¨"""
    
    def __init__(self, dataset_report_file: str):
        with open(dataset_report_file, 'r', encoding='utf-8') as f:
            self.dataset_report = json.load(f)
    
    def generate_quality_insights(self) -> Dict[str, Any]:
        """è©•ä¾¡å“è³ªã«é–¢ã™ã‚‹æ´å¯Ÿã‚’ç”Ÿæˆ"""
        projects = self.dataset_report['project_summaries']
        
        # é©åˆæ€§ã‚¹ã‚³ã‚¢åˆ†æ
        compliance_scores = [p['avg_compliance_score'] for p in projects]
        
        # ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã‚µã‚¤ã‚ºã¨å“è³ªã®é–¢ä¿‚
        size_quality = []
        for p in projects:
            size_quality.append({
                'project': p['project_name'],
                'size': p['total_evaluations'],
                'compliance': p['avg_compliance_score'],
                'cost_per_eval': p['total_cost'] / p['total_evaluations'] if p['total_evaluations'] > 0 else 0,
                'time_per_eval': p['avg_processing_time'] / p['total_evaluations'] if p['total_evaluations'] > 0 else 0
            })
        
        # çµ±è¨ˆè¨ˆç®—
        insights = {
            'compliance_analysis': {
                'mean': statistics.mean(compliance_scores),
                'median': statistics.median(compliance_scores),
                'std_dev': statistics.stdev(compliance_scores) if len(compliance_scores) > 1 else 0,
                'min': min(compliance_scores),
                'max': max(compliance_scores),
                'range': max(compliance_scores) - min(compliance_scores)
            },
            'project_size_analysis': {
                'total_evaluations': sum(p['total_evaluations'] for p in projects),
                'avg_evaluations_per_project': statistics.mean([p['total_evaluations'] for p in projects]),
                'largest_project': max(projects, key=lambda x: x['total_evaluations']),
                'smallest_project': min(projects, key=lambda x: x['total_evaluations'])
            },
            'efficiency_analysis': {
                'avg_cost_per_evaluation': statistics.mean([sq['cost_per_eval'] for sq in size_quality]),
                'avg_time_per_evaluation': statistics.mean([sq['time_per_eval'] for sq in size_quality]),
                'most_efficient_project': min(size_quality, key=lambda x: x['cost_per_eval']),
                'least_efficient_project': max(size_quality, key=lambda x: x['cost_per_eval'])
            },
            'quality_distribution': {
                'high_quality_projects': len([p for p in projects if p['avg_compliance_score'] > 0.05]),
                'medium_quality_projects': len([p for p in projects if 0.02 <= p['avg_compliance_score'] <= 0.05]),
                'low_quality_projects': len([p for p in projects if p['avg_compliance_score'] < 0.02]),
            }
        }
        
        return insights
    
    def generate_recommendations(self, insights: Dict[str, Any]) -> List[str]:
        """æ”¹å–„æ¨å¥¨äº‹é …ã‚’ç”Ÿæˆ"""
        recommendations = []
        
        # é©åˆæ€§ã‚¹ã‚³ã‚¢ã«åŸºã¥ãæ¨å¥¨äº‹é …
        compliance_mean = insights['compliance_analysis']['mean']
        compliance_std = insights['compliance_analysis']['std_dev']
        
        if compliance_mean < 0.05:
            recommendations.append(
                f"âš ï¸  å…¨ä½“çš„ãªé©åˆæ€§ã‚¹ã‚³ã‚¢ãŒä½ã„ï¼ˆå¹³å‡{compliance_mean:.3f}ï¼‰ã€‚"
                "APRã‚·ã‚¹ãƒ†ãƒ ã®ãƒ‘ãƒ¼ã‚µãƒ¼ã¨ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼æ”¹å–„ãŒå¿…è¦ã€‚"
            )
        
        if compliance_std > 0.02:
            recommendations.append(
                f"ğŸ“Š é©åˆæ€§ã‚¹ã‚³ã‚¢ã®ã°ã‚‰ã¤ããŒå¤§ãã„ï¼ˆæ¨™æº–åå·®{compliance_std:.3f}ï¼‰ã€‚"
                "ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆé–“ã§ã®å“è³ªã®ä¸€è²«æ€§ã‚’æ”¹å–„ã™ã‚‹å¿…è¦ãŒã‚ã‚‹ã€‚"
            )
        
        # åŠ¹ç‡æ€§ã«åŸºã¥ãæ¨å¥¨äº‹é …
        avg_cost = insights['efficiency_analysis']['avg_cost_per_evaluation']
        if avg_cost > 0.5:
            recommendations.append(
                f"ğŸ’° è©•ä¾¡ã‚ãŸã‚Šã®ã‚³ã‚¹ãƒˆãŒé«˜ã„ï¼ˆå¹³å‡${avg_cost:.3f}ï¼‰ã€‚"
                "ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã®æœ€é©åŒ–ã‚„ãƒ¢ãƒ‡ãƒ«é¸æŠã®è¦‹ç›´ã—ã‚’æ¤œè¨ã€‚"
            )
        
        # ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã‚µã‚¤ã‚ºã«åŸºã¥ãæ¨å¥¨äº‹é …
        largest = insights['project_size_analysis']['largest_project']
        smallest = insights['project_size_analysis']['smallest_project']
        
        if largest['total_evaluations'] / smallest['total_evaluations'] > 10:
            recommendations.append(
                f"ğŸ“ ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã‚µã‚¤ã‚ºã®åã‚ŠãŒå¤§ãã„ï¼ˆæœ€å¤§{largest['total_evaluations']}ä»¶ vs æœ€å°{smallest['total_evaluations']}ä»¶ï¼‰ã€‚"
                "ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®ãƒãƒ©ãƒ³ã‚¹èª¿æ•´ã‚’æ¤œè¨ã€‚"
            )
        
        # å“è³ªåˆ†å¸ƒã«åŸºã¥ãæ¨å¥¨äº‹é …
        quality_dist = insights['quality_distribution']
        if quality_dist['low_quality_projects'] > quality_dist['high_quality_projects']:
            recommendations.append(
                "ğŸ¯ ä½å“è³ªãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãŒå¤šæ•°ã€‚é‡ç‚¹çš„ãªæ”¹å–„ãŒå¿…è¦ãªãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã‚’ç‰¹å®šã—ã€"
                "ãƒ‘ãƒ¼ã‚µãƒ¼ã‚„ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼ã®èª¿æ•´ã‚’è¡Œã†ã€‚"
            )
        
        return recommendations
    
    def generate_html_report(self, output_file: str) -> str:
        """HTMLå½¢å¼ã®è©³ç´°ãƒ¬ãƒãƒ¼ãƒˆã‚’ç”Ÿæˆ"""
        insights = self.generate_quality_insights()
        recommendations = self.generate_recommendations(insights)
        
        html_content = f"""
<!DOCTYPE html>
<html lang="ja">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>APRã‚·ã‚¹ãƒ†ãƒ  ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆè©•ä¾¡å“è³ªãƒ¬ãƒãƒ¼ãƒˆ</title>
    <style>
        body {{ font-family: -apple-system, BlinkMacSystemFont, sans-serif; margin: 40px; }}
        .header {{ background: #f8f9fa; padding: 20px; border-radius: 8px; margin-bottom: 30px; }}
        .metric-grid {{ display: grid; grid-template-columns: repeat(auto-fit, minmax(250px, 1fr)); gap: 20px; margin: 20px 0; }}
        .metric-card {{ background: white; border: 1px solid #e9ecef; border-radius: 8px; padding: 15px; }}
        .metric-value {{ font-size: 24px; font-weight: bold; color: #2563eb; }}
        .metric-label {{ color: #6b7280; font-size: 14px; }}
        .recommendations {{ background: #fef3c7; border-left: 4px solid #f59e0b; padding: 20px; margin: 20px 0; }}
        .project-table {{ width: 100%; border-collapse: collapse; margin: 20px 0; }}
        .project-table th, .project-table td {{ border: 1px solid #e9ecef; padding: 8px; text-align: left; }}
        .project-table th {{ background: #f8f9fa; }}
        .quality-high {{ color: #059669; font-weight: bold; }}
        .quality-medium {{ color: #d97706; font-weight: bold; }}
        .quality-low {{ color: #dc2626; font-weight: bold; }}
    </style>
</head>
<body>
    <div class="header">
        <h1>ğŸ“Š APRã‚·ã‚¹ãƒ†ãƒ  ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆè©•ä¾¡å“è³ªãƒ¬ãƒãƒ¼ãƒˆ</h1>
        <p>ã‚»ãƒƒã‚·ãƒ§ãƒ³: {self.dataset_report['session_timestamp']}</p>
        <p>ç”Ÿæˆæ—¥æ™‚: {insights['compliance_analysis']['mean']:.3f}</p>
    </div>
    
    <h2>ğŸ“ˆ å…¨ä½“çµ±è¨ˆ</h2>
    <div class="metric-grid">
        <div class="metric-card">
            <div class="metric-value">{self.dataset_report['total_projects']}</div>
            <div class="metric-label">ç·ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆæ•°</div>
        </div>
        <div class="metric-card">
            <div class="metric-value">{self.dataset_report['total_evaluations']}</div>
            <div class="metric-label">ç·è©•ä¾¡æ•°</div>
        </div>
        <div class="metric-card">
            <div class="metric-value">{insights['compliance_analysis']['mean']:.3f}</div>
            <div class="metric-label">å¹³å‡é©åˆæ€§ã‚¹ã‚³ã‚¢</div>
        </div>
        <div class="metric-card">
            <div class="metric-value">${self.dataset_report['total_cost']:.2f}</div>
            <div class="metric-label">ç·ã‚³ã‚¹ãƒˆ (USD)</div>
        </div>
    </div>
    
    <h2>ğŸ¯ é©åˆæ€§åˆ†æ</h2>
    <div class="metric-grid">
        <div class="metric-card">
            <div class="metric-value">{insights['compliance_analysis']['min']:.3f}</div>
            <div class="metric-label">æœ€ä½ã‚¹ã‚³ã‚¢</div>
        </div>
        <div class="metric-card">
            <div class="metric-value">{insights['compliance_analysis']['max']:.3f}</div>
            <div class="metric-label">æœ€é«˜ã‚¹ã‚³ã‚¢</div>
        </div>
        <div class="metric-card">
            <div class="metric-value">{insights['compliance_analysis']['std_dev']:.3f}</div>
            <div class="metric-label">æ¨™æº–åå·®</div>
        </div>
        <div class="metric-card">
            <div class="metric-value">{insights['compliance_analysis']['range']:.3f}</div>
            <div class="metric-label">ã‚¹ã‚³ã‚¢ç¯„å›²</div>
        </div>
    </div>
    
    <h2>ğŸ’¡ æ¨å¥¨äº‹é …</h2>
    <div class="recommendations">
        {"<br>".join(recommendations)}
    </div>
    
    <h2>ğŸ“‹ ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆåˆ¥è©³ç´°</h2>
    <table class="project-table">
        <thead>
            <tr>
                <th>ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆ</th>
                <th>è©•ä¾¡æ•°</th>
                <th>é©åˆæ€§ã‚¹ã‚³ã‚¢</th>
                <th>å‡¦ç†æ™‚é–“ (ç§’)</th>
                <th>ã‚³ã‚¹ãƒˆ ($)</th>
                <th>å“è³ªãƒ©ãƒ³ã‚¯</th>
            </tr>
        </thead>
        <tbody>
"""
        
        # ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆè¡Œã‚’è¿½åŠ 
        for project in self.dataset_report['project_summaries']:
            compliance = project['avg_compliance_score']
            if compliance > 0.05:
                quality_class = "quality-high"
                quality_rank = "é«˜"
            elif compliance >= 0.02:
                quality_class = "quality-medium"
                quality_rank = "ä¸­"
            else:
                quality_class = "quality-low"
                quality_rank = "ä½"
                
            html_content += f"""
            <tr>
                <td>{project['project_name']}</td>
                <td>{project['total_evaluations']}</td>
                <td class="{quality_class}">{compliance:.3f}</td>
                <td>{project['avg_processing_time']:.1f}</td>
                <td>${project['total_cost']:.2f}</td>
                <td class="{quality_class}">{quality_rank}</td>
            </tr>
"""
        
        html_content += """
        </tbody>
    </table>
</body>
</html>
"""
        
        with open(output_file, 'w', encoding='utf-8') as f:
            f.write(html_content)
        
        return output_file

def main():
    parser = argparse.ArgumentParser(description='ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆè©•ä¾¡å“è³ªãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆå™¨')
    parser.add_argument('--dataset-report', type=str, required=True, help='ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆåˆ†æãƒ¬ãƒãƒ¼ãƒˆJSONãƒ•ã‚¡ã‚¤ãƒ«')
    parser.add_argument('--output', type=str, help='å‡ºåŠ›HTMLãƒ•ã‚¡ã‚¤ãƒ«å')
    
    args = parser.parse_args()
    
    if not Path(args.dataset_report).exists():
        print(f"âŒ ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {args.dataset_report}")
        return
    
    output_file = args.output or args.dataset_report.replace('.json', '_quality_report.html')
    
    reporter = EvaluationQualityReporter(args.dataset_report)
    insights = reporter.generate_quality_insights()
    recommendations = reporter.generate_recommendations(insights)
    
    print("ğŸ” ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆè©•ä¾¡å“è³ªåˆ†æ")
    print("=" * 60)
    print(f"ğŸ“Š é©åˆæ€§ã‚¹ã‚³ã‚¢çµ±è¨ˆ:")
    print(f"   å¹³å‡: {insights['compliance_analysis']['mean']:.3f}")
    print(f"   ä¸­å¤®å€¤: {insights['compliance_analysis']['median']:.3f}")
    print(f"   æ¨™æº–åå·®: {insights['compliance_analysis']['std_dev']:.3f}")
    print(f"   ç¯„å›²: {insights['compliance_analysis']['min']:.3f} - {insights['compliance_analysis']['max']:.3f}")
    
    print(f"\nâš™ï¸  åŠ¹ç‡æ€§åˆ†æ:")
    print(f"   è©•ä¾¡ã‚ãŸã‚Šå¹³å‡ã‚³ã‚¹ãƒˆ: ${insights['efficiency_analysis']['avg_cost_per_evaluation']:.3f}")
    print(f"   è©•ä¾¡ã‚ãŸã‚Šå¹³å‡æ™‚é–“: {insights['efficiency_analysis']['avg_time_per_evaluation']:.1f}ç§’")
    
    print(f"\nğŸ¯ å“è³ªåˆ†å¸ƒ:")
    print(f"   é«˜å“è³ªãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆ: {insights['quality_distribution']['high_quality_projects']}ä»¶")
    print(f"   ä¸­å“è³ªãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆ: {insights['quality_distribution']['medium_quality_projects']}ä»¶")
    print(f"   ä½å“è³ªãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆ: {insights['quality_distribution']['low_quality_projects']}ä»¶")
    
    print(f"\nğŸ’¡ æ¨å¥¨äº‹é …:")
    for i, rec in enumerate(recommendations, 1):
        print(f"   {i}. {rec}")
    
    html_file = reporter.generate_html_report(output_file)
    print(f"\nğŸ“„ è©³ç´°HTMLãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ: {html_file}")

if __name__ == "__main__":
    main()
