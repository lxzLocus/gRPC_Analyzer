#!/usr/bin/env python3
"""
è©•ä¾¡çµæœãƒ­ã‚°ãƒ“ãƒ¥ãƒ¼ãƒ¯ãƒ¼
LLMã«ã‚ˆã‚‹è©•ä¾¡çµæœã®è©³ç´°è¡¨ç¤ºã¨åˆ†ææ©Ÿèƒ½
"""

import json
import sys
from pathlib import Path
from datetime import datetime
from typing import Dict, List, Any, Optional
import argparse

# scripts/ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‹ã‚‰å®Ÿè¡Œã•ã‚Œã‚‹å ´åˆã®ãƒ‘ã‚¹è¨­å®š
sys.path.append('/app')
sys.path.append('/app/src')


class EvaluationLogViewer:
    """è©•ä¾¡çµæœãƒ­ã‚°ãƒ“ãƒ¥ãƒ¼ãƒ¯ãƒ¼"""
    
    def __init__(self, workspace_path: str = "/app"):
        self.workspace_path = Path(workspace_path)
        
        # å„ç¨®ãƒ­ã‚°ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª
        self.log_locations = {
            "verification": self.workspace_path / "output" / "verification_results",
            "logs": self.workspace_path / "logs", 
            "llm_evaluation": self.workspace_path / "logs",  # ãƒ­ã‚°å ´æ‰€ã‚’çµ±ä¸€
            "apr_output": self.workspace_path / "apr-output",
            "results": self.workspace_path / "results"
        }
    
    def list_all_evaluation_files(self) -> Dict[str, List[Path]]:
        """å…¨ã¦ã®è©•ä¾¡çµæœãƒ•ã‚¡ã‚¤ãƒ«ä¸€è¦§ã‚’å–å¾—"""
        all_files = {}
        
        for location_name, location_path in self.log_locations.items():
            if location_path.exists():
                json_files = list(location_path.glob("*.json"))
                json_files.sort(key=lambda x: x.stat().st_mtime, reverse=True)  # æ–°ã—ã„é †
                all_files[location_name] = json_files
            else:
                all_files[location_name] = []
        
        return all_files
    
    def display_file_summary(self):
        """ãƒ•ã‚¡ã‚¤ãƒ«ä¸€è¦§ã®æ¦‚è¦ã‚’è¡¨ç¤º"""
        print("ğŸ“Š è©•ä¾¡çµæœãƒ­ã‚°ãƒ•ã‚¡ã‚¤ãƒ«ä¸€è¦§")
        print("=" * 60)
        
        all_files = self.list_all_evaluation_files()
        total_files = sum(len(files) for files in all_files.values())
        
        print(f"ğŸ—‚ï¸  ç·ãƒ•ã‚¡ã‚¤ãƒ«æ•°: {total_files}ä»¶")
        print()
        
        for location_name, files in all_files.items():
            location_path = self.log_locations[location_name]
            description = {
                "verification": "å˜ä¸€ãƒªãƒã‚¸ãƒˆãƒªè©•ä¾¡çµæœ",
                "logs": "LLMè©•ä¾¡ãƒ­ã‚°", 
                "llm_evaluation": "è©³ç´°LLMè©•ä¾¡ãƒ­ã‚°ï¼ˆæ–°ï¼‰",
                "apr_output": "APRå‡¦ç†çµæœ",
                "results": "ãã®ä»–ã®çµæœ"
            }.get(location_name, location_name)
            
            print(f"ğŸ“ {location_path}")
            print(f"   {description}")
            
            if files:
                for file in files[:5]:  # æœ€æ–°5ä»¶
                    size_kb = file.stat().st_size / 1024
                    mtime = datetime.fromtimestamp(file.stat().st_mtime)
                    print(f"   - {file.name} ({size_kb:.1f}KB, {mtime.strftime('%m-%d %H:%M')})")
                if len(files) > 5:
                    print(f"   ... ä»–{len(files) - 5}ä»¶")
            else:
                print("   - (ç©º)")
            print()
    
    def load_evaluation_file(self, file_path: Path) -> Optional[Dict[str, Any]]:
        """è©•ä¾¡ãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã¿"""
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                return json.load(f)
        except Exception as e:
            print(f"âŒ ãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼ {file_path.name}: {e}")
            return None
    
    def display_detailed_evaluation(self, file_path: Path, show_llm_details: bool = True):
        """è©³ç´°ãªè©•ä¾¡å†…å®¹ã‚’è¡¨ç¤º"""
        data = self.load_evaluation_file(file_path)
        if not data:
            return
        
        print(f"ğŸ” è©³ç´°è©•ä¾¡çµæœ: {file_path.name}")
        print("=" * 60)
        
        # Mock_LLMã€Fresh_MockLLM_Analysisã€Real_XXX_Analysisã€ã¾ãŸã¯ãƒ¡ã‚¤ãƒ³ãƒ‡ãƒ¼ã‚¿ã‚’æ¢ã™
        possible_keys = ["Mock_LLM", "Fresh_MockLLM_Analysis"] + \
                       [f"Real_{provider.upper()}_Analysis" for provider in ["OPENAI", "ANTHROPIC", "MOCK"]]
        
        evaluation_data = data
        for key in possible_keys:
            if key in data:
                evaluation_data = data[key]
                break
        
        if "repository_name" in evaluation_data:
            # åŸºæœ¬æƒ…å ±
            print(f"ğŸ“‚ ãƒªãƒã‚¸ãƒˆãƒª: {evaluation_data.get('repository_name', 'N/A')}")
            print(f"ğŸ“… è©•ä¾¡æ—¥æ™‚: {evaluation_data.get('timestamp', 'N/A')}")
            print(f"â±ï¸  å‡¦ç†æ™‚é–“: {evaluation_data.get('processing_time_seconds', 0):.2f}ç§’")
            print(f"ğŸ¤– LLMãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼: {evaluation_data.get('llm_provider', 'N/A')}")
            print(f"ğŸ§  LLMãƒ¢ãƒ‡ãƒ«: {evaluation_data.get('llm_model', 'N/A')}")
            print()
            
            # è©•ä¾¡ã‚µãƒãƒªãƒ¼
            summary = evaluation_data.get("summary", {})
            print("ğŸ“Š è©•ä¾¡ã‚µãƒãƒªãƒ¼:")
            print(f"  - å‡¦ç†ãƒ­ã‚°æ•°: {summary.get('total_logs_processed', 0)}")
            print(f"  - LLMæˆåŠŸç‡: {summary.get('successful_evaluations', 0)}/{summary.get('total_logs_processed', 0)}")
            print(f"  - å¤±æ•—è©•ä¾¡: {summary.get('failed_evaluations', 0)}")
            print(f"  - ãƒ‘ãƒ¼ã‚µãƒ¼å•é¡Œæ¤œå‡º: {summary.get('parser_issues_detected', 0)}")
            print(f"  - ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼é•å: {summary.get('workflow_violations', 0)}")
            print(f"  - é‡å¤§ã‚·ã‚¹ãƒ†ãƒ : {summary.get('critical_systems', 0)}")
            print()
            
            # é©åˆæ€§ãƒ¡ãƒˆãƒªã‚¯ã‚¹
            metrics = evaluation_data.get("compliance_metrics", {})
            print("ğŸ¯ é©åˆæ€§ãƒ¡ãƒˆãƒªã‚¯ã‚¹:")
            print(f"  - åˆ¶å¾¡ãƒ•ãƒ­ãƒ¼ç²¾åº¦: {metrics.get('control_flow_accuracy', 0):.3f}")
            print(f"  - ãƒ‘ãƒ¼ã‚µãƒ¼æˆåŠŸç‡: {metrics.get('parser_success_rate', 0):.3f}")
            print(f"  - ãƒ•ã‚¡ã‚¤ãƒ«å‡¦ç†ç‡: {metrics.get('file_processing_rate', 0):.3f}")
            print(f"  - ã‚¨ãƒ©ãƒ¼å‡¦ç†ã‚¹ã‚³ã‚¢: {metrics.get('error_handling_score', 0):.3f}")
            print(f"  - ç·åˆé©åˆæ€§: {metrics.get('overall_compliance', 0):.3f}")
            print()
        
        # è©³ç´°çµæœ
        detailed_results = evaluation_data.get("detailed_results", [])
        if detailed_results and show_llm_details:
            print(f"ğŸ” å€‹åˆ¥ãƒ­ã‚°ã®è©³ç´°è©•ä¾¡ ({len(detailed_results)}ä»¶):")
            print("-" * 60)
            
            for i, result in enumerate(detailed_results, 1):
                print(f"\\n--- ãƒ­ã‚° {i}: {Path(result.get('log_path', '')).name} ---")
                print(f"ğŸ†” å®Ÿé¨“ID: {result.get('experiment_id', 'N/A')}")
                print(f"âœ… LLMè©•ä¾¡æˆåŠŸ: {result.get('llm_success', False)}")
                
                if result.get('llm_error'):
                    print(f"âŒ LLMã‚¨ãƒ©ãƒ¼: {result['llm_error']}")
                
                print(f"ğŸ“ˆ ãƒ‘ãƒ¼ã‚µãƒ¼æˆåŠŸç‡: {result.get('parser_success_rate', 0):.1%}")
                print(f"ğŸ”„ ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼é©åˆ: {result.get('workflow_compliance', False)}")
                print(f"â¤ï¸  ã‚·ã‚¹ãƒ†ãƒ å¥å…¨æ€§: {result.get('system_health', 'N/A')}")
                
                # é‡å¤§å•é¡Œ
                critical_issues = result.get("critical_issues", [])
                if critical_issues:
                    print(f"âš ï¸  é‡å¤§å•é¡Œ:")
                    for issue in critical_issues:
                        print(f"    - {issue}")
                
                # æ¨å¥¨äº‹é …
                recommendations = result.get("recommendations", [])
                if recommendations:
                    print(f"ğŸ’¡ æ¨å¥¨äº‹é …:")
                    for rec in recommendations:
                        print(f"    - {rec}")
                
                # LLMä½¿ç”¨é‡
                usage = result.get("llm_usage", {})
                if usage:
                    total_tokens = usage.get("total_tokens", 0)
                    prompt_tokens = usage.get("prompt_tokens", 0)
                    completion_tokens = usage.get("completion_tokens", 0)
                    print(f"ğŸ§® LLMä½¿ç”¨é‡: {total_tokens}ãƒˆãƒ¼ã‚¯ãƒ³ (å…¥åŠ›:{prompt_tokens}, å‡ºåŠ›:{completion_tokens})")
    
    def find_latest_evaluation(self, repository_name: str = None) -> Optional[Path]:
        """æœ€æ–°ã®è©•ä¾¡çµæœãƒ•ã‚¡ã‚¤ãƒ«ã‚’å–å¾—"""
        all_files = self.list_all_evaluation_files()
        
        # verification_resultsã‹ã‚‰å„ªå…ˆçš„ã«æ¢ã™
        verification_files = all_files.get("verification", [])
        
        if repository_name:
            # ç‰¹å®šã®ãƒªãƒã‚¸ãƒˆãƒªã®çµæœã‚’æ¢ã™
            matching_files = [f for f in verification_files if repository_name.lower() in f.name.lower()]
            if matching_files:
                return matching_files[0]  # æœ€æ–°
        
        # ãƒªãƒã‚¸ãƒˆãƒªæŒ‡å®šãªã—ã¾ãŸã¯è¦‹ã¤ã‹ã‚‰ãªã„å ´åˆã¯æœ€æ–°ã‚’è¿”ã™
        for location_files in all_files.values():
            if location_files:
                return location_files[0]  # æœ€æ–°
        
        return None
    
    def compare_evaluations(self, file1: Path, file2: Path):
        """2ã¤ã®è©•ä¾¡çµæœã‚’æ¯”è¼ƒ"""
        data1 = self.load_evaluation_file(file1)
        data2 = self.load_evaluation_file(file2)
        
        if not data1 or not data2:
            return
        
        print(f"âš–ï¸  è©•ä¾¡çµæœæ¯”è¼ƒ")
        print("=" * 60)
        print(f"ğŸ“„ ãƒ•ã‚¡ã‚¤ãƒ«1: {file1.name}")
        print(f"ğŸ“„ ãƒ•ã‚¡ã‚¤ãƒ«2: {file2.name}")
        print()
        
        # åŸºæœ¬æƒ…å ±ã®æ¯”è¼ƒ
        eval1 = data1.get("Mock_LLM", data1)
        eval2 = data2.get("Mock_LLM", data2)
        
        metrics1 = eval1.get("compliance_metrics", {})
        metrics2 = eval2.get("compliance_metrics", {})
        
        print("ğŸ¯ é©åˆæ€§ãƒ¡ãƒˆãƒªã‚¯ã‚¹æ¯”è¼ƒ:")
        metric_names = [
            ("overall_compliance", "ç·åˆé©åˆæ€§"),
            ("parser_success_rate", "ãƒ‘ãƒ¼ã‚µãƒ¼æˆåŠŸç‡"), 
            ("control_flow_accuracy", "åˆ¶å¾¡ãƒ•ãƒ­ãƒ¼ç²¾åº¦"),
            ("error_handling_score", "ã‚¨ãƒ©ãƒ¼å‡¦ç†ã‚¹ã‚³ã‚¢")
        ]
        
        for metric_key, metric_name in metric_names:
            val1 = metrics1.get(metric_key, 0)
            val2 = metrics2.get(metric_key, 0)
            diff = val2 - val1
            
            if diff > 0:
                trend = "ğŸ“ˆ"
            elif diff < 0:
                trend = "ğŸ“‰"  
            else:
                trend = "â¡ï¸"                python src/cli/real_llm_evaluator.py --repo boulder --max-logs 1 --provider mock --model gpt-5
            print(f"  {metric_name}: {val1:.3f} â†’ {val2:.3f} {trend} ({diff:+.3f})")


def main():
    """ãƒ¡ã‚¤ãƒ³é–¢æ•°"""
    parser = argparse.ArgumentParser(description="è©•ä¾¡çµæœãƒ­ã‚°ãƒ“ãƒ¥ãƒ¼ãƒ¯ãƒ¼")
    parser.add_argument("--file", "-f", help="è¡¨ç¤ºã™ã‚‹è©•ä¾¡ãƒ•ã‚¡ã‚¤ãƒ«å")
    parser.add_argument("--repo", "-r", help="ç‰¹å®šã®ãƒªãƒã‚¸ãƒˆãƒªã®è©•ä¾¡çµæœã‚’è¡¨ç¤º")
    parser.add_argument("--latest", "-l", action="store_true", help="æœ€æ–°ã®è©•ä¾¡çµæœã‚’è¡¨ç¤º")
    parser.add_argument("--summary", "-s", action="store_true", help="ãƒ•ã‚¡ã‚¤ãƒ«ä¸€è¦§ã®ã¿è¡¨ç¤º")
    parser.add_argument("--compare", "-c", nargs=2, help="2ã¤ã®ãƒ•ã‚¡ã‚¤ãƒ«ã‚’æ¯”è¼ƒ")
    
    args = parser.parse_args()
    
    viewer = EvaluationLogViewer()
    
    if args.summary:
        viewer.display_file_summary()
        return
    
    if args.compare:
        file1 = Path("/app") / args.compare[0]
        file2 = Path("/app") / args.compare[1]
        viewer.compare_evaluations(file1, file2)
        return
    
    if args.file:
        # æŒ‡å®šãƒ•ã‚¡ã‚¤ãƒ«ã‚’è¡¨ç¤º
        file_path = None
        for location_path in viewer.log_locations.values():
            candidate = location_path / args.file
            if candidate.exists():
                file_path = candidate
                break
        
        if file_path:
            viewer.display_detailed_evaluation(file_path)
        else:
            print(f"âŒ ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {args.file}")
            viewer.display_file_summary()
    
    elif args.repo or args.latest:
        # æœ€æ–°ã¾ãŸã¯ç‰¹å®šãƒªãƒã‚¸ãƒˆãƒªã®è©•ä¾¡çµæœã‚’è¡¨ç¤º
        file_path = viewer.find_latest_evaluation(args.repo)
        if file_path:
            viewer.display_detailed_evaluation(file_path)
        else:
            print("âŒ è©•ä¾¡çµæœãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
            viewer.display_file_summary()
    
    else:
        # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: ãƒ•ã‚¡ã‚¤ãƒ«ä¸€è¦§ã‚’è¡¨ç¤º
        viewer.display_file_summary()


if __name__ == "__main__":
    main()
