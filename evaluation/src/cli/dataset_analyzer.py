#!/usr/bin/env python3
"""
ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆå…¨ä½“è©•ä¾¡ã‚¢ãƒŠãƒ©ã‚¤ã‚¶ãƒ¼
å€‹åˆ¥ã®ãƒ—ãƒ«ãƒªã‚¯è©•ä¾¡çµæœã‚’çµ±åˆã—ã¦ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆå…¨ä½“ã®åˆ†æã‚’è¡Œã†
"""

import json
import os
import sys
import argparse
import glob
from pathlib import Path
from typing import Dict, List, Any, Optional
from dataclasses import dataclass, asdict
from datetime import datetime
import statistics

@dataclass
class ProjectSummary:
    """ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆåˆ¥ã‚µãƒãƒªãƒ¼"""
    project_name: str
    total_evaluations: int
    successful_evaluations: int
    failed_evaluations: int
    success_rate: float
    avg_compliance_score: float
    avg_processing_time: float
    total_cost: float
    issues_count: int
    pullrequests_count: int

@dataclass
class DatasetSummary:
    """ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆå…¨ä½“ã‚µãƒãƒªãƒ¼"""
    session_timestamp: str
    total_projects: int
    total_evaluations: int
    overall_success_rate: float
    avg_compliance_score: float
    total_processing_time: float
    total_cost: float
    project_summaries: List[ProjectSummary]
    
class DatasetAnalyzer:
    """ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆå…¨ä½“è©•ä¾¡ã‚¢ãƒŠãƒ©ã‚¤ã‚¶ãƒ¼"""
    
    def __init__(self, session_directory: str):
        self.session_dir = Path(session_directory)
        self.results_dir = self.session_dir / "results"
        self.summaries_dir = self.session_dir / "summaries"
        
    def analyze_project(self, project_name: str) -> Optional[ProjectSummary]:
        """å€‹åˆ¥ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã®åˆ†æ"""
        print(f"ğŸ” ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆåˆ†æ: {project_name}")
        
        project_dir = self.results_dir / project_name
        if not project_dir.exists():
            print(f"âš ï¸  ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªãŒå­˜åœ¨ã—ã¾ã›ã‚“: {project_dir}")
            return None
            
        # ãƒ¡ã‚¤ãƒ³è©•ä¾¡çµæœãƒ•ã‚¡ã‚¤ãƒ«ã‚’æ¢ã™
        main_result_files = list(project_dir.glob("evaluation_result_*.json"))
        if not main_result_files:
            print(f"âš ï¸  ãƒ¡ã‚¤ãƒ³è©•ä¾¡çµæœãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {project_dir}")
            return None
            
        # æœ€æ–°ã®è©•ä¾¡çµæœã‚’ä½¿ç”¨
        main_result_file = max(main_result_files, key=lambda x: x.stat().st_mtime)
        print(f"   ğŸ“„ ãƒ¡ã‚¤ãƒ³ãƒ•ã‚¡ã‚¤ãƒ«: {main_result_file.name}")
        
        try:
            with open(main_result_file, 'r', encoding='utf-8') as f:
                main_result = json.load(f)
            
            # Real_OPENAI_Analysisã‚­ãƒ¼ã®ä¸‹ã«ãƒ‡ãƒ¼ã‚¿ãŒãƒã‚¹ãƒˆã•ã‚Œã¦ã„ã‚‹å ´åˆ
            if 'Real_OPENAI_Analysis' in main_result:
                main_result = main_result['Real_OPENAI_Analysis']
                
            # å€‹åˆ¥è©•ä¾¡çµæœã‚‚åé›†
            individual_results = []
            
            # issueãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª
            issues_dir = project_dir / "issue"
            if issues_dir.exists():
                for issue_dir in issues_dir.iterdir():
                    if issue_dir.is_dir():
                        result_file = issue_dir / "evaluation_result.json"
                        if result_file.exists():
                            with open(result_file, 'r', encoding='utf-8') as f:
                                individual_results.append(json.load(f))
                                
            # pullrequestãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª
            pr_dir = project_dir / "pullrequest"
            if pr_dir.exists():
                for pr_dir_item in pr_dir.iterdir():
                    if pr_dir_item.is_dir():
                        result_file = pr_dir_item / "evaluation_result.json"
                        if result_file.exists():
                            with open(result_file, 'r', encoding='utf-8') as f:
                                individual_results.append(json.load(f))
            
            # çµ±è¨ˆè¨ˆç®—
            compliance_scores = []
            processing_times = []
            costs = []
            
            # ãƒ¡ã‚¤ãƒ³çµæœã‹ã‚‰çµ±è¨ˆå–å¾—
            if 'compliance_metrics' in main_result:
                if 'overall_compliance' in main_result['compliance_metrics']:
                    compliance_scores.append(main_result['compliance_metrics']['overall_compliance'])
                    
            if 'processing_time_seconds' in main_result:
                processing_times.append(main_result['processing_time_seconds'])
            
            # è©³ç´°çµæœã‹ã‚‰ã‚³ã‚¹ãƒˆæƒ…å ±ã‚’åé›†ï¼ˆã‚‚ã—ã‚ã‚‹å ´åˆï¼‰
            if 'detailed_results' in main_result:
                for detail in main_result['detailed_results']:
                    if 'llm_usage' in detail and detail['llm_usage']:
                        # OpenAI APIæ–™é‡‘è¨ˆç®—ï¼ˆæ¦‚ç®—ï¼‰
                        usage = detail['llm_usage']
                        if 'total_tokens' in usage:
                            # GPT-5ã®æ¦‚ç®—æ–™é‡‘ï¼ˆ$0.05/1000ãƒˆãƒ¼ã‚¯ãƒ³ï¼‰
                            estimated_cost = usage['total_tokens'] * 0.00005
                            costs.append(estimated_cost)
                
            # å€‹åˆ¥çµæœã‹ã‚‰ã‚‚çµ±è¨ˆå–å¾—
            for result in individual_results:
                # å€‹åˆ¥çµæœã¯ evaluation_result ã‚­ãƒ¼ã®ä¸‹ã«ãƒã‚¹ãƒˆã•ã‚Œã¦ã„ã‚‹
                eval_data = result.get('evaluation_result', result)
                
                if 'compliance_score' in eval_data:
                    compliance_scores.append(eval_data['compliance_score'])
                elif 'overall_compliance' in eval_data:
                    compliance_scores.append(eval_data['overall_compliance'])
                    
                if 'processing_time' in eval_data:
                    processing_times.append(eval_data['processing_time'])
                elif 'processing_time_seconds' in eval_data:
                    processing_times.append(eval_data['processing_time_seconds'])
                    
                if 'cost' in eval_data:
                    costs.append(eval_data['cost'])
                elif 'llm_usage' in eval_data and eval_data['llm_usage']:
                    usage = eval_data['llm_usage']
                    if 'total_tokens' in usage:
                        estimated_cost = usage['total_tokens'] * 0.00005
                        costs.append(estimated_cost)
            
            # issue/PRã‚«ã‚¦ãƒ³ãƒˆ
            issues_count = len(list((project_dir / "issue").iterdir())) if (project_dir / "issue").exists() else 0
            pr_count = len(list((project_dir / "pullrequest").iterdir())) if (project_dir / "pullrequest").exists() else 0
            total_evaluations = len(individual_results)
            
            print(f"   ğŸ“Š çµ±è¨ˆ: compliance_scores={len(compliance_scores)}, processing_times={len(processing_times)}, costs={len(costs)}")
            print(f"   ğŸ“ˆ é©åˆæ€§ã‚¹ã‚³ã‚¢ç¯„å›²: {min(compliance_scores) if compliance_scores else 'N/A'} - {max(compliance_scores) if compliance_scores else 'N/A'}")
            
            return ProjectSummary(
                project_name=project_name,
                total_evaluations=total_evaluations,
                successful_evaluations=total_evaluations,  # å…¨ã¦æˆåŠŸã¨ä»®å®šï¼ˆå¤±æ•—ãƒ•ã‚¡ã‚¤ãƒ«ãŒã‚ã‚Œã°åˆ¥é€”å‡¦ç†ï¼‰
                failed_evaluations=0,
                success_rate=100.0 if total_evaluations > 0 else 0.0,
                avg_compliance_score=statistics.mean(compliance_scores) if compliance_scores else 0.0,
                avg_processing_time=statistics.mean(processing_times) if processing_times else 0.0,
                total_cost=sum(costs) if costs else 0.0,
                issues_count=issues_count,
                pullrequests_count=pr_count
            )
            
        except Exception as e:
            print(f"âŒ ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆåˆ†æã‚¨ãƒ©ãƒ¼ {project_name}: {e}")
            return None
    
    def analyze_dataset(self) -> DatasetSummary:
        """ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆå…¨ä½“ã®åˆ†æ"""
        print("ğŸ” ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆå…¨ä½“åˆ†æé–‹å§‹")
        print("=" * 60)
        
        # ã‚»ãƒƒã‚·ãƒ§ãƒ³ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—å–å¾—
        session_timestamp = self.session_dir.name.replace("full_evaluation_", "")
        
        # ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªä¸€è¦§å–å¾—
        project_dirs = [d for d in self.results_dir.iterdir() if d.is_dir()]
        project_summaries = []
        
        for project_dir in project_dirs:
            project_summary = self.analyze_project(project_dir.name)
            if project_summary:
                project_summaries.append(project_summary)
        
        # å…¨ä½“çµ±è¨ˆè¨ˆç®—
        if project_summaries:
            total_evaluations = sum(ps.total_evaluations for ps in project_summaries)
            total_successful = sum(ps.successful_evaluations for ps in project_summaries)
            overall_success_rate = (total_successful / total_evaluations * 100) if total_evaluations > 0 else 0.0
            
            compliance_scores = [ps.avg_compliance_score for ps in project_summaries if ps.avg_compliance_score > 0]
            avg_compliance_score = statistics.mean(compliance_scores) if compliance_scores else 0.0
            
            total_processing_time = sum(ps.avg_processing_time for ps in project_summaries)
            total_cost = sum(ps.total_cost for ps in project_summaries)
        else:
            total_evaluations = 0
            overall_success_rate = 0.0
            avg_compliance_score = 0.0
            total_processing_time = 0.0
            total_cost = 0.0
        
        return DatasetSummary(
            session_timestamp=session_timestamp,
            total_projects=len(project_summaries),
            total_evaluations=total_evaluations,
            overall_success_rate=overall_success_rate,
            avg_compliance_score=avg_compliance_score,
            total_processing_time=total_processing_time,
            total_cost=total_cost,
            project_summaries=project_summaries
        )
    
    def generate_report(self, output_file: Optional[str] = None) -> str:
        """è©³ç´°ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ"""
        dataset_summary = self.analyze_dataset()
        
        if output_file is None:
            output_file = str(self.session_dir / "dataset_analysis_report.json")
        
        # JSONå½¢å¼ã§ä¿å­˜
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(asdict(dataset_summary), f, indent=2, ensure_ascii=False)
        
        # ã‚³ãƒ³ã‚½ãƒ¼ãƒ«å‡ºåŠ›
        print("\n" + "=" * 80)
        print("ğŸ“Š ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆå…¨ä½“è©•ä¾¡ãƒ¬ãƒãƒ¼ãƒˆ")
        print("=" * 80)
        print(f"ğŸ•’ ã‚»ãƒƒã‚·ãƒ§ãƒ³: {dataset_summary.session_timestamp}")
        print(f"ğŸ“‚ ç·ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆæ•°: {dataset_summary.total_projects}")
        print(f"ğŸ“ ç·è©•ä¾¡æ•°: {dataset_summary.total_evaluations}")
        print(f"âœ… æˆåŠŸç‡: {dataset_summary.overall_success_rate:.1f}%")
        print(f"ğŸ¯ å¹³å‡é©åˆæ€§ã‚¹ã‚³ã‚¢: {dataset_summary.avg_compliance_score:.3f}")
        print(f"â±ï¸  ç·å‡¦ç†æ™‚é–“: {dataset_summary.total_processing_time:.1f}ç§’")
        print(f"ğŸ’° ç·ã‚³ã‚¹ãƒˆ: ${dataset_summary.total_cost:.4f} USD")
        
        print("\nğŸ“‹ ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆåˆ¥è©³ç´°:")
        print("-" * 80)
        for ps in dataset_summary.project_summaries:
            print(f"ğŸ“ {ps.project_name}:")
            print(f"   ğŸ“Š è©•ä¾¡æ•°: {ps.total_evaluations} (Issues: {ps.issues_count}, PRs: {ps.pullrequests_count})")
            print(f"   âœ… æˆåŠŸç‡: {ps.success_rate:.1f}%")
            print(f"   ğŸ¯ é©åˆæ€§: {ps.avg_compliance_score:.3f}")
            print(f"   â±ï¸  æ™‚é–“: {ps.avg_processing_time:.1f}s")
            print(f"   ğŸ’° ã‚³ã‚¹ãƒˆ: ${ps.total_cost:.4f}")
            print()
        
        print(f"ğŸ“„ è©³ç´°ãƒ¬ãƒãƒ¼ãƒˆä¿å­˜: {output_file}")
        return output_file

def main():
    parser = argparse.ArgumentParser(description='ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆå…¨ä½“è©•ä¾¡ã‚¢ãƒŠãƒ©ã‚¤ã‚¶ãƒ¼')
    parser.add_argument('--session-dir', type=str, help='è©•ä¾¡ã‚»ãƒƒã‚·ãƒ§ãƒ³ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª')
    parser.add_argument('--latest', action='store_true', help='æœ€æ–°ã®ã‚»ãƒƒã‚·ãƒ§ãƒ³ã‚’ä½¿ç”¨')
    parser.add_argument('--output', type=str, help='å‡ºåŠ›ãƒ•ã‚¡ã‚¤ãƒ«å')
    
    args = parser.parse_args()
    
    if args.latest:
        # æœ€æ–°ã®ã‚»ãƒƒã‚·ãƒ§ãƒ³ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’æ¢ã™
        logs_dir = Path("/app/logs")
        session_dirs = list(logs_dir.glob("full_evaluation_*"))
        if not session_dirs:
            print("âŒ è©•ä¾¡ã‚»ãƒƒã‚·ãƒ§ãƒ³ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
            sys.exit(1)
        session_dir = max(session_dirs, key=lambda x: x.stat().st_mtime)
    elif args.session_dir:
        session_dir = Path(args.session_dir)
    else:
        print("âŒ --session-dir ã¾ãŸã¯ --latest ã‚’æŒ‡å®šã—ã¦ãã ã•ã„")
        sys.exit(1)
    
    if not session_dir.exists():
        print(f"âŒ ã‚»ãƒƒã‚·ãƒ§ãƒ³ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªãŒå­˜åœ¨ã—ã¾ã›ã‚“: {session_dir}")
        sys.exit(1)
    
    analyzer = DatasetAnalyzer(str(session_dir))
    report_file = analyzer.generate_report(args.output)
    
    print(f"\nğŸ‰ ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆåˆ†æå®Œäº†ï¼")
    print(f"ğŸ“„ ãƒ¬ãƒãƒ¼ãƒˆãƒ•ã‚¡ã‚¤ãƒ«: {report_file}")

if __name__ == "__main__":
    main()
