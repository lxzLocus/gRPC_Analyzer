#!/usr/bin/env python3
"""
APRå…¨ä½“è©•ä¾¡åˆ†æã‚·ã‚¹ãƒ†ãƒ 
ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆå…¨ä½“ã®åº¦æ•°åˆ†å¸ƒã¨ã‚«ãƒ†ã‚´ãƒªåˆ¥è©•ä¾¡ã‚’å®Ÿè£…

1. åº¦æ•°åˆ†å¸ƒåˆ†æï¼šè©•ä¾¡ã‚°ãƒ¬ãƒ¼ãƒ‰ã€ã‚·ã‚¹ãƒ†ãƒ å¥å…¨æ€§ã€é©åˆæ€§ã‚¹ã‚³ã‚¢ã®åˆ†å¸ƒ
2. ã‚«ãƒ†ã‚´ãƒªåˆ¥åˆ†æï¼šä¿®æ­£ã‚¿ã‚¹ã‚¯ã®ç¨®é¡åˆ¥ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹è©•ä¾¡
"""

import json
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import numpy as np
from pathlib import Path
from typing import Dict, List, Any, Tuple
from dataclasses import dataclass
from collections import defaultdict, Counter
import logging


@dataclass
class EvaluationRecord:
    """å€‹åˆ¥è©•ä¾¡è¨˜éŒ²"""
    experiment_id: str
    project: str
    task_type: str  # issue/pullrequest
    task_title: str
    overall_compliance: float
    system_health: str
    parser_success_rate: float
    workflow_compliance: bool
    critical_issues: List[str]
    recommendations: List[str]
    category: str = "æœªåˆ†é¡"  # å¾Œã§ã‚«ãƒ†ã‚´ãƒªåˆ†é¡ã‚’è¿½åŠ 


@dataclass
class DistributionAnalysis:
    """åº¦æ•°åˆ†å¸ƒåˆ†æçµæœ"""
    grade_distribution: Dict[str, int]
    health_distribution: Dict[str, int]
    score_statistics: Dict[str, float]
    score_histogram_data: Tuple[np.ndarray, np.ndarray]


@dataclass
class CategoryAnalysis:
    """ã‚«ãƒ†ã‚´ãƒªåˆ¥åˆ†æçµæœ"""
    category_stats: Dict[str, Dict[str, Any]]
    success_rates: Dict[str, float]
    average_scores: Dict[str, float]
    health_breakdown: Dict[str, Dict[str, int]]


class APRDatasetAnalyzer:
    """APRãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆå…¨ä½“åˆ†æå™¨"""
    
    def __init__(self, session_path: str):
        self.session_path = Path(session_path)
        self.results_path = self.session_path / "results"
        self.records: List[EvaluationRecord] = []
        
        # è©•ä¾¡ã‚°ãƒ¬ãƒ¼ãƒ‰å®šç¾©ï¼ˆEVALUATION_PROCESS_GUIDE.mdã«åŸºã¥ãï¼‰
        self.grade_thresholds = {
            'A+': 0.95,
            'A': 0.90,
            'B': 0.80,
            'C': 0.70,
            'D': 0.60,
            'F': 0.0
        }
        
        # ã‚«ãƒ†ã‚´ãƒªåˆ†é¡å®šç¾©ï¼ˆæ‹¡å¼µå¯èƒ½ï¼‰
        self.category_keywords = {
            'ãƒªãƒ•ã‚¡ã‚¯ã‚¿ãƒªãƒ³ã‚°': ['refactor', 'rename', 'restructure', 'reorganize', 'move', 'extract'],
            'ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æ”¹å–„': ['performance', 'optimize', 'cache', 'index', 'speed', 'efficient'],
            'æ©Ÿèƒ½è¿½åŠ ': ['add', 'implement', 'feature', 'endpoint', 'api', 'support'],
            'ãƒ†ã‚¹ãƒˆã‚³ãƒ¼ãƒ‰': ['test', 'unittest', 'integration', 'coverage', 'mock'],
            'ãƒã‚°ä¿®æ­£': ['fix', 'bug', 'error', 'issue', 'correct', 'resolve'],
            'å˜ç´”ä¿®æ­£': ['typo', 'license', 'header', 'format', 'style', 'comment'],
            'ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆ': ['doc', 'readme', 'documentation', 'comment', 'example'],
            'ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£': ['security', 'auth', 'permission', 'vulnerability', 'secure'],
            'ä¾å­˜é–¢ä¿‚': ['dependency', 'update', 'version', 'upgrade', 'package'],
            'è¨­å®šãƒ»ç’°å¢ƒ': ['config', 'environment', 'setup', 'install', 'deploy']
        }
        
        # ãƒ­ã‚°è¨­å®š
        logging.basicConfig(level=logging.INFO)
        self.logger = logging.getLogger(__name__)

    def load_evaluation_data(self) -> None:
        """è©•ä¾¡ãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã¿"""
        self.logger.info(f"ğŸ” è©•ä¾¡ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿é–‹å§‹: {self.session_path}")
        
        total_loaded = 0
        
        # å„ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã®çµæœã‚’èª­ã¿è¾¼ã¿
        for project_dir in self.results_path.iterdir():
            if not project_dir.is_dir():
                continue
                
            project_name = project_dir.name
            result_files = list(project_dir.glob("evaluation_result_*.json"))
            
            if not result_files:
                self.logger.warning(f"âš ï¸ {project_name}: è©•ä¾¡çµæœãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
                continue
            
            # æœ€æ–°ã®çµæœãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä½¿ç”¨
            result_file = max(result_files, key=lambda x: x.stat().st_mtime)
            
            try:
                with open(result_file, 'r', encoding='utf-8') as f:
                    data = json.load(f)
                
                # detailed_resultsã‹ã‚‰å€‹åˆ¥è¨˜éŒ²ã‚’æŠ½å‡º
                analysis_data = data.get('Real_OPENAI_Analysis', {})
                detailed_results = analysis_data.get('detailed_results', [])
                
                for result in detailed_results:
                    record = self._create_evaluation_record(result, project_name)
                    if record:
                        self.records.append(record)
                        total_loaded += 1
                
                self.logger.info(f"âœ… {project_name}: {len(detailed_results)}ä»¶ã®è©•ä¾¡ã‚’èª­ã¿è¾¼ã¿")
                
            except Exception as e:
                self.logger.error(f"âŒ {project_name}: ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼ - {e}")
        
        self.logger.info(f"ğŸ“Š ç·åˆèª­ã¿è¾¼ã¿å®Œäº†: {total_loaded}ä»¶ã®è©•ä¾¡è¨˜éŒ²")

    def _create_evaluation_record(self, result_data: Dict, project: str) -> EvaluationRecord:
        """è©•ä¾¡è¨˜éŒ²ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã‚’ä½œæˆ"""
        try:
            experiment_id = result_data.get('experiment_id', '')
            
            # task_typeã¨task_titleã‚’æŠ½å‡º
            if '/Issue_' in experiment_id:
                task_type = 'issue'
                task_title = experiment_id.split('/Issue_')[1].replace('_', ' ')
            elif '/PR_' in experiment_id:
                task_type = 'pullrequest'
                task_title = experiment_id.split('/PR_')[1].replace('_', ' ')
            else:
                task_type = 'unknown'
                task_title = experiment_id.split('/')[-1].replace('_', ' ')
            
            # ã‚«ãƒ†ã‚´ãƒªè‡ªå‹•åˆ†é¡
            category = self._classify_task_category(task_title)
            
            return EvaluationRecord(
                experiment_id=experiment_id,
                project=project,
                task_type=task_type,
                task_title=task_title,
                overall_compliance=result_data.get('parser_success_rate', 0.0),  # å€‹åˆ¥çµæœã«ã¯overall_complianceãªã—
                system_health=result_data.get('system_health', 'UNKNOWN'),
                parser_success_rate=result_data.get('parser_success_rate', 0.0),
                workflow_compliance=result_data.get('workflow_compliance', False),
                critical_issues=result_data.get('critical_issues', []),
                recommendations=result_data.get('recommendations', []),
                category=category
            )
            
        except Exception as e:
            self.logger.error(f"âš ï¸ è©•ä¾¡è¨˜éŒ²ä½œæˆã‚¨ãƒ©ãƒ¼: {e}")
            return None

    def _classify_task_category(self, task_title: str) -> str:
        """ã‚¿ã‚¹ã‚¯ã‚¿ã‚¤ãƒˆãƒ«ã‹ã‚‰ã‚«ãƒ†ã‚´ãƒªã‚’è‡ªå‹•åˆ†é¡"""
        task_lower = task_title.lower()
        
        # å„ã‚«ãƒ†ã‚´ãƒªã®ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãƒãƒƒãƒãƒ³ã‚°
        for category, keywords in self.category_keywords.items():
            for keyword in keywords:
                if keyword in task_lower:
                    return category
        
        return "ãã®ä»–"

    def analyze_distributions(self) -> DistributionAnalysis:
        """åº¦æ•°åˆ†å¸ƒåˆ†æã‚’å®Ÿè¡Œ"""
        self.logger.info("ğŸ“Š åº¦æ•°åˆ†å¸ƒåˆ†æé–‹å§‹")
        
        # è©•ä¾¡ã‚°ãƒ¬ãƒ¼ãƒ‰åˆ†å¸ƒ
        grade_counts = Counter()
        for record in self.records:
            grade = self._score_to_grade(record.overall_compliance)
            grade_counts[grade] += 1
        
        # ã‚·ã‚¹ãƒ†ãƒ å¥å…¨æ€§åˆ†å¸ƒ
        health_counts = Counter(record.system_health for record in self.records)
        
        # ã‚¹ã‚³ã‚¢çµ±è¨ˆ
        scores = [record.overall_compliance for record in self.records]
        score_stats = {
            'mean': np.mean(scores),
            'median': np.median(scores),
            'std': np.std(scores),
            'min': np.min(scores),
            'max': np.max(scores),
            'q25': np.percentile(scores, 25),
            'q75': np.percentile(scores, 75)
        }
        
        # ãƒ’ã‚¹ãƒˆã‚°ãƒ©ãƒ ç”¨ãƒ‡ãƒ¼ã‚¿
        hist_counts, hist_bins = np.histogram(scores, bins=20, range=(0.0, 1.0))
        
        return DistributionAnalysis(
            grade_distribution=dict(grade_counts),
            health_distribution=dict(health_counts),
            score_statistics=score_stats,
            score_histogram_data=(hist_counts, hist_bins)
        )

    def analyze_categories(self) -> CategoryAnalysis:
        """ã‚«ãƒ†ã‚´ãƒªåˆ¥åˆ†æã‚’å®Ÿè¡Œ"""
        self.logger.info("ğŸ·ï¸ ã‚«ãƒ†ã‚´ãƒªåˆ¥åˆ†æé–‹å§‹")
        
        category_data = defaultdict(list)
        
        # ã‚«ãƒ†ã‚´ãƒªåˆ¥ã«ãƒ‡ãƒ¼ã‚¿ã‚’ã‚°ãƒ«ãƒ¼ãƒ—åŒ–
        for record in self.records:
            category_data[record.category].append(record)
        
        category_stats = {}
        success_rates = {}
        average_scores = {}
        health_breakdown = {}
        
        for category, records in category_data.items():
            # æˆåŠŸç‡ï¼ˆEXCELLENT ã¾ãŸã¯ GOODï¼‰
            successful = sum(1 for r in records if r.system_health in ['EXCELLENT', 'GOOD'])
            success_rate = successful / len(records) if records else 0
            
            # å¹³å‡ã‚¹ã‚³ã‚¢
            avg_score = np.mean([r.overall_compliance for r in records]) if records else 0
            
            # å¥å…¨æ€§å†…è¨³
            health_count = Counter(r.system_health for r in records)
            
            # çµ±è¨ˆæƒ…å ±
            scores = [r.overall_compliance for r in records]
            stats = {
                'count': len(records),
                'success_rate': success_rate,
                'average_score': avg_score,
                'median_score': np.median(scores) if scores else 0,
                'score_std': np.std(scores) if scores else 0,
                'workflow_compliance_rate': sum(1 for r in records if r.workflow_compliance) / len(records) if records else 0
            }
            
            category_stats[category] = stats
            success_rates[category] = success_rate
            average_scores[category] = avg_score
            health_breakdown[category] = dict(health_count)
        
        return CategoryAnalysis(
            category_stats=category_stats,
            success_rates=success_rates,
            average_scores=average_scores,
            health_breakdown=health_breakdown
        )

    def _score_to_grade(self, score: float) -> str:
        """ã‚¹ã‚³ã‚¢ã‚’è©•ä¾¡ã‚°ãƒ¬ãƒ¼ãƒ‰ã«å¤‰æ›"""
        for grade, threshold in self.grade_thresholds.items():
            if score >= threshold:
                return grade
        return 'F'

    def generate_visualizations(self, output_dir: Path) -> None:
        """å¯è¦–åŒ–ã‚°ãƒ©ãƒ•ã‚’ç”Ÿæˆ"""
        self.logger.info(f"ğŸ“ˆ å¯è¦–åŒ–ã‚°ãƒ©ãƒ•ç”Ÿæˆé–‹å§‹: {output_dir}")
        
        # å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªä½œæˆ
        output_dir.mkdir(exist_ok=True)
        
        # æ—¥æœ¬èªãƒ•ã‚©ãƒ³ãƒˆè¨­å®šï¼ˆåˆ©ç”¨å¯èƒ½ã§ã‚ã‚Œã°ï¼‰
        plt.style.use('seaborn-v0_8')
        try:
            plt.rcParams['font.family'] = 'DejaVu Sans'
        except:
            pass
        
        # åˆ†æå®Ÿè¡Œ
        dist_analysis = self.analyze_distributions()
        cat_analysis = self.analyze_categories()
        
        # 1. è©•ä¾¡ã‚°ãƒ¬ãƒ¼ãƒ‰åˆ†å¸ƒï¼ˆå††ã‚°ãƒ©ãƒ•ï¼‰
        self._plot_grade_distribution(dist_analysis.grade_distribution, output_dir)
        
        # 2. ã‚·ã‚¹ãƒ†ãƒ å¥å…¨æ€§åˆ†å¸ƒï¼ˆæ£’ã‚°ãƒ©ãƒ•ï¼‰
        self._plot_health_distribution(dist_analysis.health_distribution, output_dir)
        
        # 3. ã‚¹ã‚³ã‚¢ãƒ’ã‚¹ãƒˆã‚°ãƒ©ãƒ 
        self._plot_score_histogram(dist_analysis.score_histogram_data, output_dir)
        
        # 4. ã‚«ãƒ†ã‚´ãƒªåˆ¥æˆåŠŸç‡
        self._plot_category_success_rates(cat_analysis.success_rates, output_dir)
        
        # 5. ã‚«ãƒ†ã‚´ãƒªåˆ¥å¹³å‡ã‚¹ã‚³ã‚¢
        self._plot_category_scores(cat_analysis.average_scores, output_dir)
        
        # 6. ã‚«ãƒ†ã‚´ãƒªåˆ¥å¥å…¨æ€§ãƒ’ãƒ¼ãƒˆãƒãƒƒãƒ—
        self._plot_category_health_heatmap(cat_analysis.health_breakdown, output_dir)
        
        self.logger.info("âœ… å¯è¦–åŒ–ã‚°ãƒ©ãƒ•ç”Ÿæˆå®Œäº†")

    def _plot_grade_distribution(self, grade_dist: Dict[str, int], output_dir: Path):
        """è©•ä¾¡ã‚°ãƒ¬ãƒ¼ãƒ‰åˆ†å¸ƒã®å††ã‚°ãƒ©ãƒ•"""
        plt.figure(figsize=(10, 8))
        
        # ã‚°ãƒ¬ãƒ¼ãƒ‰é †ã«ã‚½ãƒ¼ãƒˆ
        grade_order = ['A+', 'A', 'B', 'C', 'D', 'F']
        sorted_grades = [(g, grade_dist.get(g, 0)) for g in grade_order if grade_dist.get(g, 0) > 0]
        
        labels, sizes = zip(*sorted_grades)
        colors = ['#2E8B57', '#32CD32', '#FFD700', '#FF8C00', '#FF4500', '#DC143C'][:len(labels)]
        
        plt.pie(sizes, labels=labels, colors=colors, autopct='%1.1f%%', startangle=90)
        plt.title('APR System Evaluation Grade Distribution', fontsize=16, fontweight='bold')
        plt.axis('equal')
        plt.tight_layout()
        plt.savefig(output_dir / 'grade_distribution.png', dpi=300, bbox_inches='tight')
        plt.close()

    def _plot_health_distribution(self, health_dist: Dict[str, int], output_dir: Path):
        """ã‚·ã‚¹ãƒ†ãƒ å¥å…¨æ€§åˆ†å¸ƒã®æ£’ã‚°ãƒ©ãƒ•"""
        plt.figure(figsize=(12, 6))
        
        health_order = ['EXCELLENT', 'GOOD', 'POOR', 'CRITICAL']
        labels = [h for h in health_order if h in health_dist]
        values = [health_dist[h] for h in labels]
        colors = ['#2E8B57', '#32CD32', '#FF8C00', '#DC143C'][:len(labels)]
        
        bars = plt.bar(labels, values, color=colors, alpha=0.8)
        plt.title('System Health Distribution', fontsize=16, fontweight='bold')
        plt.xlabel('Health Status')
        plt.ylabel('Number of Cases')
        
        # å€¤ã‚’ãƒãƒ¼ã®ä¸Šã«è¡¨ç¤º
        for bar, value in zip(bars, values):
            plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.5, 
                    str(value), ha='center', va='bottom', fontweight='bold')
        
        plt.tight_layout()
        plt.savefig(output_dir / 'health_distribution.png', dpi=300, bbox_inches='tight')
        plt.close()

    def _plot_score_histogram(self, hist_data: Tuple[np.ndarray, np.ndarray], output_dir: Path):
        """é©åˆæ€§ã‚¹ã‚³ã‚¢ã®ãƒ’ã‚¹ãƒˆã‚°ãƒ©ãƒ """
        counts, bins = hist_data
        
        plt.figure(figsize=(12, 6))
        plt.hist(bins[:-1], bins, weights=counts, alpha=0.7, color='steelblue', edgecolor='black')
        plt.title('Overall Compliance Score Distribution', fontsize=16, fontweight='bold')
        plt.xlabel('Compliance Score')
        plt.ylabel('Frequency')
        plt.grid(True, alpha=0.3)
        plt.tight_layout()
        plt.savefig(output_dir / 'score_histogram.png', dpi=300, bbox_inches='tight')
        plt.close()

    def _plot_category_success_rates(self, success_rates: Dict[str, float], output_dir: Path):
        """ã‚«ãƒ†ã‚´ãƒªåˆ¥æˆåŠŸç‡"""
        plt.figure(figsize=(14, 8))
        
        categories = list(success_rates.keys())
        rates = [success_rates[cat] * 100 for cat in categories]  # ãƒ‘ãƒ¼ã‚»ãƒ³ãƒ†ãƒ¼ã‚¸ã«å¤‰æ›
        
        bars = plt.bar(range(len(categories)), rates, alpha=0.8, color='lightcoral')
        plt.title('Success Rate by Task Category', fontsize=16, fontweight='bold')
        plt.xlabel('Task Category')
        plt.ylabel('Success Rate (%)')
        plt.xticks(range(len(categories)), categories, rotation=45, ha='right')
        
        # å€¤ã‚’ãƒãƒ¼ã®ä¸Šã«è¡¨ç¤º
        for i, (bar, rate) in enumerate(zip(bars, rates)):
            plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 1, 
                    f'{rate:.1f}%', ha='center', va='bottom', fontweight='bold')
        
        plt.tight_layout()
        plt.savefig(output_dir / 'category_success_rates.png', dpi=300, bbox_inches='tight')
        plt.close()

    def _plot_category_scores(self, avg_scores: Dict[str, float], output_dir: Path):
        """ã‚«ãƒ†ã‚´ãƒªåˆ¥å¹³å‡ã‚¹ã‚³ã‚¢"""
        plt.figure(figsize=(14, 8))
        
        categories = list(avg_scores.keys())
        scores = [avg_scores[cat] for cat in categories]
        
        bars = plt.bar(range(len(categories)), scores, alpha=0.8, color='lightblue')
        plt.title('Average Compliance Score by Task Category', fontsize=16, fontweight='bold')
        plt.xlabel('Task Category')
        plt.ylabel('Average Compliance Score')
        plt.xticks(range(len(categories)), categories, rotation=45, ha='right')
        plt.ylim(0, 1.0)
        
        # å€¤ã‚’ãƒãƒ¼ã®ä¸Šã«è¡¨ç¤º
        for i, (bar, score) in enumerate(zip(bars, scores)):
            plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01, 
                    f'{score:.3f}', ha='center', va='bottom', fontweight='bold')
        
        plt.tight_layout()
        plt.savefig(output_dir / 'category_average_scores.png', dpi=300, bbox_inches='tight')
        plt.close()

    def _plot_category_health_heatmap(self, health_breakdown: Dict[str, Dict[str, int]], output_dir: Path):
        """ã‚«ãƒ†ã‚´ãƒªåˆ¥å¥å…¨æ€§ãƒ’ãƒ¼ãƒˆãƒãƒƒãƒ—"""
        plt.figure(figsize=(12, 8))
        
        # ãƒ‡ãƒ¼ã‚¿ã‚’è¡Œåˆ—å½¢å¼ã«å¤‰æ›
        categories = list(health_breakdown.keys())
        health_statuses = ['EXCELLENT', 'GOOD', 'POOR', 'CRITICAL']
        
        matrix = []
        for category in categories:
            row = [health_breakdown[category].get(status, 0) for status in health_statuses]
            matrix.append(row)
        
        # ãƒ’ãƒ¼ãƒˆãƒãƒƒãƒ—ä½œæˆ
        sns.heatmap(matrix, xticklabels=health_statuses, yticklabels=categories, 
                   annot=True, fmt='d', cmap='RdYlGn_r', cbar_kws={'label': 'Number of Cases'})
        plt.title('Health Status Distribution by Task Category', fontsize=16, fontweight='bold')
        plt.xlabel('Health Status')
        plt.ylabel('Task Category')
        plt.tight_layout()
        plt.savefig(output_dir / 'category_health_heatmap.png', dpi=300, bbox_inches='tight')
        plt.close()

    def generate_report(self, output_dir: Path) -> None:
        """ç·åˆãƒ¬ãƒãƒ¼ãƒˆã‚’ç”Ÿæˆ"""
        self.logger.info("ğŸ“„ ç·åˆãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆé–‹å§‹")
        
        dist_analysis = self.analyze_distributions()
        cat_analysis = self.analyze_categories()
        
        report = []
        report.append("# APR System Dataset Analysis Report")
        report.append(f"**åˆ†ææ—¥æ™‚**: {pd.Timestamp.now().strftime('%Y-%m-%d %H:%M:%S')}")
        report.append(f"**ç·è©•ä¾¡ä»¶æ•°**: {len(self.records):,}")
        report.append("")
        
        # 1. åº¦æ•°åˆ†å¸ƒã‚µãƒãƒªãƒ¼
        report.append("## 1. åº¦æ•°åˆ†å¸ƒåˆ†æ")
        report.append("")
        
        # ã‚°ãƒ¬ãƒ¼ãƒ‰åˆ†å¸ƒ
        report.append("### è©•ä¾¡ã‚°ãƒ¬ãƒ¼ãƒ‰åˆ†å¸ƒ")
        for grade, count in sorted(dist_analysis.grade_distribution.items()):
            percentage = (count / len(self.records)) * 100
            report.append(f"- **{grade}**: {count:,}ä»¶ ({percentage:.1f}%)")
        report.append("")
        
        # å¥å…¨æ€§åˆ†å¸ƒ
        report.append("### ã‚·ã‚¹ãƒ†ãƒ å¥å…¨æ€§åˆ†å¸ƒ")
        for health, count in dist_analysis.health_distribution.items():
            percentage = (count / len(self.records)) * 100
            report.append(f"- **{health}**: {count:,}ä»¶ ({percentage:.1f}%)")
        report.append("")
        
        # ã‚¹ã‚³ã‚¢çµ±è¨ˆ
        report.append("### é©åˆæ€§ã‚¹ã‚³ã‚¢çµ±è¨ˆ")
        stats = dist_analysis.score_statistics
        report.append(f"- **å¹³å‡**: {stats['mean']:.3f}")
        report.append(f"- **ä¸­å¤®å€¤**: {stats['median']:.3f}")
        report.append(f"- **æ¨™æº–åå·®**: {stats['std']:.3f}")
        report.append(f"- **æœ€å°å€¤**: {stats['min']:.3f}")
        report.append(f"- **æœ€å¤§å€¤**: {stats['max']:.3f}")
        report.append("")
        
        # 2. ã‚«ãƒ†ã‚´ãƒªåˆ¥åˆ†æ
        report.append("## 2. ã‚«ãƒ†ã‚´ãƒªåˆ¥åˆ†æ")
        report.append("")
        
        # ã‚«ãƒ†ã‚´ãƒªåˆ¥ãƒ†ãƒ¼ãƒ–ãƒ«
        report.append("| ã‚«ãƒ†ã‚´ãƒª | ä»¶æ•° | æˆåŠŸç‡ | å¹³å‡ã‚¹ã‚³ã‚¢ | æœ€é«˜å¥å…¨æ€§ |")
        report.append("|---------|------|--------|-----------|------------|")
        
        for category, stats in cat_analysis.category_stats.items():
            health_dist = cat_analysis.health_breakdown[category]
            best_health = max(health_dist.keys(), key=lambda k: health_dist[k])
            
            report.append(f"| {category} | {stats['count']:,} | {stats['success_rate']:.1%} | {stats['average_score']:.3f} | {best_health} |")
        
        report.append("")
        
        # 3. ä¸»è¦ãªç™ºè¦‹
        report.append("## 3. ä¸»è¦ãªç™ºè¦‹")
        report.append("")
        
        # æœ€é«˜æˆåŠŸç‡ã‚«ãƒ†ã‚´ãƒª
        best_category = max(cat_analysis.success_rates.keys(), key=lambda k: cat_analysis.success_rates[k])
        worst_category = min(cat_analysis.success_rates.keys(), key=lambda k: cat_analysis.success_rates[k])
        
        report.append(f"- **æœ€é«˜æˆåŠŸç‡ã‚«ãƒ†ã‚´ãƒª**: {best_category} ({cat_analysis.success_rates[best_category]:.1%})")
        report.append(f"- **æœ€ä½æˆåŠŸç‡ã‚«ãƒ†ã‚´ãƒª**: {worst_category} ({cat_analysis.success_rates[worst_category]:.1%})")
        report.append("")
        
        # æœ€é«˜ã‚¹ã‚³ã‚¢ã‚«ãƒ†ã‚´ãƒª
        best_score_category = max(cat_analysis.average_scores.keys(), key=lambda k: cat_analysis.average_scores[k])
        worst_score_category = min(cat_analysis.average_scores.keys(), key=lambda k: cat_analysis.average_scores[k])
        
        report.append(f"- **æœ€é«˜å¹³å‡ã‚¹ã‚³ã‚¢ã‚«ãƒ†ã‚´ãƒª**: {best_score_category} ({cat_analysis.average_scores[best_score_category]:.3f})")
        report.append(f"- **æœ€ä½å¹³å‡ã‚¹ã‚³ã‚¢ã‚«ãƒ†ã‚´ãƒª**: {worst_score_category} ({cat_analysis.average_scores[worst_score_category]:.3f})")
        report.append("")
        
        # ãƒ¬ãƒãƒ¼ãƒˆä¿å­˜
        with open(output_dir / 'analysis_report.md', 'w', encoding='utf-8') as f:
            f.write('\n'.join(report))
        
        self.logger.info("âœ… ç·åˆãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆå®Œäº†")

    def export_detailed_data(self, output_dir: Path) -> None:
        """è©³ç´°ãƒ‡ãƒ¼ã‚¿ã‚’CSVã§ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆ"""
        self.logger.info("ğŸ’¾ è©³ç´°ãƒ‡ãƒ¼ã‚¿ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆé–‹å§‹")
        
        # åŸºæœ¬ãƒ‡ãƒ¼ã‚¿ã‚’DataFrameã«å¤‰æ›
        data = []
        for record in self.records:
            data.append({
                'experiment_id': record.experiment_id,
                'project': record.project,
                'task_type': record.task_type,
                'task_title': record.task_title,
                'category': record.category,
                'overall_compliance': record.overall_compliance,
                'system_health': record.system_health,
                'parser_success_rate': record.parser_success_rate,
                'workflow_compliance': record.workflow_compliance,
                'critical_issues_count': len(record.critical_issues),
                'recommendations_count': len(record.recommendations),
                'grade': self._score_to_grade(record.overall_compliance)
            })
        
        df = pd.DataFrame(data)
        df.to_csv(output_dir / 'detailed_evaluation_data.csv', index=False, encoding='utf-8')
        
        self.logger.info("âœ… è©³ç´°ãƒ‡ãƒ¼ã‚¿ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆå®Œäº†")


def main():
    """ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œé–¢æ•°"""
    import argparse
    
    parser = argparse.ArgumentParser(description='APR Dataset Analysis Tool')
    parser.add_argument('session_path', help='è©•ä¾¡ã‚»ãƒƒã‚·ãƒ§ãƒ³ã®ãƒ‘ã‚¹')
    parser.add_argument('--output', '-o', default='./analysis_output', help='å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª')
    
    args = parser.parse_args()
    
    # åˆ†æå™¨åˆæœŸåŒ–
    analyzer = APRDatasetAnalyzer(args.session_path)
    
    # å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªæº–å‚™
    output_dir = Path(args.output)
    output_dir.mkdir(exist_ok=True)
    
    try:
        # ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿
        analyzer.load_evaluation_data()
        
        # åˆ†æå®Ÿè¡Œ
        analyzer.generate_visualizations(output_dir)
        analyzer.generate_report(output_dir)
        analyzer.export_detailed_data(output_dir)
        
        print(f"ğŸ‰ åˆ†æå®Œäº†ï¼çµæœã¯ {output_dir} ã«ä¿å­˜ã•ã‚Œã¾ã—ãŸ")
        
    except Exception as e:
        print(f"âŒ åˆ†æã‚¨ãƒ©ãƒ¼: {e}")
        raise


if __name__ == "__main__":
    main()
