#!/usr/bin/env python3
"""
è©³ç´°åˆ†æä»˜ããƒªãƒã‚¸ãƒˆãƒªå‹•ä½œæ¤œè¨¼
ç‰¹å®šã®ãƒªãƒã‚¸ãƒˆãƒªã®è©³ç´°ãªãƒ­ã‚°åˆ†æã¨çµæœä¿å­˜
"""

import asyncio
import json
import sys
from datetime import datetime
from pathlib import Path

# ãƒ‘ã‚¹ã‚’è¿½åŠ 
sys.path.append('/app')

from src.evaluators.compliance_evaluator import SystemComplianceEvaluator
from src.utils.log_iterator import APRLogIterator


async def detailed_repository_analysis(repo_name: str = "servantes", max_logs: int = 5):
    """è©³ç´°ãªãƒªãƒã‚¸ãƒˆãƒªåˆ†æ"""
    print(f"ğŸ”¬ è©³ç´°ãƒªãƒã‚¸ãƒˆãƒªåˆ†æ: {repo_name}")
    print("=" * 60)
    
    # çµæœä¿å­˜ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’ä½œæˆ
    results_dir = Path("/app/output/verification_results")
    results_dir.mkdir(exist_ok=True)
    
    # ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—ä»˜ããƒ•ã‚¡ã‚¤ãƒ«å
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    result_file = results_dir / f"analysis_{repo_name}_{timestamp}.json"
    
    # å„LLMãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ã§ãƒ†ã‚¹ãƒˆ
    test_configs = [
        {"provider": "mock", "model": None, "name": "Mock_LLM"}
    ]
    
    # OpenAI APIã‚­ãƒ¼ãŒè¨­å®šã•ã‚Œã¦ã„ã‚Œã°ã€OpenAIãƒ†ã‚¹ãƒˆã‚‚è¿½åŠ 
    import os
    if os.getenv("OPENAI_API_KEY"):
        test_configs.extend([
            {"provider": "openai", "model": "gpt-4.1-mini", "name": "GPT4o_Mini"},
            {"provider": "openai", "model": "gpt-4.1", "name": "GPT4o"}
        ])
        print("ğŸ”‘ OpenAI API Key detected - real LLM tests will be included")
    else:
        print("âš ï¸  OpenAI API Key not found - using Mock LLM only")
    
    all_results = {}
    
    for config in test_configs:
        print(f"\nğŸ§ª å®Ÿè¡Œä¸­: {config['name']}")
        print("-" * 40)
        
        try:
            # è©•ä¾¡å™¨ã‚’åˆæœŸåŒ–
            evaluator = SystemComplianceEvaluator(
                llm_provider=config["provider"],
                llm_model=config["model"]
            )
            
            # å˜ä¸€ãƒªãƒã‚¸ãƒˆãƒªè©•ä¾¡ã‚’å®Ÿè¡Œ
            results = await evaluator.evaluate_single_repository(repo_name, max_logs)
            
            # çµæœã‚’ãƒ‡ã‚£ã‚¯ã‚·ãƒ§ãƒŠãƒªã«ä¿å­˜
            all_results[config["name"]] = results
            
            # çµæœã‚µãƒãƒªãƒ¼ã‚’è¡¨ç¤º
            print_detailed_summary(results, config["name"])
            
        except Exception as e:
            print(f"âŒ ã‚¨ãƒ©ãƒ¼ ({config['name']}): {e}")
            all_results[config["name"]] = {"error": str(e)}
    
    # çµæœã‚’JSONãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜
    print(f"\nğŸ’¾ çµæœã‚’ä¿å­˜ä¸­: {result_file}")
    with open(result_file, 'w', encoding='utf-8') as f:
        json.dump(all_results, f, indent=2, ensure_ascii=False, default=str)
    
    print(f"âœ… åˆ†æå®Œäº† - çµæœãƒ•ã‚¡ã‚¤ãƒ«: {result_file}")
    
    # æ¯”è¼ƒåˆ†æã‚’è¡¨ç¤º
    print_comparison_analysis(all_results)
    
    return all_results


def print_detailed_summary(results: dict, config_name: str):
    """è©³ç´°ã‚µãƒãƒªãƒ¼ã‚’è¡¨ç¤º"""
    print(f"ğŸ“Š è©³ç´°çµæœ - {config_name}:")
    
    if "error" in results:
        print(f"âŒ ã‚¨ãƒ©ãƒ¼: {results['error']}")
        return
    
    summary = results.get("summary", {})
    metrics = results.get("compliance_metrics", {})
    
    print(f"  ğŸ“ˆ ãƒ¡ãƒˆãƒªã‚¯ã‚¹è©³ç´°:")
    print(f"    - ç·åˆé©åˆæ€§: {metrics.get('overall_compliance', 0):.4f}")
    print(f"    - ãƒ‘ãƒ¼ã‚µãƒ¼æˆåŠŸç‡: {metrics.get('parser_success_rate', 0):.4f}")
    print(f"    - åˆ¶å¾¡ãƒ•ãƒ­ãƒ¼ç²¾åº¦: {metrics.get('control_flow_accuracy', 0):.4f}")
    print(f"    - ãƒ•ã‚¡ã‚¤ãƒ«å‡¦ç†ç‡: {metrics.get('file_processing_rate', 0):.4f}")
    print(f"    - ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°: {metrics.get('error_handling_score', 0):.4f}")
    
    print(f"  ğŸ“ å‡¦ç†çµ±è¨ˆ:")
    print(f"    - å‡¦ç†æ™‚é–“: {results.get('processing_time_seconds', 0):.2f}ç§’")
    print(f"    - æˆåŠŸ/å¤±æ•—: {summary.get('successful_evaluations', 0)}/{summary.get('failed_evaluations', 0)}")
    print(f"    - ãƒ‘ãƒ¼ã‚µãƒ¼å•é¡Œ: {summary.get('parser_issues_detected', 0)}ä»¶")
    print(f"    - ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼é•å: {summary.get('workflow_violations', 0)}ä»¶")
    print(f"    - ã‚¯ãƒªãƒ†ã‚£ã‚«ãƒ«ã‚·ã‚¹ãƒ†ãƒ : {summary.get('critical_systems', 0)}ä»¶")


def print_comparison_analysis(all_results: dict):
    """è¤‡æ•°è¨­å®šã®æ¯”è¼ƒåˆ†æ"""
    print(f"\nğŸ” æ¯”è¼ƒåˆ†æ")
    print("=" * 40)
    
    valid_results = {k: v for k, v in all_results.items() if "error" not in v}
    
    if len(valid_results) < 2:
        print("âš ï¸  æ¯”è¼ƒã«ååˆ†ãªçµæœãŒã‚ã‚Šã¾ã›ã‚“")
        return
    
    # ãƒ¡ãƒˆãƒªã‚¯ã‚¹æ¯”è¼ƒè¡¨
    print("ğŸ“Š ãƒ¡ãƒˆãƒªã‚¯ã‚¹æ¯”è¼ƒ:")
    print(f"{'è¨­å®š':<15} {'ç·åˆé©åˆæ€§':<12} {'ãƒ‘ãƒ¼ã‚µãƒ¼ç‡':<10} {'åˆ¶å¾¡ãƒ•ãƒ­ãƒ¼':<10} {'å‡¦ç†æ™‚é–“':<10}")
    print("-" * 60)
    
    for name, result in valid_results.items():
        metrics = result.get("compliance_metrics", {})
        processing_time = result.get("processing_time_seconds", 0)
        
        print(f"{name:<15} "
              f"{metrics.get('overall_compliance', 0):<12.4f} "
              f"{metrics.get('parser_success_rate', 0):<10.4f} "
              f"{metrics.get('control_flow_accuracy', 0):<10.4f} "
              f"{processing_time:<10.2f}")
    
    # æœ€è‰¯çµæœã®ç‰¹å®š
    best_overall = max(valid_results.items(), 
                      key=lambda x: x[1].get("compliance_metrics", {}).get("overall_compliance", 0))
    fastest = min(valid_results.items(),
                  key=lambda x: x[1].get("processing_time_seconds", float('inf')))
    
    print(f"\nğŸ† ãƒ™ã‚¹ãƒˆãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹:")
    print(f"  - æœ€é«˜é©åˆæ€§: {best_overall[0]} ({best_overall[1]['compliance_metrics']['overall_compliance']:.4f})")
    print(f"  - æœ€é€Ÿå‡¦ç†: {fastest[0]} ({fastest[1]['processing_time_seconds']:.2f}ç§’)")


def analyze_log_content(repo_name: str):
    """ãƒ­ã‚°ãƒ•ã‚¡ã‚¤ãƒ«ã®å†…å®¹åˆ†æ"""
    print(f"\nğŸ” ãƒ­ã‚°ãƒ•ã‚¡ã‚¤ãƒ«å†…å®¹åˆ†æ: {repo_name}")
    print("-" * 30)
    
    log_iterator = APRLogIterator("/app")
    sample_count = 0
    
    for log_entry in log_iterator.iterate_all_logs():
        if log_entry.project_name != repo_name or sample_count >= 2:
            continue
            
        try:
            print(f"\nğŸ“„ ãƒ­ã‚°ãƒ•ã‚¡ã‚¤ãƒ«: {log_entry.log_path}")
            print(f"  ã‚µã‚¤ã‚º: {log_entry.size:,} bytes")
            print(f"  ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—: {log_entry.timestamp}")
            
            # ãƒ­ã‚°ãƒ•ã‚¡ã‚¤ãƒ«ã®æœ€åˆã®æ•°è¡Œã‚’è¡¨ç¤º
            with open(log_entry.log_path, 'r', encoding='utf-8') as f:
                content = f.read()
                lines = content.split('\n')[:10]  # æœ€åˆã®10è¡Œ
                print(f"  å†…å®¹ãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼:")
                for i, line in enumerate(lines, 1):
                    if line.strip():
                        print(f"    {i:2d}: {line[:100]}")
            
            sample_count += 1
            
        except Exception as e:
            print(f"  âŒ ãƒ­ã‚°èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}")


async def main():
    """ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œé–¢æ•°"""
    # ã‚³ãƒãƒ³ãƒ‰ãƒ©ã‚¤ãƒ³å¼•æ•°å‡¦ç†
    repo_name = sys.argv[1] if len(sys.argv) > 1 else "servantes"
    max_logs = int(sys.argv[2]) if len(sys.argv) > 2 else 5
    
    print(f"ğŸš€ APRã‚·ã‚¹ãƒ†ãƒ é©åˆæ€§è©•ä¾¡ - è©³ç´°åˆ†æ")
    print(f"ğŸ¯ å¯¾è±¡ãƒªãƒã‚¸ãƒˆãƒª: {repo_name}")
    print(f"ğŸ“Š æœ€å¤§ãƒ­ã‚°æ•°: {max_logs}")
    print("=" * 60)
    
    # ãƒ­ã‚°å†…å®¹ã®äº‹å‰åˆ†æ
    analyze_log_content(repo_name)
    
    # è©³ç´°åˆ†æå®Ÿè¡Œ
    results = await detailed_repository_analysis(repo_name, max_logs)
    
    print(f"\nâœ… å…¨åˆ†æå®Œäº†")
    print(f"ğŸ“ çµæœãƒ•ã‚¡ã‚¤ãƒ«ã¯ /app/output/verification_results/ ã«ä¿å­˜ã•ã‚Œã¾ã—ãŸ")


if __name__ == "__main__":
    asyncio.run(main())
