#!/usr/bin/env python3
"""
APRè©•ä¾¡çµæœã®è©³ç´°åˆ†æãƒ„ãƒ¼ãƒ«
å€‹åˆ¥ã®LLMè©•ä¾¡ãƒ¬ã‚¹ãƒãƒ³ã‚¹ã‹ã‚‰è©³ç´°ãªãƒ¡ãƒˆãƒªã‚¯ã‚¹ã‚’æŠ½å‡ºã—ã€ã‚ˆã‚Šç²¾å¯†ãªåˆ†æã‚’å®Ÿè¡Œ
"""

import json
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import numpy as np
from pathlib import Path
from typing import Dict, List, Any, Optional
from dataclasses import dataclass
from collections import defaultdict, Counter
import re
import logging


@dataclass 
class DetailedEvaluationMetrics:
    """è©³ç´°è©•ä¾¡ãƒ¡ãƒˆãƒªã‚¯ã‚¹"""
    experiment_id: str
    project: str
    task_title: str
    category: str
    
    # LLMè©•ä¾¡ã‚¹ã‚³ã‚¢
    parser_success_rate: float
    workflow_compliance: bool
    system_health: str
    
    # LLMä½¿ç”¨é‡çµ±è¨ˆ
    prompt_tokens: int
    completion_tokens: int
    total_tokens: int
    
    # æ¨å¥¨äº‹é …ã¨ã‚¯ãƒªãƒ†ã‚£ã‚«ãƒ«å•é¡Œ
    recommendations_count: int
    critical_issues_count: int
    
    # è©³ç´°åˆ†æç”¨
    task_complexity: str = "MEDIUM"  # ã‚¿ã‚¹ã‚¯ã®è¤‡é›‘åº¦
    estimated_difficulty: float = 0.5  # æ¨å®šé›£æ˜“åº¦


class EnhancedAPRAnalyzer:
    """æ‹¡å¼µAPRåˆ†æå™¨"""
    
    def __init__(self, session_path: str):
        self.session_path = Path(session_path)
        self.results_path = self.session_path / "results"
        self.detailed_responses_path = self.session_path / "detailed_responses"
        self.metrics: List[DetailedEvaluationMetrics] = []
        
        # ã‚«ãƒ†ã‚´ãƒªåˆ†é¡ï¼ˆå‰å›ã¨åŒã˜ï¼‰
        self.category_keywords = {
            'ãƒªãƒ•ã‚¡ã‚¯ã‚¿ãƒªãƒ³ã‚°': ['refactor', 'rename', 'restructure', 'reorganize', 'move', 'extract'],
            'ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æ”¹å–„': ['performance', 'optimize', 'cache', 'index', 'speed', 'efficient'],
            'æ©Ÿèƒ½è¿½åŠ ': ['add', 'implement', 'feature', 'endpoint', 'api', 'support'],
            'ãƒ†ã‚¹ãƒˆã‚³ãƒ¼ãƒ‰': ['test', 'unittest', 'integration', 'coverage', 'mock'],
            'ãƒã‚°ä¿®æ­£': ['fix', 'bug', 'error', 'issue', 'correct', 'resolve'],
            'å˜ç´”ä¿®æ­£': ['typo', 'license', 'header', 'format', 'style', 'comment'],
            'ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆ': ['doc', 'readme', 'documentation', 'comment', 'example'],
            'ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£': ['security', 'auth', 'permission', 'vulnerability', 'secure'],
            'ä¾å­˜é–¢ä¿‚': ['dependency', 'update', 'version', 'upgrade', 'package'],
            'è¨­å®šãƒ»ç’°å¢ƒ': ['config', 'environment', 'setup', 'install', 'deploy'],
            'ãƒ—ãƒ­ãƒˆã‚³ãƒ«ãƒ»é€šä¿¡': ['grpc', 'http', 'api', 'protocol', 'endpoint', 'request'],
            'ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹': ['database', 'sql', 'query', 'table', 'schema', 'migration']
        }
        
        # è¤‡é›‘åº¦åˆ¤å®šã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰
        self.complexity_indicators = {
            'HIGH': ['multiple', 'complex', 'integration', 'migration', 'refactor', 'architecture'],
            'MEDIUM': ['add', 'update', 'modify', 'implement', 'enhance'],
            'LOW': ['fix', 'typo', 'format', 'simple', 'minor']
        }
        
        logging.basicConfig(level=logging.INFO)
        self.logger = logging.getLogger(__name__)

    def load_enhanced_data(self) -> None:
        """æ‹¡å¼µãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã¿"""
        self.logger.info(f"ğŸ” æ‹¡å¼µè©•ä¾¡ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿é–‹å§‹: {self.session_path}")
        
        total_loaded = 0
        
        for project_dir in self.results_path.iterdir():
            if not project_dir.is_dir():
                continue
                
            project_name = project_dir.name
            result_files = list(project_dir.glob("evaluation_result_*.json"))
            
            if not result_files:
                continue
            
            result_file = max(result_files, key=lambda x: x.stat().st_mtime)
            
            try:
                with open(result_file, 'r', encoding='utf-8') as f:
                    data = json.load(f)
                
                analysis_data = data.get('Real_OPENAI_Analysis', {})
                detailed_results = analysis_data.get('detailed_results', [])
                
                for result in detailed_results:
                    metrics = self._create_detailed_metrics(result, project_name)
                    if metrics:
                        self.metrics.append(metrics)
                        total_loaded += 1
                
                self.logger.info(f"âœ… {project_name}: {len(detailed_results)}ä»¶ã®è©³ç´°ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã‚’èª­ã¿è¾¼ã¿")
                
            except Exception as e:
                self.logger.error(f"âŒ {project_name}: {e}")
        
        self.logger.info(f"ğŸ“Š ç·åˆèª­ã¿è¾¼ã¿å®Œäº†: {total_loaded}ä»¶ã®ãƒ¡ãƒˆãƒªã‚¯ã‚¹")

    def _create_detailed_metrics(self, result_data: Dict, project: str) -> Optional[DetailedEvaluationMetrics]:
        """è©³ç´°ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã‚’ä½œæˆ"""
        try:
            experiment_id = result_data.get('experiment_id', '')
            
            # ã‚¿ã‚¹ã‚¯ã‚¿ã‚¤ãƒˆãƒ«æŠ½å‡º
            if '/Issue_' in experiment_id:
                task_title = experiment_id.split('/Issue_')[1].replace('_', ' ')
            elif '/PR_' in experiment_id:
                task_title = experiment_id.split('/PR_')[1].replace('_', ' ')
            else:
                task_title = experiment_id.split('/')[-1].replace('_', ' ')
            
            # ã‚«ãƒ†ã‚´ãƒªè‡ªå‹•åˆ†é¡
            category = self._classify_task_category(task_title)
            
            # è¤‡é›‘åº¦åˆ¤å®š
            complexity = self._estimate_task_complexity(task_title)
            difficulty = self._estimate_task_difficulty(task_title, category)
            
            # LLMä½¿ç”¨é‡çµ±è¨ˆ
            llm_usage = result_data.get('llm_usage', {})
            
            return DetailedEvaluationMetrics(
                experiment_id=experiment_id,
                project=project,
                task_title=task_title,
                category=category,
                parser_success_rate=result_data.get('parser_success_rate', 0.0),
                workflow_compliance=result_data.get('workflow_compliance', False),
                system_health=result_data.get('system_health', 'UNKNOWN'),
                prompt_tokens=llm_usage.get('prompt_tokens', 0),
                completion_tokens=llm_usage.get('completion_tokens', 0),
                total_tokens=llm_usage.get('total_tokens', 0),
                recommendations_count=len(result_data.get('recommendations', [])),
                critical_issues_count=len(result_data.get('critical_issues', [])),
                task_complexity=complexity,
                estimated_difficulty=difficulty
            )
            
        except Exception as e:
            self.logger.error(f"âš ï¸ è©³ç´°ãƒ¡ãƒˆãƒªã‚¯ã‚¹ä½œæˆã‚¨ãƒ©ãƒ¼: {e}")
            return None

    def _classify_task_category(self, task_title: str) -> str:
        """ã‚¿ã‚¹ã‚¯ã‚«ãƒ†ã‚´ãƒªã‚’åˆ†é¡"""
        task_lower = task_title.lower()
        
        for category, keywords in self.category_keywords.items():
            for keyword in keywords:
                if keyword in task_lower:
                    return category
        
        return "ãã®ä»–"

    def _estimate_task_complexity(self, task_title: str) -> str:
        """ã‚¿ã‚¹ã‚¯ã®è¤‡é›‘åº¦ã‚’æ¨å®š"""
        task_lower = task_title.lower()
        
        for complexity, keywords in self.complexity_indicators.items():
            for keyword in keywords:
                if keyword in task_lower:
                    return complexity
        
        return "MEDIUM"

    def _estimate_task_difficulty(self, task_title: str, category: str) -> float:
        """ã‚¿ã‚¹ã‚¯ã®é›£æ˜“åº¦ã‚’æ¨å®šï¼ˆ0.0-1.0ï¼‰"""
        # åŸºæœ¬é›£æ˜“åº¦ï¼ˆã‚«ãƒ†ã‚´ãƒªãƒ™ãƒ¼ã‚¹ï¼‰
        category_difficulty = {
            'ãƒ†ã‚¹ãƒˆã‚³ãƒ¼ãƒ‰': 0.8,
            'ãƒªãƒ•ã‚¡ã‚¯ã‚¿ãƒªãƒ³ã‚°': 0.7,
            'ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æ”¹å–„': 0.6,
            'æ©Ÿèƒ½è¿½åŠ ': 0.5,
            'ãƒã‚°ä¿®æ­£': 0.4,
            'ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£': 0.7,
            'è¨­å®šãƒ»ç’°å¢ƒ': 0.3,
            'å˜ç´”ä¿®æ­£': 0.2,
            'ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆ': 0.3,
            'ãã®ä»–': 0.5
        }
        
        base_difficulty = category_difficulty.get(category, 0.5)
        
        # ã‚¿ã‚¤ãƒˆãƒ«ã®é•·ã•ã«ã‚ˆã‚‹èª¿æ•´
        title_length_factor = min(len(task_title.split()) / 10, 0.3)
        
        # æŠ€è¡“çš„ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã«ã‚ˆã‚‹èª¿æ•´
        technical_keywords = ['grpc', 'api', 'protocol', 'database', 'integration', 'migration']
        tech_factor = sum(0.1 for keyword in technical_keywords if keyword in task_title.lower())
        
        difficulty = min(base_difficulty + title_length_factor + tech_factor, 1.0)
        return difficulty

    def analyze_comprehensive_metrics(self) -> Dict[str, Any]:
        """åŒ…æ‹¬çš„ãƒ¡ãƒˆãƒªã‚¯ã‚¹åˆ†æ"""
        self.logger.info("ğŸ“Š åŒ…æ‹¬çš„ãƒ¡ãƒˆãƒªã‚¯ã‚¹åˆ†æé–‹å§‹")
        
        df = pd.DataFrame([{
            'experiment_id': m.experiment_id,
            'project': m.project,
            'task_title': m.task_title,
            'category': m.category,
            'parser_success_rate': m.parser_success_rate,
            'workflow_compliance': m.workflow_compliance,
            'system_health': m.system_health,
            'prompt_tokens': m.prompt_tokens,
            'completion_tokens': m.completion_tokens,
            'total_tokens': m.total_tokens,
            'recommendations_count': m.recommendations_count,
            'critical_issues_count': m.critical_issues_count,
            'task_complexity': m.task_complexity,
            'estimated_difficulty': m.estimated_difficulty
        } for m in self.metrics])
        
        analysis = {}
        
        # 1. ã‚«ãƒ†ã‚´ãƒªåˆ¥çµ±è¨ˆ
        category_stats = df.groupby('category').agg({
            'parser_success_rate': ['count', 'mean', 'std'],
            'workflow_compliance': 'mean',
            'total_tokens': ['mean', 'std'],
            'recommendations_count': 'mean',
            'critical_issues_count': 'mean',
            'estimated_difficulty': 'mean'
        }).round(3)
        
        analysis['category_statistics'] = category_stats
        
        # 2. ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆåˆ¥çµ±è¨ˆ
        project_stats = df.groupby('project').agg({
            'parser_success_rate': 'mean',
            'total_tokens': 'mean',
            'estimated_difficulty': 'mean'
        }).round(3)
        
        analysis['project_statistics'] = project_stats
        
        # 3. è¤‡é›‘åº¦åˆ¥çµ±è¨ˆ
        complexity_stats = df.groupby('task_complexity').agg({
            'parser_success_rate': 'mean',
            'total_tokens': 'mean',
            'recommendations_count': 'mean'
        }).round(3)
        
        analysis['complexity_statistics'] = complexity_stats
        
        # 4. ç›¸é–¢åˆ†æ
        correlation_matrix = df[['parser_success_rate', 'total_tokens', 'recommendations_count', 
                               'critical_issues_count', 'estimated_difficulty']].corr().round(3)
        analysis['correlation_matrix'] = correlation_matrix
        
        return analysis

    def generate_enhanced_visualizations(self, output_dir: Path) -> None:
        """æ‹¡å¼µå¯è¦–åŒ–ã‚°ãƒ©ãƒ•ã‚’ç”Ÿæˆ"""
        self.logger.info(f"ğŸ“ˆ æ‹¡å¼µå¯è¦–åŒ–ã‚°ãƒ©ãƒ•ç”Ÿæˆé–‹å§‹: {output_dir}")
        
        output_dir.mkdir(exist_ok=True)
        
        # ã‚¹ã‚¿ã‚¤ãƒ«è¨­å®š
        plt.style.use('seaborn-v0_8')
        
        # ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ ä½œæˆ
        df = pd.DataFrame([{
            'project': m.project,
            'category': m.category,
            'parser_success_rate': m.parser_success_rate,
            'total_tokens': m.total_tokens,
            'recommendations_count': m.recommendations_count,
            'estimated_difficulty': m.estimated_difficulty,
            'task_complexity': m.task_complexity
        } for m in self.metrics])
        
        # 1. ã‚«ãƒ†ã‚´ãƒªåˆ¥ãƒˆãƒ¼ã‚¯ãƒ³ä½¿ç”¨é‡
        plt.figure(figsize=(14, 8))
        sns.boxplot(data=df, x='category', y='total_tokens')
        plt.title('Token Usage Distribution by Task Category', fontsize=16, fontweight='bold')
        plt.xticks(rotation=45, ha='right')
        plt.tight_layout()
        plt.savefig(output_dir / 'category_token_usage.png', dpi=300, bbox_inches='tight')
        plt.close()
        
        # 2. æ¨å®šé›£æ˜“åº¦ vs ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹
        plt.figure(figsize=(12, 8))
        scatter = plt.scatter(df['estimated_difficulty'], df['parser_success_rate'], 
                            c=df['total_tokens'], s=60, alpha=0.7, cmap='viridis')
        plt.colorbar(scatter, label='Total Tokens')
        plt.xlabel('Estimated Task Difficulty')
        plt.ylabel('Parser Success Rate')
        plt.title('Task Difficulty vs Performance (Token Usage)', fontsize=16, fontweight='bold')
        plt.tight_layout()
        plt.savefig(output_dir / 'difficulty_vs_performance.png', dpi=300, bbox_inches='tight')
        plt.close()
        
        # 3. ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆåˆ¥æ¨å¥¨äº‹é …æ•°
        plt.figure(figsize=(12, 6))
        project_recommendations = df.groupby('project')['recommendations_count'].mean().sort_values(ascending=True)
        project_recommendations.plot(kind='barh', color='lightgreen')
        plt.title('Average Recommendations Count by Project', fontsize=16, fontweight='bold')
        plt.xlabel('Average Recommendations Count')
        plt.tight_layout()
        plt.savefig(output_dir / 'project_recommendations.png', dpi=300, bbox_inches='tight')
        plt.close()
        
        # 4. è¤‡é›‘åº¦åˆ¥ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹åˆ†å¸ƒ
        plt.figure(figsize=(10, 6))
        complexity_order = ['LOW', 'MEDIUM', 'HIGH']
        df_complexity = df[df['task_complexity'].isin(complexity_order)]
        sns.boxplot(data=df_complexity, x='task_complexity', y='parser_success_rate', order=complexity_order)
        plt.title('Performance Distribution by Task Complexity', fontsize=16, fontweight='bold')
        plt.ylabel('Parser Success Rate')
        plt.tight_layout()
        plt.savefig(output_dir / 'complexity_performance.png', dpi=300, bbox_inches='tight')
        plt.close()
        
        self.logger.info("âœ… æ‹¡å¼µå¯è¦–åŒ–ã‚°ãƒ©ãƒ•ç”Ÿæˆå®Œäº†")

    def generate_enhanced_report(self, output_dir: Path) -> None:
        """æ‹¡å¼µãƒ¬ãƒãƒ¼ãƒˆã‚’ç”Ÿæˆ"""
        self.logger.info("ğŸ“„ æ‹¡å¼µãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆé–‹å§‹")
        
        analysis = self.analyze_comprehensive_metrics()
        
        report = []
        report.append("# Enhanced APR System Analysis Report")
        report.append(f"**åˆ†ææ—¥æ™‚**: {pd.Timestamp.now().strftime('%Y-%m-%d %H:%M:%S')}")
        report.append(f"**ç·è©•ä¾¡ä»¶æ•°**: {len(self.metrics):,}")
        report.append("")
        
        # ã‚«ãƒ†ã‚´ãƒªåˆ¥è©³ç´°çµ±è¨ˆ
        report.append("## ã‚«ãƒ†ã‚´ãƒªåˆ¥è©³ç´°çµ±è¨ˆ")
        report.append("")
        report.append("| ã‚«ãƒ†ã‚´ãƒª | ä»¶æ•° | æˆåŠŸç‡ | å¹³å‡ãƒˆãƒ¼ã‚¯ãƒ³ | æ¨å¥¨äº‹é … | æ¨å®šé›£æ˜“åº¦ |")
        report.append("|---------|------|--------|-------------|----------|-----------|")
        
        category_stats = analysis['category_statistics']
        for category in category_stats.index:
            count = category_stats.loc[category, ('parser_success_rate', 'count')]
            success_rate = category_stats.loc[category, ('parser_success_rate', 'mean')]
            avg_tokens = category_stats.loc[category, ('total_tokens', 'mean')]
            avg_recommendations = category_stats.loc[category, ('recommendations_count', 'mean')]
            avg_difficulty = category_stats.loc[category, ('estimated_difficulty', 'mean')]
            
            report.append(f"| {category} | {count} | {success_rate:.3f} | {avg_tokens:.0f} | {avg_recommendations:.1f} | {avg_difficulty:.3f} |")
        
        report.append("")
        
        # ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆåˆ¥çµ±è¨ˆ
        report.append("## ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆåˆ¥çµ±è¨ˆ")
        report.append("")
        report.append("| ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆ | æˆåŠŸç‡ | å¹³å‡ãƒˆãƒ¼ã‚¯ãƒ³ | æ¨å®šé›£æ˜“åº¦ |")
        report.append("|-------------|--------|-------------|-----------|")
        
        project_stats = analysis['project_statistics']
        for project in project_stats.index:
            success_rate = project_stats.loc[project, 'parser_success_rate']
            avg_tokens = project_stats.loc[project, 'total_tokens']
            avg_difficulty = project_stats.loc[project, 'estimated_difficulty']
            
            report.append(f"| {project} | {success_rate:.3f} | {avg_tokens:.0f} | {avg_difficulty:.3f} |")
        
        report.append("")
        
        # ä¸»è¦ãªç™ºè¦‹
        report.append("## ä¸»è¦ãªç™ºè¦‹")
        report.append("")
        
        df = pd.DataFrame([{
            'category': m.category,
            'total_tokens': m.total_tokens,
            'estimated_difficulty': m.estimated_difficulty
        } for m in self.metrics])
        
        # æœ€ã‚‚ãƒˆãƒ¼ã‚¯ãƒ³ã‚’æ¶ˆè²»ã™ã‚‹ã‚«ãƒ†ã‚´ãƒª
        avg_tokens_by_category = df.groupby('category')['total_tokens'].mean()
        most_tokens_category = avg_tokens_by_category.idxmax()
        most_tokens_value = avg_tokens_by_category.max()
        
        # æœ€ã‚‚å›°é›£ãªã‚«ãƒ†ã‚´ãƒª
        avg_difficulty_by_category = df.groupby('category')['estimated_difficulty'].mean()
        most_difficult_category = avg_difficulty_by_category.idxmax()
        most_difficult_value = avg_difficulty_by_category.max()
        
        report.append(f"- **æœ€ã‚‚ãƒˆãƒ¼ã‚¯ãƒ³ã‚’æ¶ˆè²»ã™ã‚‹ã‚«ãƒ†ã‚´ãƒª**: {most_tokens_category} ({most_tokens_value:.0f} tokens)")
        report.append(f"- **æœ€ã‚‚å›°é›£ãªã‚«ãƒ†ã‚´ãƒª**: {most_difficult_category} (é›£æ˜“åº¦: {most_difficult_value:.3f})")
        report.append("")
        
        # ãƒ¬ãƒãƒ¼ãƒˆä¿å­˜
        with open(output_dir / 'enhanced_analysis_report.md', 'w', encoding='utf-8') as f:
            f.write('\n'.join(report))
        
        self.logger.info("âœ… æ‹¡å¼µãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆå®Œäº†")


def main():
    """ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œé–¢æ•°"""
    import argparse
    
    parser = argparse.ArgumentParser(description='Enhanced APR Dataset Analysis Tool')
    parser.add_argument('session_path', help='è©•ä¾¡ã‚»ãƒƒã‚·ãƒ§ãƒ³ã®ãƒ‘ã‚¹')
    parser.add_argument('--output', '-o', default='./enhanced_analysis_output', help='å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª')
    
    args = parser.parse_args()
    
    # æ‹¡å¼µåˆ†æå™¨åˆæœŸåŒ–
    analyzer = EnhancedAPRAnalyzer(args.session_path)
    
    # å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªæº–å‚™
    output_dir = Path(args.output)
    output_dir.mkdir(exist_ok=True)
    
    try:
        # ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿
        analyzer.load_enhanced_data()
        
        # åˆ†æå®Ÿè¡Œ
        analyzer.generate_enhanced_visualizations(output_dir)
        analyzer.generate_enhanced_report(output_dir)
        
        print(f"ğŸ‰ æ‹¡å¼µåˆ†æå®Œäº†ï¼çµæœã¯ {output_dir} ã«ä¿å­˜ã•ã‚Œã¾ã—ãŸ")
        
    except Exception as e:
        print(f"âŒ æ‹¡å¼µåˆ†æã‚¨ãƒ©ãƒ¼: {e}")
        raise


if __name__ == "__main__":
    main()
