# APRè©•ä¾¡ã‚·ã‚¹ãƒ†ãƒ  - ã‚¯ã‚¤ãƒƒã‚¯ã‚¹ã‚¿ãƒ¼ãƒˆã‚¬ã‚¤ãƒ‰

## ğŸ“‹ æ¦‚è¦

ã“ã®ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã«ã¯ã€LLMãƒ™ãƒ¼ã‚¹ã®APR (Automatic Program Repair) ã‚·ã‚¹ãƒ†ãƒ ã®åŒ…æ‹¬çš„ãªè©•ä¾¡æ©Ÿèƒ½ãŒå«ã¾ã‚Œã¦ã„ã¾ã™ã€‚2æ®µéšè©•ä¾¡ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯ã«ã‚ˆã‚Šã€ã‚·ã‚¹ãƒ†ãƒ ã®é©åˆæ€§ã¨ãƒ‘ãƒƒãƒå“è³ªã‚’å®šé‡çš„ãƒ»å®šæ€§çš„ã«è©•ä¾¡ã—ã¾ã™ã€‚

## ğŸš€ ã‚¯ã‚¤ãƒƒã‚¯ã‚¹ã‚¿ãƒ¼ãƒˆ

### 1. ç’°å¢ƒç¢ºèª
```bash
cd /app/.docker
./manage.sh eval-setup
```

### 2. ãƒ†ã‚¹ãƒˆå®Ÿè¡Œ
```bash
./manage.sh eval-dry-run
```

### 3. å°è¦æ¨¡è©•ä¾¡
```bash
./manage.sh eval-run --mode compliance --limit 10
```

### 4. å®Œå…¨è©•ä¾¡
```bash
./manage.sh eval-run --mode full
```

## ğŸ“ ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªæ§‹é€ 

```
## ğŸ“ ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªæ§‹é€ 

```text
/app/
â”œâ”€â”€ main.py                        # ãƒ¡ã‚¤ãƒ³ã‚¨ãƒ³ãƒˆãƒªãƒ¼ãƒã‚¤ãƒ³ãƒˆ
â”œâ”€â”€ src/                           # ã‚³ã‚¢ãƒ©ã‚¤ãƒ–ãƒ©ãƒª
â”‚   â”œâ”€â”€ cli/                       # ã‚³ãƒãƒ³ãƒ‰ãƒ©ã‚¤ãƒ³ãƒ„ãƒ¼ãƒ«
â”‚   â”‚   â”œâ”€â”€ real_llm_evaluator.py  # ãƒ¡ã‚¤ãƒ³è©•ä¾¡ã‚¨ãƒ³ã‚¸ãƒ³
â”‚   â”‚   â”œâ”€â”€ evaluation_log_viewer.py # ãƒ­ã‚°è©³ç´°è¡¨ç¤º
â”‚   â”‚   â”œâ”€â”€ llm_response_viewer.py  # LLMå¿œç­”è©³ç´°è¡¨ç¤º
â”‚   â”‚   â”œâ”€â”€ verification_report.py  # ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ
â”‚   â”‚   â”œâ”€â”€ verify_single_repo.py   # å˜ä¸€ãƒªãƒæ¤œè¨¼
â”‚   â”‚   â”œâ”€â”€ run_full_evaluation.sh  # å…¨ãƒ­ã‚°è©•ä¾¡ï¼ˆæ¨å¥¨ï¼‰
â”‚   â”‚   â””â”€â”€ run_real_llm_evaluation.sh # å­¦ç¿’ãƒ»ãƒ†ã‚¹ãƒˆç”¨
â”‚   â”œâ”€â”€ analysis/                  # åˆ†æãƒ„ãƒ¼ãƒ«
â”‚   â”‚   â”œâ”€â”€ analyze_evaluation_results.py # è©•ä¾¡çµæœåˆ†æ
â”‚   â”‚   â”œâ”€â”€ detailed_analysis.py    # è©³ç´°åˆ†æ
â”‚   â”‚   â”œâ”€â”€ full_dataset_evaluation.py # å¤§è¦æ¨¡ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆè©•ä¾¡
â”‚   â”‚   â”œâ”€â”€ apr_dataset_analyzer.py # APRãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆåˆ†æ
â”‚   â”‚   â”œâ”€â”€ enhanced_apr_analyzer.py # æ‹¡å¼µAPRåˆ†æ
â”‚   â”‚   â””â”€â”€ task_category_classifier.py # ã‚¿ã‚¹ã‚¯åˆ†é¡
â”‚   â”œâ”€â”€ demo/                      # ãƒ‡ãƒ¢ãƒ³ã‚¹ãƒˆãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³
â”‚   â”‚   â””â”€â”€ demo_openai_models.py   # OpenAIæ¥ç¶šãƒ†ã‚¹ãƒˆ
â”‚   â”œâ”€â”€ evaluators/                # è©•ä¾¡å™¨
â”‚   â”‚   â”œâ”€â”€ compliance_evaluator.py # ã‚·ã‚¹ãƒ†ãƒ é©åˆæ€§è©•ä¾¡
â”‚   â”‚   â”œâ”€â”€ quality_evaluator.py    # å“è³ªè©•ä¾¡
â”‚   â”‚   â””â”€â”€ integrated_evaluator.py # çµ±åˆè©•ä¾¡
â”‚   â”œâ”€â”€ llm/                       # LLMçµ±åˆ
â”‚   â”‚   â””â”€â”€ llm_integration.py      # LLMæ¥ç¶šãƒ»ç®¡ç†
â”‚   â”œâ”€â”€ utils/                     # ãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£
â”‚   â”‚   â”œâ”€â”€ log_parser.py           # ãƒ­ã‚°è§£æ
â”‚   â”‚   â”œâ”€â”€ log_iterator.py         # ãƒ­ã‚°åå¾©å‡¦ç†
â”‚   â”‚   â””â”€â”€ template_manager.py     # ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆç®¡ç†
â”‚   â””â”€â”€ tests/                     # ãƒ†ã‚¹ãƒˆã‚³ãƒ¼ãƒ‰
â”‚   â”œâ”€â”€ evaluators/
â”‚   â”œâ”€â”€ analyzers/
â”‚   â”œâ”€â”€ llm/
â”‚   â”œâ”€â”€ reporters/
â”‚   â”œâ”€â”€ utils/
â”‚   â””â”€â”€ tests/
â”œâ”€â”€ logs/                          # è©•ä¾¡ãƒ­ã‚°
â”‚   â”œâ”€â”€ evaluation_results_*.json
â”‚   â”œâ”€â”€ error_reports_*.json
â”‚   â””â”€â”€ processing_summaries_*.json
â”œâ”€â”€ apr-logs/                      # APRãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ
â”œâ”€â”€ apr-output/                    # è©•ä¾¡å‡ºåŠ›
â”œâ”€â”€ config/                        # è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«
â”œâ”€â”€ dataset/                       # ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ
â”œâ”€â”€ results/                       # è©•ä¾¡çµæœ
â””â”€â”€ docs/                          # ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆ
    â”œâ”€â”€ HANDOVER_DOCUMENTATION.md  # å¼•ãç¶™ããƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆï¼ˆé‡è¦ï¼‰
    â”œâ”€â”€ DEVELOPMENT_GUIDE.md       # é–‹ç™ºè€…ã‚¬ã‚¤ãƒ‰
    â”œâ”€â”€ API_SPECIFICATION.md       # APIä»•æ§˜æ›¸
    â”œâ”€â”€ SYSTEM_SPECIFICATION.md    # ã‚·ã‚¹ãƒ†ãƒ ä»•æ§˜æ›¸
    â”œâ”€â”€ SYSTEM_FILE_DOCUMENTATION.md # ãƒ•ã‚¡ã‚¤ãƒ«æ§‹æˆèª¬æ˜
    â””â”€â”€ FILE_DEPENDENCY_DIAGRAM.md # ä¾å­˜é–¢ä¿‚å›³
```
â”œâ”€â”€ apr-logs/                      # å…¥åŠ›APRãƒ­ã‚°ï¼ˆ86ä»¶ï¼‰
â”œâ”€â”€ logs/                          # è©•ä¾¡ãƒ­ã‚°å‡ºåŠ›
â”œâ”€â”€ output/
â”‚   â””â”€â”€ verification_results/          # è©•ä¾¡çµæœ
â”œâ”€â”€ config/                        # è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«
â”œâ”€â”€ evaluation-design/             # è©•ä¾¡è¨­è¨ˆ
â”œâ”€â”€ Dockerfile_evaluation          # Dockerè¨­å®š
â””â”€â”€ requirements_evaluation.txt    # Pythonä¾å­˜é–¢ä¿‚
â”‚   â”œâ”€â”€ utils/
â”‚   â””â”€â”€ tests/
â”œâ”€â”€ config/                        # è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«
â”œâ”€â”€ results/                       # è©•ä¾¡çµæœå‡ºåŠ›
â”œâ”€â”€ logs/                          # ãƒ­ã‚°ãƒ•ã‚¡ã‚¤ãƒ«
â”œâ”€â”€ apr-logs/                      # APRãƒ­ã‚°ï¼ˆèª­ã¿å–ã‚Šå°‚ç”¨ï¼‰
â”œâ”€â”€ apr-output/                    # APRçµæœï¼ˆèª­ã¿å–ã‚Šå°‚ç”¨ï¼‰
â””â”€â”€ dataset/                       # ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆï¼ˆèª­ã¿å–ã‚Šå°‚ç”¨ï¼‰
```

## âš™ï¸ ä¸»è¦ã‚³ãƒãƒ³ãƒ‰

### ğŸ¯ çµ±åˆã‚³ãƒãƒ³ãƒ‰ãƒ©ã‚¤ãƒ³

```bash
# APRè©•ä¾¡å®Ÿè¡Œ
python main.py evaluate --repo boulder --max-logs 3 --provider openai --model gpt-4.1-mini

# çµæœåˆ†æ
python main.py analyze --input-dir /app/output/verification_results

# ãƒ­ã‚°è¡¨ç¤º
python main.py view --log-file /app/logs/evaluation.log

# ãƒ‡ãƒ¢å®Ÿè¡Œ
python main.py demo --model gpt-4.1-mini
```

### ğŸ”¥ å…¨ãƒ­ã‚°è©•ä¾¡ï¼ˆæ¨å¥¨ï¼‰

**ã™ã¹ã¦ã®APRãƒ­ã‚°ï¼ˆ86ãƒ­ã‚°ï¼‰ã‚’è©•ä¾¡ã—ãŸã„å ´åˆ**:

```bash
# å…¨ãƒ­ã‚°ä¸€æ‹¬è©•ä¾¡ï¼ˆæ¨å¥¨ï¼‰
bash src/cli/run_full_evaluation.sh

# ã¾ãŸã¯å€‹åˆ¥ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆè©•ä¾¡
python src/cli/real_llm_evaluator.py --repo boulder --max-logs 999 --provider openai --model gpt-4.1-mini
python src/cli/real_llm_evaluator.py --repo daos --max-logs 999 --provider openai --model gpt-4.1-mini
# ...å…¨11ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆ
```

**å­¦ç¿’ãƒ»ãƒ†ã‚¹ãƒˆç”¨ã‚¬ã‚¤ãƒ‰**:

```bash
bash scripts/run_real_llm_evaluation.sh
```

**è©•ä¾¡ãƒ¢ãƒ¼ãƒ‰é¸æŠ**:

- ğŸ­ **Mock LLM**: ãƒ†ã‚¹ãƒˆç”¨ãƒ»é«˜é€Ÿãƒ»ç„¡æ–™
- ğŸ’° **gpt-4.1-mini**: é«˜å“è³ªãƒ»ä½ã‚³ã‚¹ãƒˆï¼ˆå…¨86ãƒ­ã‚°ã§ç´„$0.15ï¼‰
- ğŸ¯ **gpt-4.1**: æœ€é«˜å“è³ªãƒ»é«˜ã‚³ã‚¹ãƒˆï¼ˆå…¨86ãƒ­ã‚°ã§ç´„$3.87ï¼‰

### manage.sh ã‚¹ã‚¯ãƒªãƒ—ãƒˆä½¿ç”¨

```bash
# ã‚³ãƒ³ãƒ†ãƒŠãƒ“ãƒ«ãƒ‰
./manage.sh build

# è©•ä¾¡å®Ÿè¡Œ
./manage.sh eval-run

# ãƒ‰ãƒ©ã‚¤ãƒ©ãƒ³ï¼ˆè¨­å®šç¢ºèªï¼‰
./manage.sh eval-dry-run

# ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—ç¢ºèª
./manage.sh eval-setup

# ãƒ‡ãƒ¼ã‚¿ãƒªã‚»ãƒƒãƒˆ
./manage.sh reset

# ãƒ­ã‚°ç¢ºèª
./manage.sh logs evaluation-system
```

### ç›´æ¥å®Ÿè¡Œï¼ˆã‚³ãƒ³ãƒ†ãƒŠå†…ï¼‰

```bash
# ã‚³ãƒ³ãƒ†ãƒŠã«å…¥ã‚‹
docker exec -it grpc-analyzer-evaluation bash

# åŸºæœ¬è©•ä¾¡
python main.py --mode full

# ç‰¹å®šãƒ¢ãƒ¼ãƒ‰
python main.py --mode compliance --project servantes
python main.py --mode quality --dataset filtered_confirmed

# ãƒ‡ãƒãƒƒã‚°ãƒ¢ãƒ¼ãƒ‰
python main.py --debug --verbose --log-level DEBUG
```

## ğŸ¯ è©•ä¾¡ãƒ¢ãƒ¼ãƒ‰

| ãƒ¢ãƒ¼ãƒ‰ | èª¬æ˜ | å®Ÿè¡Œæ™‚é–“ |
|--------|------|----------|
| `setup` | ç’°å¢ƒç¢ºèªãƒ»åˆæœŸè¨­å®š | 1-2åˆ† |
| `compliance` | ã‚·ã‚¹ãƒ†ãƒ é©åˆæ€§è©•ä¾¡ã®ã¿ | 5-10åˆ† |
| `quality` | ãƒ‘ãƒƒãƒå“è³ªè©•ä¾¡ã®ã¿ | 15-30åˆ† |
| `full` | å®Œå…¨è©•ä¾¡ï¼ˆä¸¡æ®µéšï¼‰ | 20-40åˆ† |
| `dry-run` | è¨­å®šç¢ºèªï¼ˆå®Ÿéš›ã®è©•ä¾¡ãªã—ï¼‰ | 1åˆ† |

## ğŸ“Š å‡ºåŠ›çµæœ

### è©•ä¾¡çµæœ
- **å ´æ‰€**: `/app/results/`
- **å½¢å¼**: JSON, CSV, HTML
- **å†…å®¹**: è©³ç´°ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã€çµ±è¨ˆã‚µãƒãƒªãƒ¼

### å¯è¦–åŒ–
- **å ´æ‰€**: `/app/results/charts/`
- **å½¢å¼**: PNG, PDF
- **å†…å®¹**: çµ±è¨ˆã‚°ãƒ©ãƒ•ã€æ¯”è¼ƒãƒãƒ£ãƒ¼ãƒˆ

## âš ï¸ é‡è¦ãªè¨­å®š

### ç’°å¢ƒå¤‰æ•°
```bash
PYTHONPATH=/app                    # å¿…é ˆ
EVALUATION_MODE=production         # å¿…é ˆ
OPENAI_API_KEY=your_key_here      # OpenAIä½¿ç”¨æ™‚
ANTHROPIC_API_KEY=your_key_here   # Anthropicä½¿ç”¨æ™‚
```

### ãƒ›ã‚¹ãƒˆãƒ‘ã‚¹ãƒãƒƒãƒ”ãƒ³ã‚°
```
F:/Workspace/gRPC_Analyzer/evaluation â†’ /app/
F:/Workspace/gRPC_Analyzer/dataset   â†’ /app/dataset
F:/Workspace/gRPC_Analyzer/log       â†’ /app/apr-logs
F:/Workspace/gRPC_Analyzer/output    â†’ /app/apr-output
```

## ğŸ†˜ ãƒˆãƒ©ãƒ–ãƒ«ã‚·ãƒ¥ãƒ¼ãƒ†ã‚£ãƒ³ã‚°

### ã‚ˆãã‚ã‚‹å•é¡Œ

1. **ãƒ‘ã‚¹é–¢é€£ã‚¨ãƒ©ãƒ¼**
   ```bash
   # è§£æ±ºç­–: docker-compose.ymlã®ãƒœãƒªãƒ¥ãƒ¼ãƒ ãƒãƒƒãƒ”ãƒ³ã‚°ç¢ºèª
   ERROR: Required directory not found: /app/apr-logs
   ```

2. **LLM API ã‚¨ãƒ©ãƒ¼**
   ```bash
   # è§£æ±ºç­–: API ã‚­ãƒ¼ç¢ºèªã€ãƒ¬ãƒ¼ãƒˆåˆ¶é™ã®èª¿æ•´
   OpenAI API rate limit exceeded
   ```

3. **ãƒ¡ãƒ¢ãƒªä¸è¶³**
   ```bash
   # è§£æ±ºç­–: ãƒãƒƒãƒã‚µã‚¤ã‚ºã‚’å°ã•ãè¨­å®š
   MemoryError during batch processing
   ```

### ãƒ­ã‚°ç¢ºèª
```bash
# è©•ä¾¡ãƒ­ã‚°
./manage.sh logs evaluation-system

# ã‚¨ãƒ©ãƒ¼æ¤œç´¢
docker exec grpc-analyzer-evaluation grep -r "ERROR" /app/logs/

# ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ç¢ºèª
docker exec grpc-analyzer-evaluation ls -la /app/logs/performance/
```

## ğŸ“š è©³ç´°ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆ

- **ğŸ¯ æ–°è¦å¼•ãç¶™ã**: `HANDOVER_DOCUMENTATION.md` - **å¿…èª­**
- **ğŸ› ï¸ é–‹ç™ºç¶™ç¶š**: `DEVELOPMENT_GUIDE.md`
- **ğŸ“š APIä½¿ç”¨**: `API_SPECIFICATION.md`
- **ğŸ“Š ä»•æ§˜è©³ç´°**: `SYSTEM_SPECIFICATION.md`
- **ğŸ¨ è©•ä¾¡è¨­è¨ˆ**: `evaluation-design/`

## ğŸš€ å§‹ã‚ã¦ã¿ã‚ˆã†

**æ–°è¦å¼•ãç¶™ãè€…ã®æ–¹ã¸**:

1. **å¿…èª­ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆ**: `HANDOVER_DOCUMENTATION.md`
2. **ç’°å¢ƒç¢ºèª**: `./manage.sh eval-setup`
3. **ãƒ†ã‚¹ãƒˆå®Ÿè¡Œ**: `./manage.sh eval-dry-run`
4. **å°è¦æ¨¡è©•ä¾¡**: `./manage.sh eval-run --mode compliance --limit 5`

**é–‹ç™ºè€…ã®æ–¹ã¸**:

1. **é–‹ç™ºã‚¬ã‚¤ãƒ‰**: `DEVELOPMENT_GUIDE.md`
2. **APIä»•æ§˜**: `API_SPECIFICATION.md`
3. **ãƒ†ã‚¹ãƒˆ**: `python -m pytest src/tests/ -v`

---

**ä½œæˆæ—¥**: 2025-07-22  
**ãƒãƒ¼ã‚¸ãƒ§ãƒ³**: 1.0  
**ã‚·ã‚¹ãƒ†ãƒ **: gRPC_Analyzer APRè©•ä¾¡ã‚·ã‚¹ãƒ†ãƒ 
cd /app/.docker
./manage.sh build

# è©•ä¾¡å®Ÿè¡Œ
./manage.sh eval-all

# è©•ä¾¡ã‚·ã‚¹ãƒ†ãƒ ã‚·ã‚§ãƒ«
./manage.sh shell evaluation-system
```

### è©•ä¾¡ã‚·ã‚¹ãƒ†ãƒ å†…ã§ç›´æ¥å®Ÿè¡Œ
```bash
# å…¨è©•ä¾¡å®Ÿè¡Œ
python main.py --step all

# å€‹åˆ¥è©•ä¾¡
python main.py --step 1  # ã‚·ã‚¹ãƒ†ãƒ ä»•æ§˜æº–æ‹ æ€§
python main.py --step 2  # ãƒ‘ãƒƒãƒå“è³ª

# ç’°å¢ƒãƒã‚§ãƒƒã‚¯
python main.py --dry-run
```

## ğŸ“Š è©•ä¾¡ãƒ•ãƒ­ãƒ¼

1. **ã‚¹ãƒ†ãƒƒãƒ—1**: ã‚·ã‚¹ãƒ†ãƒ ä»•æ§˜æº–æ‹ æ€§è©•ä¾¡
   - APRãƒ­ã‚°ï¼ˆ`/app/apr-logs/`ï¼‰ã‚’åˆ†æ
   - åˆ¶å¾¡ãƒ•ãƒ­ãƒ¼ã€ãƒ‘ãƒ¼ã‚µãƒ¼ç²¾åº¦ã€ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°ã‚’è©•ä¾¡
   - çµæœ: `/app/results/step1/`

2. **ã‚¹ãƒ†ãƒƒãƒ—2**: ãƒ‘ãƒƒãƒå“è³ªè©•ä¾¡
   - ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆï¼ˆ`/app/dataset/`ï¼‰ã¨APRçµæœï¼ˆ`/app/apr-output/`ï¼‰ã‚’æ¯”è¼ƒ
   - Plausibilityã€Correctnessã€Reasoning Qualityã‚’è©•ä¾¡
   - çµæœ: `/app/results/step2/`

3. **çµ±åˆãƒ¬ãƒãƒ¼ãƒˆ**: åŒ…æ‹¬çš„ãªè©•ä¾¡ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ
   - çµæœ: `/app/results/comprehensive_*.json`

## ğŸ”§ è¨­å®š

### ç’°å¢ƒå¤‰æ•°
- `PYTHONPATH=/app`
- `EVALUATION_MODE=production`

### å…¥åŠ›ãƒ‡ãƒ¼ã‚¿
- **APRãƒ­ã‚°**: `/app/apr-logs/` (F:\Workspace\gRPC_Analyzer\log)
- **APRçµæœ**: `/app/apr-output/` (F:\Workspace\gRPC_Analyzer\output)
- **ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ**: `/app/dataset/` (F:\Workspace\gRPC_Analyzer\dataset)

### å‡ºåŠ›ãƒ‡ãƒ¼ã‚¿
- **è©•ä¾¡çµæœ**: `/app/results/` (F:\Workspace\gRPC_Analyzer\evaluation\results)

## ğŸ› ï¸ é–‹ç™º

### ãƒ­ãƒ¼ã‚«ãƒ«é–‹ç™º
```bash
# è©•ä¾¡ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã§ä½œæ¥­
cd /app/evaluation

# ä¾å­˜é–¢ä¿‚ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«
pip install -r requirements_evaluation.txt

# ãƒ†ã‚¹ãƒˆå®Ÿè¡Œ
python main.py --dry-run
```

### æ–°ã—ã„è©•ä¾¡å™¨è¿½åŠ 
1. `step3_*.py` ãªã©ã®æ–°ã—ã„ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ä½œæˆ
2. `main.py` ã«çµ±åˆ
3. ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆè¿½åŠ 
