# APRè©•ä¾¡ã‚·ã‚¹ãƒ†ãƒ  é–‹ç™ºã‚¬ã‚¤ãƒ‰

## ğŸ› ï¸ é–‹ç™ºè€…å‘ã‘è©³ç´°ã‚¬ã‚¤ãƒ‰

### ğŸ“‹ ç›®æ¬¡
1. [ã‚³ãƒ¼ãƒ‰ãƒ™ãƒ¼ã‚¹æ§‹é€ ](#ã‚³ãƒ¼ãƒ‰ãƒ™ãƒ¼ã‚¹æ§‹é€ )
2. [é–‹ç™ºç’°å¢ƒã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—](#é–‹ç™ºç’°å¢ƒã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—)
3. [å®Ÿè£…è©³ç´°](#å®Ÿè£…è©³ç´°)
4. [ãƒ†ã‚¹ãƒˆæˆ¦ç•¥](#ãƒ†ã‚¹ãƒˆæˆ¦ç•¥)
5. [ãƒ‡ãƒãƒƒã‚°ãƒ»ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒªãƒ³ã‚°](#ãƒ‡ãƒãƒƒã‚°ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒªãƒ³ã‚°)
6. [æ–°æ©Ÿèƒ½è¿½åŠ ã‚¬ã‚¤ãƒ‰](#æ–°æ©Ÿèƒ½è¿½åŠ ã‚¬ã‚¤ãƒ‰)
7. [ã‚³ãƒ¼ãƒ‰å“è³ªç®¡ç†](#ã‚³ãƒ¼ãƒ‰å“è³ªç®¡ç†)

---

## ã‚³ãƒ¼ãƒ‰ãƒ™ãƒ¼ã‚¹æ§‹é€ 

### ãƒ¡ã‚¤ãƒ³ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«æ§‹æˆ

```
src/
â”œâ”€â”€ __init__.py
â”œâ”€â”€ main.py                     # ã‚¨ãƒ³ãƒˆãƒªãƒ¼ãƒã‚¤ãƒ³ãƒˆ
â”œâ”€â”€ evaluators/                 # è©•ä¾¡ã‚¨ãƒ³ã‚¸ãƒ³
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ base_evaluator.py      # åŸºåº•è©•ä¾¡ã‚¯ãƒ©ã‚¹
â”‚   â”œâ”€â”€ compliance_evaluator.py # Step1: ã‚·ã‚¹ãƒ†ãƒ é©åˆæ€§è©•ä¾¡
â”‚   â”œâ”€â”€ quality_evaluator.py   # Step2: ãƒ‘ãƒƒãƒå“è³ªè©•ä¾¡
â”‚   â””â”€â”€ integrated_evaluator.py # çµ±åˆè©•ä¾¡å™¨
â”œâ”€â”€ analyzers/                  # ãƒ‡ãƒ¼ã‚¿è§£æ
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ apr_log_analyzer.py    # APRãƒ­ã‚°è§£æ
â”‚   â”œâ”€â”€ patch_analyzer.py      # ãƒ‘ãƒƒãƒè§£æ
â”‚   â”œâ”€â”€ dataset_analyzer.py    # ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆè§£æ
â”‚   â””â”€â”€ statistical_analyzer.py # çµ±è¨ˆè§£æ
â”œâ”€â”€ llm/                       # LLMçµ±åˆ
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ base_provider.py       # LLMãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼åŸºåº•ã‚¯ãƒ©ã‚¹
â”‚   â”œâ”€â”€ openai_provider.py     # OpenAIçµ±åˆ
â”‚   â”œâ”€â”€ anthropic_provider.py  # Anthropicçµ±åˆ
â”‚   â””â”€â”€ prompt_manager.py      # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆç®¡ç†
â”œâ”€â”€ reporters/                 # ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ json_reporter.py       # JSONãƒ¬ãƒãƒ¼ãƒˆ
â”‚   â”œâ”€â”€ csv_reporter.py        # CSVãƒ¬ãƒãƒ¼ãƒˆ
â”‚   â”œâ”€â”€ html_reporter.py       # HTMLãƒ¬ãƒãƒ¼ãƒˆ
â”‚   â””â”€â”€ chart_generator.py     # å¯è¦–åŒ–
â”œâ”€â”€ utils/                     # ãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ file_handler.py        # ãƒ•ã‚¡ã‚¤ãƒ«æ“ä½œ
â”‚   â”œâ”€â”€ logger.py             # ãƒ­ã‚°ç®¡ç†
â”‚   â”œâ”€â”€ config_manager.py     # è¨­å®šç®¡ç†
â”‚   â””â”€â”€ validation.py         # ãƒ‡ãƒ¼ã‚¿æ¤œè¨¼
â””â”€â”€ tests/                     # ãƒ†ã‚¹ãƒˆã‚¹ã‚¤ãƒ¼ãƒˆ
    â”œâ”€â”€ unit/                  # å˜ä½“ãƒ†ã‚¹ãƒˆ
    â”œâ”€â”€ integration/           # çµ±åˆãƒ†ã‚¹ãƒˆ
    â””â”€â”€ fixtures/              # ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿
```

### ä¸»è¦ã‚¯ãƒ©ã‚¹è¨­è¨ˆ

```python
# åŸºåº•è©•ä¾¡ã‚¯ãƒ©ã‚¹
class BaseEvaluator:
    def __init__(self, config):
        self.config = config
        self.logger = setup_logger()
    
    def evaluate(self, data):
        raise NotImplementedError
    
    def generate_report(self, results):
        raise NotImplementedError

# è©•ä¾¡çµæœãƒ‡ãƒ¼ã‚¿æ§‹é€ 
@dataclass
class EvaluationResult:
    timestamp: datetime
    evaluator_type: str
    project_name: str
    metrics: Dict[str, float]
    details: Dict[str, Any]
    status: str  # 'success', 'partial', 'failed'
```

---

## é–‹ç™ºç’°å¢ƒã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—

### ãƒ­ãƒ¼ã‚«ãƒ«é–‹ç™ºç”¨ã‚³ãƒ³ãƒ†ãƒŠ

```bash
# é–‹ç™ºç”¨ã‚³ãƒ³ãƒ†ãƒŠã®èµ·å‹•
docker-compose -f docker-compose.yml up -d evaluation-system

# ã‚³ãƒ³ãƒ†ãƒŠã«å…¥ã‚‹
docker exec -it grpc-analyzer-evaluation bash

# é–‹ç™ºãƒ¢ãƒ¼ãƒ‰ã§å®Ÿè¡Œ
export EVALUATION_MODE=development
export PYTHONPATH=/app
python -m src.main --debug
```

### IDEè¨­å®šï¼ˆVS Codeï¼‰

`.vscode/settings.json`:
```json
{
    "python.pythonPath": "/usr/local/bin/python",
    "python.linting.enabled": true,
    "python.linting.pylintEnabled": true,
    "python.formatting.provider": "black",
    "python.testing.pytestEnabled": true,
    "python.testing.pytestArgs": ["src/tests"]
}
```

`.vscode/launch.json`:
```json
{
    "configurations": [
        {
            "name": "Debug Evaluation",
            "type": "python",
            "request": "launch",
            "program": "main.py",
            "args": ["--mode", "dry-run", "--debug"],
            "cwd": "/app",
            "env": {
                "PYTHONPATH": "/app",
                "EVALUATION_MODE": "development"
            }
        }
    ]
}
```

---

## å®Ÿè£…è©³ç´°

### Step1: ã‚·ã‚¹ãƒ†ãƒ é©åˆæ€§è©•ä¾¡å™¨

```python
# src/evaluators/compliance_evaluator.py
class ComplianceEvaluator(BaseEvaluator):
    def __init__(self, config):
        super().__init__(config)
        self.metrics = {
            'control_flow_accuracy': 0.0,
            'parser_success_rate': 0.0,
            'file_processing_rate': 0.0,
            'error_handling_score': 0.0
        }
    
    def evaluate_control_flow(self, apr_logs):
        """åˆ¶å¾¡ãƒ•ãƒ­ãƒ¼è§£æã®ç²¾åº¦ã‚’è©•ä¾¡"""
        try:
            # APRãƒ­ã‚°ã‹ã‚‰åˆ¶å¾¡ãƒ•ãƒ­ãƒ¼æƒ…å ±ã‚’æŠ½å‡º
            flow_data = self._extract_control_flow(apr_logs)
            
            # æœŸå¾…ã•ã‚Œã‚‹åˆ¶å¾¡ãƒ•ãƒ­ãƒ¼ã¨æ¯”è¼ƒ
            accuracy = self._calculate_flow_accuracy(flow_data)
            
            return accuracy
        except Exception as e:
            self.logger.error(f"Control flow evaluation failed: {e}")
            return 0.0
    
    def evaluate_parser_accuracy(self, apr_logs):
        """ãƒ‘ãƒ¼ã‚µãƒ¼ã®æ­£ç¢ºæ€§ã‚’è©•ä¾¡"""
        success_count = 0
        total_files = 0
        
        for log_entry in apr_logs:
            if 'parsing' in log_entry:
                total_files += 1
                if log_entry.get('parsing_success', False):
                    success_count += 1
        
        return success_count / total_files if total_files > 0 else 0.0
```

### Step2: ãƒ‘ãƒƒãƒå“è³ªè©•ä¾¡å™¨

```python
# src/evaluators/quality_evaluator.py
class QualityEvaluator(BaseEvaluator):
    def __init__(self, config, llm_provider):
        super().__init__(config)
        self.llm_provider = llm_provider
        self.metrics = {
            'plausibility_score': 0.0,
            'correctness_r1': 0.0,
            'correctness_r5': 0.0,
            'correctness_r10': 0.0,
            'reasoning_quality': 0.0
        }
    
    async def evaluate_patch_quality(self, generated_patch, ground_truth):
        """LLMã‚’ä½¿ç”¨ã—ã¦ãƒ‘ãƒƒãƒå“è³ªã‚’è©•ä¾¡"""
        
        # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆç”Ÿæˆ
        prompt = self._generate_quality_prompt(generated_patch, ground_truth)
        
        # LLMè©•ä¾¡å®Ÿè¡Œ
        llm_response = await self.llm_provider.evaluate(prompt)
        
        # çµæœè§£æ
        scores = self._parse_llm_response(llm_response)
        
        return scores
    
    def _calculate_correctness_at_k(self, predictions, ground_truth, k):
        """Correctness@K ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã®è¨ˆç®—"""
        if not predictions or not ground_truth:
            return 0.0
        
        # Top-Käºˆæ¸¬ã®ä¸­ã«ground truthãŒå«ã¾ã‚Œã¦ã„ã‚‹ã‹ãƒã‚§ãƒƒã‚¯
        top_k_predictions = predictions[:k]
        
        for pred in top_k_predictions:
            if self._is_functionally_equivalent(pred, ground_truth):
                return 1.0
        
        return 0.0
```

### LLMãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼çµ±åˆ

```python
# src/llm/openai_provider.py
class OpenAIProvider(BaseLLMProvider):
    def __init__(self, api_key, model="gpt-4"):
        self.client = openai.OpenAI(api_key=api_key)
        self.model = model
        self.rate_limiter = AsyncRateLimiter(max_calls=100, time_window=60)
    
    async def evaluate(self, prompt, max_tokens=2000):
        """OpenAI APIã‚’ä½¿ç”¨ã—ãŸè©•ä¾¡"""
        async with self.rate_limiter:
            try:
                response = await self.client.chat.completions.acreate(
                    model=self.model,
                    messages=[
                        {"role": "system", "content": "You are an expert code reviewer."},
                        {"role": "user", "content": prompt}
                    ],
                    max_tokens=max_tokens,
                    temperature=0.1
                )
                
                return response.choices[0].message.content
                
            except Exception as e:
                self.logger.error(f"OpenAI API error: {e}")
                raise
```

---

## ãƒ†ã‚¹ãƒˆæˆ¦ç•¥

### å˜ä½“ãƒ†ã‚¹ãƒˆ

```python
# src/tests/unit/test_compliance_evaluator.py
import pytest
from src.evaluators.compliance_evaluator import ComplianceEvaluator

class TestComplianceEvaluator:
    @pytest.fixture
    def evaluator(self):
        config = {'test_mode': True}
        return ComplianceEvaluator(config)
    
    def test_control_flow_evaluation(self, evaluator):
        # ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿æº–å‚™
        test_logs = [
            {'control_flow': 'if-then-else', 'accuracy': 0.95},
            {'control_flow': 'loop', 'accuracy': 0.87}
        ]
        
        # è©•ä¾¡å®Ÿè¡Œ
        result = evaluator.evaluate_control_flow(test_logs)
        
        # ã‚¢ã‚µãƒ¼ã‚·ãƒ§ãƒ³
        assert 0.0 <= result <= 1.0
        assert isinstance(result, float)
    
    @pytest.mark.asyncio
    async def test_parser_accuracy(self, evaluator):
        test_logs = [
            {'parsing': True, 'parsing_success': True},
            {'parsing': True, 'parsing_success': False},
            {'parsing': True, 'parsing_success': True}
        ]
        
        result = evaluator.evaluate_parser_accuracy(test_logs)
        
        assert result == 2/3  # 3ä»¶ä¸­2ä»¶æˆåŠŸ
```

### çµ±åˆãƒ†ã‚¹ãƒˆ

```python
# src/tests/integration/test_evaluation_pipeline.py
class TestEvaluationPipeline:
    @pytest.mark.integration
    async def test_full_evaluation_pipeline(self):
        """å®Œå…¨ãªè©•ä¾¡ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã®ãƒ†ã‚¹ãƒˆ"""
        
        # ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿æº–å‚™
        test_data = self._prepare_test_data()
        
        # Step1è©•ä¾¡
        compliance_evaluator = ComplianceEvaluator(config)
        compliance_results = compliance_evaluator.evaluate(test_data)
        
        # Step2è©•ä¾¡
        quality_evaluator = QualityEvaluator(config, mock_llm_provider)
        quality_results = await quality_evaluator.evaluate(test_data)
        
        # çµ±åˆçµæœç¢ºèª
        assert compliance_results.status == 'success'
        assert quality_results.status == 'success'
        assert len(compliance_results.metrics) > 0
        assert len(quality_results.metrics) > 0
```

### ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ†ã‚¹ãƒˆ

```python
# src/tests/performance/test_scalability.py
class TestScalability:
    @pytest.mark.performance
    def test_large_dataset_processing(self):
        """å¤§é‡ãƒ‡ãƒ¼ã‚¿å‡¦ç†ã®ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ†ã‚¹ãƒˆ"""
        
        # å¤§é‡ã®ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ç”Ÿæˆ
        large_dataset = self._generate_large_dataset(size=1000)
        
        start_time = time.time()
        
        # è©•ä¾¡å®Ÿè¡Œ
        results = evaluator.evaluate_batch(large_dataset)
        
        execution_time = time.time() - start_time
        
        # ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹è¦ä»¶ç¢ºèª
        assert execution_time < 300  # 5åˆ†ä»¥å†…
        assert len(results) == len(large_dataset)
```

---

## ãƒ‡ãƒãƒƒã‚°ãƒ»ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒªãƒ³ã‚°

### ãƒ­ã‚°è¨­å®š

```python
# src/utils/logger.py
import logging
import sys
from datetime import datetime

def setup_logger(name="evaluation", level=logging.INFO):
    """è©³ç´°ãªãƒ­ã‚°è¨­å®š"""
    
    logger = logging.getLogger(name)
    logger.setLevel(level)
    
    # ã‚³ãƒ³ã‚½ãƒ¼ãƒ«ãƒãƒ³ãƒ‰ãƒ©ãƒ¼
    console_handler = logging.StreamHandler(sys.stdout)
    console_formatter = logging.Formatter(
        '%(asctime)s - %(name)s - %(levelname)s - %(message)s'
    )
    console_handler.setFormatter(console_formatter)
    
    # ãƒ•ã‚¡ã‚¤ãƒ«ãƒãƒ³ãƒ‰ãƒ©ãƒ¼
    file_handler = logging.FileHandler(
        f'/app/logs/evaluation_{datetime.now().strftime("%Y%m%d")}.log'
    )
    file_formatter = logging.Formatter(
        '%(asctime)s - %(name)s - %(levelname)s - %(filename)s:%(lineno)d - %(message)s'
    )
    file_handler.setFormatter(file_formatter)
    
    logger.addHandler(console_handler)
    logger.addHandler(file_handler)
    
    return logger
```

### ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒªãƒ³ã‚°

```python
# src/utils/profiler.py
import cProfile
import pstats
from functools import wraps

def profile_function(output_file=None):
    """é–¢æ•°ã®ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒªãƒ³ã‚°ãƒ‡ã‚³ãƒ¬ãƒ¼ã‚¿"""
    def decorator(func):
        @wraps(func)
        def wrapper(*args, **kwargs):
            profiler = cProfile.Profile()
            profiler.enable()
            
            try:
                result = func(*args, **kwargs)
            finally:
                profiler.disable()
                
                if output_file:
                    profiler.dump_stats(output_file)
                else:
                    stats = pstats.Stats(profiler)
                    stats.sort_stats('cumulative')
                    stats.print_stats(20)  # Top 20
            
            return result
        return wrapper
    return decorator

# ä½¿ç”¨ä¾‹
@profile_function('/app/logs/performance/quality_evaluation.prof')
async def evaluate_patch_quality(self, patch_data):
    # é‡ã„å‡¦ç†
    pass
```

### ãƒ¡ãƒ¢ãƒªç›£è¦–

```python
# src/utils/memory_monitor.py
import psutil
import gc
from functools import wraps

def monitor_memory(threshold_mb=1000):
    """ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ç›£è¦–ãƒ‡ã‚³ãƒ¬ãƒ¼ã‚¿"""
    def decorator(func):
        @wraps(func)
        def wrapper(*args, **kwargs):
            process = psutil.Process()
            
            # å®Ÿè¡Œå‰ãƒ¡ãƒ¢ãƒª
            memory_before = process.memory_info().rss / 1024 / 1024
            
            result = func(*args, **kwargs)
            
            # å®Ÿè¡Œå¾Œãƒ¡ãƒ¢ãƒª
            memory_after = process.memory_info().rss / 1024 / 1024
            memory_diff = memory_after - memory_before
            
            if memory_diff > threshold_mb:
                logger.warning(
                    f"High memory usage in {func.__name__}: {memory_diff:.2f}MB"
                )
                gc.collect()  # ã‚¬ãƒ™ãƒ¼ã‚¸ã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³å®Ÿè¡Œ
            
            return result
        return wrapper
    return decorator
```

---

## æ–°æ©Ÿèƒ½è¿½åŠ ã‚¬ã‚¤ãƒ‰

### æ–°ã—ã„è©•ä¾¡ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã®è¿½åŠ 

1. **åŸºåº•ã‚¯ãƒ©ã‚¹ã‹ã‚‰ç¶™æ‰¿**:
```python
# src/evaluators/custom_evaluator.py
class CustomEvaluator(BaseEvaluator):
    def __init__(self, config):
        super().__init__(config)
        self.metrics = {
            'custom_metric_1': 0.0,
            'custom_metric_2': 0.0
        }
    
    def evaluate(self, data):
        """ã‚«ã‚¹ã‚¿ãƒ è©•ä¾¡ãƒ­ã‚¸ãƒƒã‚¯"""
        results = {}
        
        # ãƒ¡ãƒˆãƒªã‚¯ã‚¹è¨ˆç®—
        results['custom_metric_1'] = self._calculate_custom_metric_1(data)
        results['custom_metric_2'] = self._calculate_custom_metric_2(data)
        
        return EvaluationResult(
            timestamp=datetime.now(),
            evaluator_type='custom',
            metrics=results,
            status='success'
        )
```

2. **çµ±åˆè©•ä¾¡å™¨ã«ç™»éŒ²**:
```python
# src/evaluators/integrated_evaluator.py
class IntegratedEvaluator:
    def __init__(self, config):
        self.evaluators = [
            ComplianceEvaluator(config),
            QualityEvaluator(config),
            CustomEvaluator(config)  # è¿½åŠ 
        ]
```

### æ–°ã—ã„LLMãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ã®è¿½åŠ 

1. **ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ã‚¯ãƒ©ã‚¹å®Ÿè£…**:
```python
# src/llm/custom_provider.py
class CustomLLMProvider(BaseLLMProvider):
    def __init__(self, api_key, model_name):
        self.api_key = api_key
        self.model_name = model_name
        self.client = CustomLLMClient(api_key)
    
    async def evaluate(self, prompt, **kwargs):
        """ã‚«ã‚¹ã‚¿ãƒ LLM APIã¨ã®çµ±åˆ"""
        try:
            response = await self.client.generate(
                prompt=prompt,
                model=self.model_name,
                **kwargs
            )
            
            return self._parse_response(response)
            
        except Exception as e:
            self.logger.error(f"Custom LLM API error: {e}")
            raise
```

2. **ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ãƒ•ã‚¡ã‚¯ãƒˆãƒªãƒ¼ã«è¿½åŠ **:
```python
# src/llm/provider_factory.py
def create_llm_provider(provider_name, config):
    providers = {
        'openai': OpenAIProvider,
        'anthropic': AnthropicProvider,
        'custom': CustomLLMProvider  # è¿½åŠ 
    }
    
    if provider_name not in providers:
        raise ValueError(f"Unknown LLM provider: {provider_name}")
    
    return providers[provider_name](config)
```

---

## ã‚³ãƒ¼ãƒ‰å“è³ªç®¡ç†

### é™çš„è§£æè¨­å®š

**pyproject.toml**:
```toml
[tool.black]
line-length = 88
target-version = ['py312']

[tool.isort]
profile = "black"
multi_line_output = 3

[tool.pylint]
max-line-length = 88
disable = ["C0114", "C0116"]  # docstring warnings

[tool.mypy]
python_version = "3.12"
warn_return_any = true
warn_unused_configs = true
```

### pre-commitãƒ•ãƒƒã‚¯

**.pre-commit-config.yaml**:
```yaml
repos:
  - repo: https://github.com/psf/black
    rev: 23.3.0
    hooks:
      - id: black
  
  - repo: https://github.com/pycqa/isort
    rev: 5.12.0
    hooks:
      - id: isort
  
  - repo: https://github.com/pycqa/flake8
    rev: 6.0.0
    hooks:
      - id: flake8
  
  - repo: https://github.com/pre-commit/mirrors-mypy
    rev: v1.3.0
    hooks:
      - id: mypy
```

### ç¶™ç¶šçš„ã‚¤ãƒ³ãƒ†ã‚°ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³

**Makefile**:
```makefile
.PHONY: test lint format check

test:
	python -m pytest src/tests/ -v --cov=src

lint:
	pylint src/
	flake8 src/
	mypy src/

format:
	black src/
	isort src/

check: lint test
	echo "All checks passed!"

install-dev:
	pip install -r requirements_evaluation.txt
	pip install -r requirements_dev.txt
	pre-commit install
```

---

## ğŸš€ é–‹ç™ºãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼

### æ—¥å¸¸çš„ãªé–‹ç™ºãƒ•ãƒ­ãƒ¼

1. **ç’°å¢ƒç¢ºèª**:
   ```bash
   docker exec -it grpc-analyzer-evaluation bash
   python --version  # 3.12.4ç¢ºèª
   pip list | grep -E "(pandas|openai|anthropic)"
   ```

2. **ã‚³ãƒ¼ãƒ‰å¤‰æ›´**:
   ```bash
   # ãƒ­ãƒ¼ã‚«ãƒ«ã§ç·¨é›†
   vim src/evaluators/quality_evaluator.py
   
   # é™çš„è§£æ
   make lint
   
   # ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆ
   make format
   ```

3. **ãƒ†ã‚¹ãƒˆ**:
   ```bash
   # å˜ä½“ãƒ†ã‚¹ãƒˆ
   python -m pytest src/tests/unit/ -v
   
   # çµ±åˆãƒ†ã‚¹ãƒˆ
   python -m pytest src/tests/integration/ -v -m "not slow"
   
   # ãƒ‡ãƒãƒƒã‚°å®Ÿè¡Œ
   python main.py --mode dry-run --debug
   ```

4. **è©•ä¾¡å®Ÿè¡Œ**:
   ```bash
   # å°è¦æ¨¡ãƒ†ã‚¹ãƒˆ
   python main.py --mode compliance --limit 5
   
   # æœ¬æ ¼å®Ÿè¡Œ
   python main.py --mode full
   ```

### ãƒˆãƒ©ãƒ–ãƒ«ã‚·ãƒ¥ãƒ¼ãƒ†ã‚£ãƒ³ã‚°æ‰‹é †

1. **ãƒ­ã‚°ç¢ºèª**:
   ```bash
   tail -f /app/logs/evaluation.log
   grep -i error /app/logs/*.log
   ```

2. **è¨­å®šç¢ºèª**:
   ```bash
   python -c "import json; print(json.dumps(config, indent=2))"
   ```

3. **ä¾å­˜é–¢ä¿‚ç¢ºèª**:
   ```bash
   pip check
   pip list --outdated
   ```

4. **ãƒªã‚½ãƒ¼ã‚¹ç¢ºèª**:
   ```bash
   docker stats grpc-analyzer-evaluation
   df -h /app/
   ```

---

**ä½œæˆæ—¥**: 2025-07-22  
**å¯¾è±¡**: é–‹ç™ºè€…ãƒ»æŠ€è¡“è€…å‘ã‘  
**æ›´æ–°**: æ©Ÿèƒ½è¿½åŠ ãƒ»å¤‰æ›´æ™‚ã«éšæ™‚æ›´æ–°
