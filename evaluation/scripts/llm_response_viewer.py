#!/usr/bin/env python3
"""
LLMç”Ÿæˆãƒ¬ã‚¹ãƒãƒ³ã‚¹å†…å®¹è¡¨ç¤ºãƒ„ãƒ¼ãƒ«
è©•ä¾¡ã‚·ã‚¹ãƒ†ãƒ ã®LLMãŒå®Ÿéš›ã«ç”Ÿæˆã—ãŸè©³ç´°ãªãƒ¬ã‚¹ãƒãƒ³ã‚¹å†…å®¹ã‚’è¡¨ç¤º
"""

import sys
import json
import asyncio

sys.path.append('/app')
sys.path.append('/app/src')

from src.llm.llm_integration import MockLLMProvider, OpenAIProvider, AnthropicProvider
from src.utils.template_manager import APRPromptTemplates


async def show_llm_response_details():
    """LLMå¿œç­”ã®è©³ç´°å†…å®¹ã‚’è¡¨ç¤º"""
    print("ğŸ¤– LLMè©•ä¾¡ãƒ¬ã‚¹ãƒãƒ³ã‚¹è©³ç´°è¡¨ç¤ºãƒ„ãƒ¼ãƒ«")
    print("=" * 60)
    
    # å„ç¨®ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ã‚’ãƒ†ã‚¹ãƒˆ
    providers_info = [
        ("Mock LLM", MockLLMProvider()),
        # å®Ÿéš›ã®APIã‚­ãƒ¼ãŒã‚ã‚Œã°ä»¥ä¸‹ã‚‚æœ‰åŠ¹åŒ–å¯èƒ½
        # ("OpenAI GPT-4", OpenAIProvider("gpt-4.1")),
        # ("Anthropic Claude", AnthropicProvider("claude-3-sonnet-20240229"))
    ]
    
    # APRãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆç®¡ç†
    template_manager = APRPromptTemplates(template_style="default")
    
    # ã‚µãƒ³ãƒ—ãƒ«è©•ä¾¡ãƒ‡ãƒ¼ã‚¿
    sample_evaluation_data = {
        "experiment_id": "servantes/Issue_sample_evaluation",
        "turn_count": 2,
        "overall_status": "SUCCESS",
        "turns_data": [
            {
                "turn": 1,
                "action": "think",
                "content": "Analyzing the problem requirements and understanding the scope of the issue.",
                "parsed_content": {"analysis": "initial problem assessment", "status": "completed"}
            },
            {
                "turn": 2, 
                "action": "plan",
                "content": "Developing a comprehensive strategy to address the identified issues.",
                "parsed_content": {"strategy": "implementation plan", "status": "completed"}
            }
        ]
    }
    
    for provider_name, provider in providers_info:
        print(f"\nğŸ§  {provider_name} ãƒ¬ã‚¹ãƒãƒ³ã‚¹è©³ç´°")
        print("-" * 40)
        
        try:
            # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’ç”Ÿæˆ
            prompt = template_manager.get_evaluation_prompt(sample_evaluation_data)
            print(f"ğŸ“ é€ä¿¡ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆé•·: {len(prompt)} æ–‡å­—")
            print(f"ğŸ“„ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆå†…å®¹ï¼ˆæœ€åˆã®200æ–‡å­—ï¼‰:")
            print(prompt[:200] + "...")
            print()
            
            # LLMå¿œç­”ã‚’ç”Ÿæˆ
            response = await provider.generate(prompt)
            
            print(f"âœ… ãƒ¬ã‚¹ãƒãƒ³ã‚¹æˆåŠŸ: {response.success}")
            if not response.success:
                print(f"âŒ ã‚¨ãƒ©ãƒ¼: {response.error_message}")
                continue
            
            print(f"ğŸ“Š ä½¿ç”¨é‡: {response.usage}")
            print(f"ğŸ¤– ãƒ¢ãƒ‡ãƒ«: {response.model}")
            print(f"ğŸ”— ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼: {response.provider}")
            print()
            
            print("ğŸ“ƒ ç”Ÿæˆã•ã‚ŒãŸLLMè©•ä¾¡å†…å®¹:")
            print("-" * 30)
            
            # JSONã¨ã—ã¦æ•´å½¢è¡¨ç¤ºã‚’è©¦è¡Œ
            try:
                parsed_response = json.loads(response.content)
                print(json.dumps(parsed_response, indent=2, ensure_ascii=False))
                
                # è©•ä¾¡å†…å®¹ã®è§£æ
                print("\\nğŸ” è©•ä¾¡å†…å®¹è§£æ:")
                if "parser_evaluation" in parsed_response:
                    parser_evals = parsed_response["parser_evaluation"]
                    print(f"  - ãƒ‘ãƒ¼ã‚µãƒ¼è©•ä¾¡ã‚¿ãƒ¼ãƒ³æ•°: {len(parser_evals)}")
                    for i, eval_item in enumerate(parser_evals, 1):
                        print(f"    Turn {i}: {eval_item.get('status', 'N/A')} - {eval_item.get('reasoning', 'N/A')[:50]}...")
                
                if "workflow_evaluation" in parsed_response:
                    workflow = parsed_response["workflow_evaluation"]
                    print(f"  - ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼é©åˆ: {workflow.get('is_compliant', 'N/A')}")
                    print(f"  - ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼ç†ç”±: {workflow.get('reasoning', 'N/A')[:80]}...")
                
                if "overall_assessment" in parsed_response:
                    assessment = parsed_response["overall_assessment"]
                    print(f"  - ã‚·ã‚¹ãƒ†ãƒ å¥å…¨æ€§: {assessment.get('system_health', 'N/A')}")
                    print(f"  - é‡å¤§å•é¡Œæ•°: {len(assessment.get('critical_issues', []))}")
                    print(f"  - æ¨å¥¨äº‹é …æ•°: {len(assessment.get('recommendations', []))}")
                
            except json.JSONDecodeError:
                # JSONä»¥å¤–ã®å ´åˆã¯ãã®ã¾ã¾è¡¨ç¤º
                print(response.content)
                
        except Exception as e:
            print(f"âŒ {provider_name} ã§ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿ: {e}")
    
    print("\\n" + "=" * 60)
    print("ğŸ’¡ å®Ÿéš›ã®è©•ä¾¡çµæœãƒ•ã‚¡ã‚¤ãƒ«ã§LLMãƒ¬ã‚¹ãƒãƒ³ã‚¹å†…å®¹ã‚’ç¢ºèªã™ã‚‹ã«ã¯:")
    print("   python scripts/evaluation_log_viewer.py --latest")
    print("   python scripts/evaluation_log_viewer.py --repo servantes")
    print("   python scripts/evaluation_log_viewer.py -f [ãƒ•ã‚¡ã‚¤ãƒ«å]")


async def compare_different_templates():
    """ç•°ãªã‚‹ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆã§ã®LLMå¿œç­”ã‚’æ¯”è¼ƒ"""
    print("\\nğŸ¨ ç•°ãªã‚‹ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆã§ã®LLMå¿œç­”æ¯”è¼ƒ")
    print("=" * 60)
    
    template_styles = ["default", "japanese", "simple"]
    provider = MockLLMProvider()
    
    sample_data = {
        "experiment_id": "sample/test_comparison",
        "turn_count": 1,
        "overall_status": "SUCCESS", 
        "turns_data": [{"turn": 1, "status": "completed"}]
    }
    
    for style in template_styles:
        print(f"\\nğŸ“‹ ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆã‚¹ã‚¿ã‚¤ãƒ«: {style}")
        print("-" * 30)
        
        template_manager = APRPromptTemplates(template_style=style)
        prompt = template_manager.get_evaluation_prompt(sample_data)
        
        print(f"ğŸ“ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆé•·: {len(prompt)} æ–‡å­—")
        
        response = await provider.generate(prompt)
        if response.success:
            print(f"ğŸ“Š ãƒˆãƒ¼ã‚¯ãƒ³ä½¿ç”¨é‡: {response.usage.get('total_tokens', 'N/A')}")
            print(f"ğŸ“„ å¿œç­”å†…å®¹ï¼ˆæœ€åˆã®150æ–‡å­—ï¼‰:")
            print(response.content[:150] + "...")
        else:
            print(f"âŒ å¿œç­”å¤±æ•—: {response.error_message}")


if __name__ == "__main__":
    import argparse
    parser = argparse.ArgumentParser(description="LLMå¿œç­”è©³ç´°è¡¨ç¤ºãƒ„ãƒ¼ãƒ«")
    parser.add_argument("--compare-templates", action="store_true", help="ç•°ãªã‚‹ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆã§ã®å¿œç­”ã‚’æ¯”è¼ƒ")
    
    args = parser.parse_args()
    
    if args.compare_templates:
        asyncio.run(compare_different_templates())
    else:
        asyncio.run(show_llm_response_details())
