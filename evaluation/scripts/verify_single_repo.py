#!/usr/bin/env python3
"""
å˜ä¸€ãƒªãƒã‚¸ãƒˆãƒªå‹•ä½œæ¤œè¨¼ã‚¹ã‚¯ãƒªãƒ—ãƒˆ
ç‰¹å®šã®ãƒªãƒã‚¸ãƒˆãƒªã§å°‘æ•°ã®ãƒ­ã‚°ãƒ•ã‚¡ã‚¤ãƒ«ã‚’å¯¾è±¡ã«å‹•ä½œæ¤œè¨¼ã‚’å®Ÿè¡Œ
"""

import asyncio
import sys
from pathlib import Path

# ãƒ‘ã‚¹ã‚’è¿½åŠ 
sys.path.append('/app')

from src.evaluators.compliance_evaluator import SystemComplianceEvaluator
from src.utils.log_iterator import APRLogIterator


async def verify_single_repository():
    """å˜ä¸€ãƒªãƒã‚¸ãƒˆãƒªã§ã®å‹•ä½œæ¤œè¨¼"""
    print("ğŸ” å˜ä¸€ãƒªãƒã‚¸ãƒˆãƒªå‹•ä½œæ¤œè¨¼")
    print("=" * 50)
    
    # åˆ©ç”¨å¯èƒ½ãªãƒªãƒã‚¸ãƒˆãƒªã‚’è¡¨ç¤º
    log_iterator = APRLogIterator("/app")
    available_repos = log_iterator.get_project_names()
    
    print(f"\nğŸ“‹ åˆ©ç”¨å¯èƒ½ãªãƒªãƒã‚¸ãƒˆãƒª ({len(available_repos)}ä»¶):")
    for i, repo in enumerate(available_repos, 1):
        print(f"  {i}. {repo}")
    
    # æ¤œè¨¼å¯¾è±¡ãƒªãƒã‚¸ãƒˆãƒªã‚’é¸æŠï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: servantesï¼‰
    target_repo = "servantes"  # ä¿®æ­£å¯èƒ½
    max_logs = 5
    
    print(f"\nğŸ¯ æ¤œè¨¼å¯¾è±¡: {target_repo}")
    print(f"ğŸ“Š æœ€å¤§å‡¦ç†ãƒ­ã‚°æ•°: {max_logs}ä»¶")
    
    # å„LLMãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ã§ãƒ†ã‚¹ãƒˆ
    test_configs = [
        {"provider": "mock", "model": None, "description": "Mock LLM (é«˜é€Ÿãƒ†ã‚¹ãƒˆ)"},
        # å®Ÿéš›ã®OpenAI APIã‚­ãƒ¼ãŒè¨­å®šã•ã‚Œã¦ã„ã‚‹å ´åˆã¯ä»¥ä¸‹ã‚‚è©¦ã›ã¾ã™
        # {"provider": "openai", "model": "gpt-4.1-mini", "description": "OpenAI gpt-4.1 Mini (è»½é‡)"},
    ]
    
    print(f"\nğŸ§ª ãƒ†ã‚¹ãƒˆè¨­å®š:")
    for i, config in enumerate(test_configs, 1):
        print(f"  {i}. {config['description']}")
    
    # å„è¨­å®šã§ãƒ†ã‚¹ãƒˆå®Ÿè¡Œ
    for config in test_configs:
        print(f"\n" + "="*60)
        print(f"ğŸš€ ãƒ†ã‚¹ãƒˆå®Ÿè¡Œ: {config['description']}")
        print("="*60)
        
        try:
            # è©•ä¾¡å™¨ã‚’åˆæœŸåŒ–
            evaluator = SystemComplianceEvaluator(
                llm_provider=config["provider"],
                llm_model=config["model"]
            )
            
            # å˜ä¸€ãƒªãƒã‚¸ãƒˆãƒªè©•ä¾¡ã‚’å®Ÿè¡Œ
            results = await evaluator.evaluate_single_repository(target_repo, max_logs)
            
            # çµæœã‚’è¡¨ç¤º
            print_evaluation_summary(results, config["description"])
            
        except Exception as e:
            print(f"âŒ ãƒ†ã‚¹ãƒˆå®Ÿè¡Œã‚¨ãƒ©ãƒ¼ ({config['description']}): {e}")
            import traceback
            traceback.print_exc()


def print_evaluation_summary(results: dict, config_name: str):
    """è©•ä¾¡çµæœã‚µãƒãƒªãƒ¼ã‚’è¡¨ç¤º"""
    print(f"\nğŸ“Š è©•ä¾¡çµæœã‚µãƒãƒªãƒ¼ - {config_name}")
    print("-" * 40)
    
    if "error_reason" in results:
        print(f"âŒ ã‚¨ãƒ©ãƒ¼: {results['error_reason']}")
        return
    
    summary = results.get("summary", {})
    metrics = results.get("compliance_metrics", {})
    
    print(f"ğŸ¯ å¯¾è±¡ãƒªãƒã‚¸ãƒˆãƒª: {results.get('repository_name', 'N/A')}")
    print(f"â±ï¸  å‡¦ç†æ™‚é–“: {results.get('processing_time_seconds', 0):.2f}ç§’")
    print(f"ğŸ“ å‡¦ç†ãƒ­ã‚°æ•°: {summary.get('total_logs_processed', 0)}")
    print(f"âœ… LLMè©•ä¾¡æˆåŠŸ: {summary.get('successful_evaluations', 0)}")
    print(f"âŒ LLMè©•ä¾¡å¤±æ•—: {summary.get('failed_evaluations', 0)}")
    print(f"ğŸ¤– ä½¿ç”¨ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼: {results.get('llm_provider', 'N/A')}")
    print(f"ğŸ§  ä½¿ç”¨ãƒ¢ãƒ‡ãƒ«: {results.get('llm_model', 'N/A')}")
    
    print(f"\nğŸ“ˆ ãƒ¡ãƒˆãƒªã‚¯ã‚¹:")
    print(f"  - ç·åˆé©åˆæ€§ã‚¹ã‚³ã‚¢: {metrics.get('overall_compliance', 0):.3f}")
    print(f"  - ãƒ‘ãƒ¼ã‚µãƒ¼æˆåŠŸç‡: {metrics.get('parser_success_rate', 0):.3f}")
    print(f"  - ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼é©åˆç‡: {metrics.get('control_flow_accuracy', 0):.3f}")
    print(f"  - ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°ã‚¹ã‚³ã‚¢: {metrics.get('error_handling_score', 0):.3f}")
    
    # è©³ç´°çµæœã®ä¸€éƒ¨ã‚’è¡¨ç¤º
    detailed_results = results.get("detailed_results", [])
    if detailed_results:
        print(f"\nğŸ“‹ è©³ç´°çµæœ (æœ€åˆã®3ä»¶):")
        for i, result in enumerate(detailed_results[:3], 1):
            print(f"  {i}. {result.get('experiment_id', 'N/A')}")
            print(f"     LLMæˆåŠŸ: {result.get('llm_success', False)}")
            print(f"     ã‚·ã‚¹ãƒ†ãƒ å¥å…¨æ€§: {result.get('system_health', 'N/A')}")
            print(f"     ã‚¯ãƒªãƒ†ã‚£ã‚«ãƒ«å•é¡Œ: {len(result.get('critical_issues', []))}")


async def test_specific_repository(repo_name: str, max_logs: int = 3):
    """ç‰¹å®šã®ãƒªãƒã‚¸ãƒˆãƒªã‚’ãƒ†ã‚¹ãƒˆï¼ˆå€‹åˆ¥å®Ÿè¡Œç”¨ï¼‰"""
    print(f"ğŸ¯ å€‹åˆ¥ãƒªãƒã‚¸ãƒˆãƒªãƒ†ã‚¹ãƒˆ: {repo_name}")
    
    evaluator = SystemComplianceEvaluator(llm_provider="mock")
    results = await evaluator.evaluate_single_repository(repo_name, max_logs)
    
    print_evaluation_summary(results, f"Mock Test - {repo_name}")
    return results


async def quick_verification():
    """ã‚¯ã‚¤ãƒƒã‚¯æ¤œè¨¼ï¼ˆæœ€å°é™ã®ãƒ†ã‚¹ãƒˆï¼‰"""
    print("âš¡ ã‚¯ã‚¤ãƒƒã‚¯å‹•ä½œæ¤œè¨¼")
    print("=" * 30)
    
    # è¤‡æ•°ã®ãƒªãƒã‚¸ãƒˆãƒªã§ç°¡å˜ãªãƒ†ã‚¹ãƒˆ
    test_repos = ["servantes", "boulder", "weaviate"]  # å­˜åœ¨ã™ã‚‹ãƒªãƒã‚¸ãƒˆãƒª
    
    for repo in test_repos:
        print(f"\nğŸ”„ ãƒ†ã‚¹ãƒˆ: {repo}")
        try:
            result = await test_specific_repository(repo, max_logs=2)
            status = "âœ… æˆåŠŸ" if result.get("summary", {}).get("total_logs_processed", 0) > 0 else "âš ï¸  ãƒ­ã‚°ãªã—"
            print(f"   çµæœ: {status}")
        except Exception as e:
            print(f"   çµæœ: âŒ ã‚¨ãƒ©ãƒ¼ - {e}")


if __name__ == "__main__":
    import os
    
    print("ğŸš€ APR ã‚·ã‚¹ãƒ†ãƒ é©åˆæ€§è©•ä¾¡ - å‹•ä½œæ¤œè¨¼")
    print("=" * 60)
    
    # å¼•æ•°ã«å¿œã˜ã¦å®Ÿè¡Œãƒ¢ãƒ¼ãƒ‰ã‚’é¸æŠ
    if len(sys.argv) > 1:
        mode = sys.argv[1]
        if mode == "quick":
            asyncio.run(quick_verification())
        elif mode == "full":
            asyncio.run(verify_single_repository())
        else:
            print("ä½¿ç”¨æ–¹æ³•: python verify_single_repo.py [quick|full]")
    else:
        # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã¯å˜ä¸€ãƒªãƒã‚¸ãƒˆãƒªæ¤œè¨¼
        asyncio.run(verify_single_repository())
