# APRè©•ä¾¡ã‚·ã‚¹ãƒ†ãƒ  å¼•ãç¶™ããƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆ

## ğŸ“‹ ç›®æ¬¡
1. [ã‚·ã‚¹ãƒ†ãƒ æ¦‚è¦](#ã‚·ã‚¹ãƒ†ãƒ æ¦‚è¦)
2. [ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£](#ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£)
3. [ç’°å¢ƒæ§‹ç¯‰](#ç’°å¢ƒæ§‹ç¯‰)
4. [å®Ÿè¡Œæ–¹æ³•](#å®Ÿè¡Œæ–¹æ³•)
5. [è©•ä¾¡ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯](#è©•ä¾¡ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯)
6. [ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ­ãƒ¼](#ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ­ãƒ¼)
7. [è¨­å®šã¨ã‚«ã‚¹ã‚¿ãƒã‚¤ã‚º](#è¨­å®šã¨ã‚«ã‚¹ã‚¿ãƒã‚¤ã‚º)
8. [ãƒˆãƒ©ãƒ–ãƒ«ã‚·ãƒ¥ãƒ¼ãƒ†ã‚£ãƒ³ã‚°](#ãƒˆãƒ©ãƒ–ãƒ«ã‚·ãƒ¥ãƒ¼ãƒ†ã‚£ãƒ³ã‚°)
9. [é–‹ç™ºç¶™ç¶šã®ã‚¬ã‚¤ãƒ‰](#é–‹ç™ºç¶™ç¶šã®ã‚¬ã‚¤ãƒ‰)

---

## ã‚·ã‚¹ãƒ†ãƒ æ¦‚è¦

### ç›®çš„
LLMãƒ™ãƒ¼ã‚¹ã®APRï¼ˆAutomatic Program Repairï¼‰ã‚·ã‚¹ãƒ†ãƒ ãŒç”Ÿæˆã—ãŸãƒ‘ãƒƒãƒã®å“è³ªã‚’ã€ã‚°ãƒ©ã‚¦ãƒ³ãƒ‰ãƒˆã‚¥ãƒ«ãƒ¼ã‚¹ã¨æ¯”è¼ƒã—ã¦è‡ªå‹•è©•ä¾¡ã™ã‚‹ã‚·ã‚¹ãƒ†ãƒ ã€‚

### ä¸»è¦æ©Ÿèƒ½
- **2æ®µéšè©•ä¾¡ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯**
  - Step 1: ã‚·ã‚¹ãƒ†ãƒ é©åˆæ€§è©•ä¾¡ï¼ˆåˆ¶å¾¡ãƒ•ãƒ­ãƒ¼ã€ãƒ‘ãƒ¼ã‚µãƒ¼ç²¾åº¦ï¼‰
  - Step 2: ãƒ‘ãƒƒãƒå“è³ªè©•ä¾¡ï¼ˆå¦¥å½“æ€§ã€æ­£ç¢ºæ€§R1-R10ã€æ¨è«–å“è³ªï¼‰
- **LLMçµ±åˆè©•ä¾¡**: OpenAI/Anthropic APIã‚’ä½¿ç”¨ã—ãŸé«˜åº¦ãªè©•ä¾¡
- **çµ±è¨ˆåˆ†æ**: pandas/matplotlib/seabornã«ã‚ˆã‚‹è©³ç´°ãªåˆ†æã¨ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ
- **Dockerã‚³ãƒ³ãƒ†ãƒŠåŒ–**: ç’°å¢ƒã®ä¸€è²«æ€§ã¨å†ç¾æ€§ã®ç¢ºä¿

---

## ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£

### ã‚³ãƒ³ãƒ†ãƒŠæ§‹æˆ
```
docker-compose.yml
â”œâ”€â”€ dev                  # é–‹ç™ºç’°å¢ƒ
â”œâ”€â”€ grpc-analyzer-node  # æœ¬ç•ªAPRã‚·ã‚¹ãƒ†ãƒ 
â””â”€â”€ evaluation-system   # Pythonè©•ä¾¡ã‚·ã‚¹ãƒ†ãƒ ï¼ˆâ˜…ã“ã®ã‚·ã‚¹ãƒ†ãƒ ï¼‰
```

### ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªæ§‹é€ 
```
/app/evaluation/                    # è©•ä¾¡ã‚·ã‚¹ãƒ†ãƒ ãƒ«ãƒ¼ãƒˆ
â”œâ”€â”€ main.py                        # ãƒ¡ã‚¤ãƒ³ã‚¨ãƒ³ãƒˆãƒªãƒ¼ãƒã‚¤ãƒ³ãƒˆ
â”œâ”€â”€ Dockerfile_evaluation          # Dockerãƒ•ã‚¡ã‚¤ãƒ«
â”œâ”€â”€ requirements_evaluation.txt    # Pythonä¾å­˜é–¢ä¿‚
â”œâ”€â”€ README.md                      # åŸºæœ¬èª¬æ˜
â”œâ”€â”€ HANDOVER_DOCUMENTATION.md      # æœ¬ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆ
â”œâ”€â”€ DEVELOPMENT_GUIDE.md           # é–‹ç™ºã‚¬ã‚¤ãƒ‰
â”œâ”€â”€ API_SPECIFICATION.md           # APIä»•æ§˜æ›¸
â”œâ”€â”€ SYSTEM_SPECIFICATION.md       # ã‚·ã‚¹ãƒ†ãƒ ä»•æ§˜æ›¸
â”œâ”€â”€ evaluation-design/             # è©•ä¾¡è¨­è¨ˆ
â”‚   â”œâ”€â”€ integrated-evaluation-system.md
â”‚   â”œâ”€â”€ step1-system-compliance-evaluator.md
â”‚   â”œâ”€â”€ step2-patch-quality-evaluator.md
â”‚   â””â”€â”€ prompt-templates/
â”‚       â”œâ”€â”€ system-compliance-evaluation.txt
â”‚       â””â”€â”€ patch-quality-evaluation.txt
â”œâ”€â”€ src/                           # ã‚½ãƒ¼ã‚¹ã‚³ãƒ¼ãƒ‰
â”‚   â”œâ”€â”€ evaluators/
â”‚   â”œâ”€â”€ analyzers/
â”‚   â”œâ”€â”€ llm/
â”‚   â”œâ”€â”€ reporters/
â”‚   â”œâ”€â”€ utils/
â”‚   â””â”€â”€ tests/
â”œâ”€â”€ config/                        # è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«
â”œâ”€â”€ results/                       # è©•ä¾¡çµæœå‡ºåŠ›
â”œâ”€â”€ logs/                          # ãƒ­ã‚°ãƒ•ã‚¡ã‚¤ãƒ«
â”œâ”€â”€ apr-logs/                      # APRãƒ­ã‚°ï¼ˆèª­ã¿å–ã‚Šå°‚ç”¨ãƒã‚¦ãƒ³ãƒˆï¼‰
â”œâ”€â”€ apr-output/                    # APRçµæœï¼ˆèª­ã¿å–ã‚Šå°‚ç”¨ãƒã‚¦ãƒ³ãƒˆï¼‰
â””â”€â”€ dataset/                       # ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆï¼ˆèª­ã¿å–ã‚Šå°‚ç”¨ãƒã‚¦ãƒ³ãƒˆï¼‰
```

### ãƒ‡ãƒ¼ã‚¿ãƒãƒƒãƒ”ãƒ³ã‚°ï¼ˆãƒ›ã‚¹ãƒˆOS â†’ ã‚³ãƒ³ãƒ†ãƒŠï¼‰
```
F:/Workspace/gRPC_Analyzer/evaluation â†’ /app/         # è©•ä¾¡ã‚·ã‚¹ãƒ†ãƒ å®Œå…¨ç‰ˆ
F:/Workspace/gRPC_Analyzer/dataset   â†’ /app/dataset   # ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆï¼ˆèª­ã¿å–ã‚Šå°‚ç”¨ï¼‰
F:/Workspace/gRPC_Analyzer/log       â†’ /app/apr-logs  # APRãƒ­ã‚°ï¼ˆèª­ã¿å–ã‚Šå°‚ç”¨ï¼‰
F:/Workspace/gRPC_Analyzer/output    â†’ /app/apr-output # APRçµæœï¼ˆèª­ã¿å–ã‚Šå°‚ç”¨ï¼‰
```

---

## ç’°å¢ƒæ§‹ç¯‰

### å‰ææ¡ä»¶
- **ãƒ›ã‚¹ãƒˆOS**: Windows/Linuxï¼ˆdev containerå¯¾å¿œï¼‰
- **Docker**: Docker Compose v2.0+
- **Python**: 3.12.4ï¼ˆã‚³ãƒ³ãƒ†ãƒŠå†…ï¼‰
- **ãƒ›ã‚¹ãƒˆãƒ‘ã‚¹**: `F:/Workspace/gRPC_Analyzer/` ãŒåˆ©ç”¨å¯èƒ½

### ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—æ‰‹é †

1. **ãƒªãƒã‚¸ãƒˆãƒªã®æº–å‚™**
   ```bash
   cd /app/.docker
   ```

2. **ã‚³ãƒ³ãƒ†ãƒŠãƒ“ãƒ«ãƒ‰**
   ```bash
   ./manage.sh build
   ```

3. **ç’°å¢ƒãƒ†ã‚¹ãƒˆ**
   ```bash
   ./manage.sh eval-dry-run
   ```

4. **è¨­å®šç¢ºèª**
   ```bash
   ./manage.sh eval-setup
   ```

---

## å®Ÿè¡Œæ–¹æ³•

### åŸºæœ¬ã‚³ãƒãƒ³ãƒ‰ï¼ˆmanage.shã‚’ä½¿ç”¨ï¼‰

```bash
# è©•ä¾¡ã‚·ã‚¹ãƒ†ãƒ ã®èµ·å‹•
./manage.sh eval-run

# ãƒ‰ãƒ©ã‚¤ãƒ©ãƒ³ï¼ˆè¨­å®šç¢ºèªï¼‰
./manage.sh eval-dry-run

# ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—ç¢ºèª
./manage.sh eval-setup

# ãƒ‡ãƒ¼ã‚¿ãƒªã‚»ãƒƒãƒˆ
./manage.sh reset

# ãƒ­ã‚°ç¢ºèª
./manage.sh logs evaluation-system
```

### ç›´æ¥å®Ÿè¡Œï¼ˆã‚³ãƒ³ãƒ†ãƒŠå†…ï¼‰

```bash
# ã‚³ãƒ³ãƒ†ãƒŠã«å…¥ã‚‹
docker exec -it grpc-analyzer-evaluation bash

# è©•ä¾¡å®Ÿè¡Œ
python main.py --mode full --output-format json

# ç‰¹å®šã®è©•ä¾¡ã®ã¿
python main.py --mode compliance --project servantes
python main.py --mode quality --dataset filtered_confirmed
```

### è©•ä¾¡ãƒ¢ãƒ¼ãƒ‰

| ãƒ¢ãƒ¼ãƒ‰ | èª¬æ˜ | å®Ÿè¡Œæ™‚é–“ |
|--------|------|----------|
| `setup` | ç’°å¢ƒç¢ºèªãƒ»åˆæœŸè¨­å®š | 1-2åˆ† |
| `compliance` | ã‚·ã‚¹ãƒ†ãƒ é©åˆæ€§è©•ä¾¡ã®ã¿ | 5-10åˆ† |
| `quality` | ãƒ‘ãƒƒãƒå“è³ªè©•ä¾¡ã®ã¿ | 15-30åˆ† |
| `full` | å®Œå…¨è©•ä¾¡ï¼ˆä¸¡æ®µéšï¼‰ | 20-40åˆ† |
| `dry-run` | è¨­å®šç¢ºèªï¼ˆå®Ÿéš›ã®è©•ä¾¡ãªã—ï¼‰ | 1åˆ† |

---

## è©•ä¾¡ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯

### Step 1: ã‚·ã‚¹ãƒ†ãƒ é©åˆæ€§è©•ä¾¡

**ç›®çš„**: APRã‚·ã‚¹ãƒ†ãƒ ã®åŸºæœ¬æ©Ÿèƒ½æ¤œè¨¼

**è©•ä¾¡é …ç›®**:
- åˆ¶å¾¡ãƒ•ãƒ­ãƒ¼è§£æç²¾åº¦
- ã‚³ãƒ¼ãƒ‰ãƒ‘ãƒ¼ã‚µãƒ¼æ­£ç¢ºæ€§
- ãƒ•ã‚¡ã‚¤ãƒ«å‡¦ç†èƒ½åŠ›
- ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°

**è©•ä¾¡æ–¹æ³•**:
```python
# å®Ÿè£…ä¾‹
from src.evaluators.compliance_evaluator import ComplianceEvaluator

evaluator = ComplianceEvaluator()
results = evaluator.evaluate_project('servantes')
```

### Step 2: ãƒ‘ãƒƒãƒå“è³ªè©•ä¾¡

**ç›®çš„**: ç”Ÿæˆãƒ‘ãƒƒãƒã®å®Ÿè³ªçš„å“è³ªæ¸¬å®š

**è©•ä¾¡é …ç›®**:
- **å¦¥å½“æ€§**: æ§‹æ–‡ãƒ»æ„å‘³çš„æ­£ç¢ºæ€§
- **æ­£ç¢ºæ€§R1-R10**: ã‚°ãƒ©ã‚¦ãƒ³ãƒ‰ãƒˆã‚¥ãƒ«ãƒ¼ã‚¹ã¨ã®ä¸€è‡´åº¦
- **æ¨è«–å“è³ª**: LLMã®æ¨è«–ãƒ—ãƒ­ã‚»ã‚¹è©•ä¾¡

**è©•ä¾¡æ–¹æ³•**:
```python
# å®Ÿè£…ä¾‹
from src.evaluators.quality_evaluator import QualityEvaluator

evaluator = QualityEvaluator(llm_provider='openai')
results = evaluator.evaluate_patches(patch_data, ground_truth)
```

### LLMçµ±åˆè©•ä¾¡

**ã‚µãƒãƒ¼ãƒˆãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼**:
- OpenAI (GPT-4, GPT-3.5-turbo)
- Anthropic (Claude-3)

**ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆ**:
- `evaluation-design/prompt-templates/system-compliance-evaluation.txt`
- `evaluation-design/prompt-templates/patch-quality-evaluation.txt`

---

## ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ­ãƒ¼

### å…¥åŠ›ãƒ‡ãƒ¼ã‚¿
1. **ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ**: `dataset/` é…ä¸‹ã®CSVãƒ•ã‚¡ã‚¤ãƒ«
   - `gRPC_reps_list.csv`: ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆä¸€è¦§
   - `P.U_merged_filtered - Final_merged_only_not_excluded_yes_ms_unarchived_commit_hash v2.0.csv`: ãƒ¡ã‚¤ãƒ³ãƒ‡ãƒ¼ã‚¿

2. **APRãƒ­ã‚°**: `apr-logs/` é…ä¸‹
   - ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆåˆ¥å®Ÿè¡Œãƒ­ã‚°
   - ã‚¨ãƒ©ãƒ¼ãƒ»è­¦å‘Šæƒ…å ±

3. **APRçµæœ**: `apr-output/` é…ä¸‹
   - ç”Ÿæˆãƒ‘ãƒƒãƒãƒ•ã‚¡ã‚¤ãƒ«
   - å‡¦ç†ã‚µãƒãƒªãƒ¼

### å‡ºåŠ›ãƒ‡ãƒ¼ã‚¿
1. **è©•ä¾¡çµæœ**: `results/` é…ä¸‹
   - `evaluation_report_YYYY-MM-DDTHH-MM-SS.json`: è©³ç´°ãƒ¬ãƒãƒ¼ãƒˆ
   - `summary_statistics_YYYY-MM-DDTHH-MM-SS.json`: çµ±è¨ˆã‚µãƒãƒªãƒ¼

2. **å¯è¦–åŒ–**: `results/charts/` é…ä¸‹
   - çµ±è¨ˆã‚°ãƒ©ãƒ•ï¼ˆPNG/PDFï¼‰
   - æ¯”è¼ƒãƒãƒ£ãƒ¼ãƒˆ

### å‡¦ç†ãƒ•ãƒ­ãƒ¼
```
ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆèª­ã¿è¾¼ã¿ â†’ APRçµæœè§£æ â†’ Step1è©•ä¾¡ â†’ Step2è©•ä¾¡ â†’ ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ â†’ å¯è¦–åŒ–
```

---

## è¨­å®šã¨ã‚«ã‚¹ã‚¿ãƒã‚¤ã‚º

### ç’°å¢ƒå¤‰æ•°

```bash
# å¿…é ˆ
PYTHONPATH=/app
EVALUATION_MODE=production

# ã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼ˆLLM APIï¼‰
OPENAI_API_KEY=your_key_here
ANTHROPIC_API_KEY=your_key_here
```

### è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«

**config/evaluation_config.json**:
```json
{
  "evaluation": {
    "step1_enabled": true,
    "step2_enabled": true,
    "llm_provider": "openai",
    "model": "gpt-4",
    "timeout": 300
  },
  "output": {
    "format": "json",
    "include_charts": true,
    "chart_format": "png"
  },
  "data": {
    "batch_size": 10,
    "parallel_workers": 4
  }
}
```

### ã‚«ã‚¹ã‚¿ãƒã‚¤ã‚ºãƒã‚¤ãƒ³ãƒˆ

1. **è©•ä¾¡ãƒ¡ãƒˆãƒªã‚¯ã‚¹è¿½åŠ **: `src/evaluators/` é…ä¸‹ã«æ–°ã—ã„è©•ä¾¡å™¨ã‚’è¿½åŠ 
2. **LLMãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼è¿½åŠ **: `src/llm/` é…ä¸‹ã«ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼å®Ÿè£…
3. **ãƒ¬ãƒãƒ¼ãƒˆå½¢å¼**: `src/reporters/` ã§ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆå™¨ã‚’ã‚«ã‚¹ã‚¿ãƒã‚¤ã‚º
4. **å¯è¦–åŒ–**: `src/visualizers/` ã§ãƒãƒ£ãƒ¼ãƒˆç”Ÿæˆã‚’æ‹¡å¼µ

---

## ãƒˆãƒ©ãƒ–ãƒ«ã‚·ãƒ¥ãƒ¼ãƒ†ã‚£ãƒ³ã‚°

### ã‚ˆãã‚ã‚‹å•é¡Œ

**1. ãƒ‘ã‚¹é–¢é€£ã‚¨ãƒ©ãƒ¼**
```
ERROR: Required directory not found: /app/apr-logs
```
**è§£æ±ºç­–**: docker-compose.ymlã®ãƒœãƒªãƒ¥ãƒ¼ãƒ ãƒãƒƒãƒ”ãƒ³ã‚°ç¢ºèª

**2. LLM API ã‚¨ãƒ©ãƒ¼**
```
OpenAI API rate limit exceeded
```
**è§£æ±ºç­–**: API ã‚­ãƒ¼ç¢ºèªã€ãƒ¬ãƒ¼ãƒˆåˆ¶é™ã®èª¿æ•´

**3. ãƒ¡ãƒ¢ãƒªä¸è¶³**
```
MemoryError during batch processing
```
**è§£æ±ºç­–**: ãƒãƒƒãƒã‚µã‚¤ã‚ºã‚’å°ã•ãè¨­å®šã€ä¸¦åˆ—åº¦ã‚’ä¸‹ã’ã‚‹

### ãƒ­ã‚°ç¢ºèª

```bash
# è©•ä¾¡ã‚·ã‚¹ãƒ†ãƒ ãƒ­ã‚°
./manage.sh logs evaluation-system

# ç‰¹å®šã‚¨ãƒ©ãƒ¼ã®æ¤œç´¢
docker exec grpc-analyzer-evaluation grep -r "ERROR" /app/logs/

# ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ç¢ºèª
docker exec grpc-analyzer-evaluation ls -la /app/logs/performance/
```

### ãƒ‡ãƒãƒƒã‚°ãƒ¢ãƒ¼ãƒ‰

```bash
# ãƒ‡ãƒãƒƒã‚°å®Ÿè¡Œ
python main.py --debug --verbose --log-level DEBUG

# å˜ä½“ãƒ†ã‚¹ãƒˆ
python -m pytest src/tests/ -v
```

---

## é–‹ç™ºç¶™ç¶šã®ã‚¬ã‚¤ãƒ‰

### é‡è¦ãªå®Ÿè£…ãƒ•ã‚¡ã‚¤ãƒ«

1. **main.py**: ã‚¨ãƒ³ãƒˆãƒªãƒ¼ãƒã‚¤ãƒ³ãƒˆã€ã‚³ãƒãƒ³ãƒ‰ãƒ©ã‚¤ãƒ³å‡¦ç†
2. **src/evaluators/**: è©•ä¾¡ãƒ­ã‚¸ãƒƒã‚¯
3. **src/analyzers/**: ãƒ‡ãƒ¼ã‚¿è§£æ
4. **src/reporters/**: ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ
5. **evaluation-design/**: è©•ä¾¡è¨­è¨ˆä»•æ§˜

### æ‹¡å¼µãƒã‚¤ãƒ³ãƒˆ

**æ–°ã—ã„è©•ä¾¡ãƒ¡ãƒˆãƒªã‚¯ã‚¹è¿½åŠ **:
```python
# src/evaluators/custom_evaluator.py
class CustomEvaluator(BaseEvaluator):
    def evaluate(self, data):
        # ã‚«ã‚¹ã‚¿ãƒ è©•ä¾¡ãƒ­ã‚¸ãƒƒã‚¯
        pass
```

**æ–°ã—ã„LLMãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼**:
```python
# src/llm/custom_provider.py
class CustomLLMProvider(BaseLLMProvider):
    def generate_evaluation(self, prompt):
        # ã‚«ã‚¹ã‚¿ãƒ LLMçµ±åˆ
        pass
```

### ãƒ†ã‚¹ãƒˆæˆ¦ç•¥

```bash
# å˜ä½“ãƒ†ã‚¹ãƒˆ
python -m pytest src/tests/unit/ -v

# çµ±åˆãƒ†ã‚¹ãƒˆ
python -m pytest src/tests/integration/ -v

# ã‚¨ãƒ³ãƒ‰ãƒ„ãƒ¼ã‚¨ãƒ³ãƒ‰ãƒ†ã‚¹ãƒˆ
./manage.sh eval-dry-run
```

### ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æœ€é©åŒ–

1. **ä¸¦åˆ—å‡¦ç†**: `config/evaluation_config.json` ã® `parallel_workers`
2. **ãƒãƒƒãƒã‚µã‚¤ã‚º**: `batch_size` ã®èª¿æ•´
3. **ã‚­ãƒ£ãƒƒã‚·ãƒ³ã‚°**: ä¸­é–“çµæœã®ã‚­ãƒ£ãƒƒã‚·ãƒ¥æ©Ÿèƒ½
4. **ãƒ¡ãƒ¢ãƒªç®¡ç†**: å¤§é‡ãƒ‡ãƒ¼ã‚¿å‡¦ç†æ™‚ã®ãƒ¡ãƒ¢ãƒªåŠ¹ç‡åŒ–

### é‹ç”¨ç›£è¦–

```bash
# ãƒªã‚½ãƒ¼ã‚¹ç›£è¦–
docker stats grpc-analyzer-evaluation

# è©•ä¾¡é€²è¡ŒçŠ¶æ³
tail -f /app/logs/evaluation.log

# ã‚¨ãƒ©ãƒ¼ç›£è¦–
grep -i error /app/logs/*.log
```

---

## ğŸš€ ã‚¯ã‚¤ãƒƒã‚¯ã‚¹ã‚¿ãƒ¼ãƒˆï¼ˆæ–°è¦å¼•ãç¶™ãè€…å‘ã‘ï¼‰

1. **ç’°å¢ƒç¢ºèª**:
   ```bash
   cd /app/.docker
   ./manage.sh eval-setup
   ```

2. **ãƒ†ã‚¹ãƒˆå®Ÿè¡Œ**:
   ```bash
   ./manage.sh eval-dry-run
   ```

3. **å°è¦æ¨¡è©•ä¾¡**:
   ```bash
   ./manage.sh eval-run --mode compliance --limit 5
   ```

4. **çµæœç¢ºèª**:
   ```bash
   ls -la F:/Workspace/gRPC_Analyzer/evaluation/results/
   ```

5. **ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆè©³èª­**:
   - `evaluation-design/integrated-evaluation-system.md`
   - `DEVELOPMENT_GUIDE.md`

---

## ğŸ“ ã‚µãƒãƒ¼ãƒˆãƒ»ãƒªã‚½ãƒ¼ã‚¹

### è¨­è¨ˆãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆ
- `evaluation-design/`: è©³ç´°ãªè©•ä¾¡ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯è¨­è¨ˆ
- `docs/`: è¿½åŠ ã®æŠ€è¡“æ–‡æ›¸

### ãƒ­ã‚°ãƒ•ã‚¡ã‚¤ãƒ«
- `logs/evaluation.log`: ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œãƒ­ã‚°
- `logs/performance/`: ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹è¨ˆæ¸¬
- `logs/errors/`: ã‚¨ãƒ©ãƒ¼è©³ç´°

### è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«
- `config/evaluation_config.json`: ãƒ¡ã‚¤ãƒ³è¨­å®š
- `requirements_evaluation.txt`: Pythonä¾å­˜é–¢ä¿‚
- `docker-compose.yml`: ã‚³ãƒ³ãƒ†ãƒŠè¨­å®š

---

**ä½œæˆæ—¥**: 2025-07-22  
**ãƒãƒ¼ã‚¸ãƒ§ãƒ³**: 1.0  
**å¯¾è±¡ã‚·ã‚¹ãƒ†ãƒ **: gRPC_Analyzer APRè©•ä¾¡ã‚·ã‚¹ãƒ†ãƒ 
