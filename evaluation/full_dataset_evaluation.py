#!/usr/bin/env python3
"""
„Éá„Éº„Çø„Çª„ÉÉ„ÉàÂÖ®‰ª∂LLMË©ï‰æ°ÂÆüË°å„ÉÑ„Éº„É´ - GPT-4o‰ΩøÁî®
ÊúÄÊñ∞GPT-4o„É¢„Éá„É´„Çí‰ΩøÁî®„Åó„ÅüÂÖ®421„É≠„Ç∞„ÅÆÂ§ßË¶èÊ®°Ë©ï‰æ°
"""

import asyncio
import json
import sys
import os
import time
from datetime import datetime
from pathlib import Path
from typing import Dict, Any, List
from concurrent.futures import ThreadPoolExecutor

# .env„Éï„Ç°„Ç§„É´„ÇíÊòéÁ§∫ÁöÑ„Å´„É≠„Éº„Éâ
try:
    from dotenv import load_dotenv
    load_dotenv("/app/.env")
    print(f"‚úÖ .env„Éï„Ç°„Ç§„É´„Çí„É≠„Éº„Éâ: OPENAI_API_KEY={'Ë®≠ÂÆöÊ∏à„Åø' if os.getenv('OPENAI_API_KEY') else 'Êú™Ë®≠ÂÆö'}")
except ImportError:
    print("‚ö†Ô∏è  python-dotenv„ÅåÊú™„Ç§„É≥„Çπ„Éà„Éº„É´ - OS„ÅÆÁí∞Â¢ÉÂ§âÊï∞„Çí‰ΩøÁî®")

sys.path.append('/app')
sys.path.append('/app/src')

from real_llm_evaluator import EnhancedSystemComplianceEvaluator, DetailedLLMLogger


class FullDatasetEvaluator:
    """„Éá„Éº„Çø„Çª„ÉÉ„ÉàÂÖ®‰ª∂Ë©ï‰æ°„ÇØ„É©„Çπ"""
    
    def __init__(self, llm_model: str = "gpt-4o", concurrent_limit: int = 5):
        self.llm_model = llm_model
        self.concurrent_limit = concurrent_limit
        self.start_time = datetime.now()
        self.timestamp = self.start_time.strftime("%Y%m%d_%H%M%S")
        
        # Âá∫Âäõ„Éá„Ç£„É¨„ÇØ„Éà„É™„ÅÆ‰ΩúÊàê
        self.output_dir = Path("/app/full_dataset_results")
        self.output_dir.mkdir(exist_ok=True)
        
        # „Éó„É≠„Ç∏„Çß„ÇØ„Éà„É™„Çπ„Éà
        self.projects = [
            "boulder", "daos", "emojivoto", "go-micro-services", 
            "hmda-platform", "loop", "orchestra", "pravega", 
            "rasa-sdk", "servantes", "weaviate"
        ]
        
        print(f"üöÄ „Éá„Éº„Çø„Çª„ÉÉ„ÉàÂÖ®‰ª∂Ë©ï‰æ°„Ç∑„Çπ„ÉÜ„É†ÂàùÊúüÂåñ")
        print(f"üß† ‰ΩøÁî®„É¢„Éá„É´: {self.llm_model}")
        print(f"‚ö° ‰∏¶ÂàóÂá¶ÁêÜÊï∞: {self.concurrent_limit}")
        print(f"üìä ÂØæË±°„Éó„É≠„Ç∏„Çß„ÇØ„Éà: {len(self.projects)}‰ª∂")
        print(f"üìÅ Âá∫Âäõ„Éá„Ç£„É¨„ÇØ„Éà„É™: {self.output_dir}")
    
    def get_all_log_files(self) -> Dict[str, List[Path]]:
        """ÂÖ®„É≠„Ç∞„Éï„Ç°„Ç§„É´„ÇíÂèñÂæó"""
        all_logs = {}
        total_logs = 0
        
        for project in self.projects:
            project_dir = Path(f"/app/apr-logs/{project}")
            if project_dir.exists():
                log_files = list(project_dir.glob("**/*.log"))
                all_logs[project] = log_files
                total_logs += len(log_files)
                print(f"üìÅ {project}: {len(log_files)} „É≠„Ç∞")
            else:
                print(f"‚ö†Ô∏è  {project}: „Éá„Ç£„É¨„ÇØ„Éà„É™„ÅåÂ≠òÂú®„Åó„Åæ„Åõ„Çì")
                all_logs[project] = []
        
        print(f"üìä Á∑è„É≠„Ç∞„Éï„Ç°„Ç§„É´Êï∞: {total_logs}")
        return all_logs
    
    async def evaluate_single_project(self, project: str, max_logs: int = None) -> Dict[str, Any]:
        """Âçò‰∏Ä„Éó„É≠„Ç∏„Çß„ÇØ„Éà„ÅÆË©ï‰æ°"""
        print(f"\nüéØ „Éó„É≠„Ç∏„Çß„ÇØ„ÉàË©ï‰æ°ÈñãÂßã: {project}")
        
        # „Éó„É≠„Ç∏„Çß„ÇØ„ÉàÂ∞ÇÁî®„É≠„Ç∞Âá∫Âäõ
        project_logger = DetailedLLMLogger(
            output_dir=f"/app/full_dataset_results/{project}_logs"
        )
        project_logger.log_evaluation_start(project, max_logs or "ÂÖ®‰ª∂", "openai", self.llm_model)
        
        try:
            # Ë©ï‰æ°Âô®„ÅÆÂàùÊúüÂåñ
            evaluator = EnhancedSystemComplianceEvaluator(
                workspace_path="/app",
                llm_provider="openai",
                llm_model=self.llm_model,
                prompt_template_style="default",
                detailed_logger=project_logger
            )
            
            # Ë©ï‰æ°ÂÆüË°å
            result = await evaluator.evaluate_single_repository(project, max_logs=max_logs)
            
            # ÁµêÊûú„ÅÆ„É≠„Ç∞Âá∫Âäõ
            project_logger.log_evaluation_summary(result)
            
            # ÁµêÊûú„Çí‰øùÂ≠ò
            output_file = self.output_dir / f"{project}_analysis_{self.timestamp}.json"
            result_data = {
                f"Full_Dataset_{self.llm_model.upper()}_Analysis": result
            }
            
            with open(output_file, 'w', encoding='utf-8') as f:
                json.dump(result_data, f, indent=2, ensure_ascii=False)
            
            project_logger.logger.info(f"üíæ „Éó„É≠„Ç∏„Çß„ÇØ„ÉàË©ï‰æ°ÁµêÊûú„Çí‰øùÂ≠ò: {output_file}")
            
            return {
                "project": project,
                "result": result,
                "success": True,
                "logger": project_logger
            }
            
        except Exception as e:
            project_logger.logger.error(f"‚ùå „Éó„É≠„Ç∏„Çß„ÇØ„ÉàË©ï‰æ°„Ç®„É©„Éº: {project} - {e}")
            return {
                "project": project,
                "result": None,
                "success": False,
                "error": str(e),
                "logger": project_logger
            }
        finally:
            project_logger.finalize()
    
    async def evaluate_all_projects(self, max_logs_per_project: int = None) -> Dict[str, Any]:
        """ÂÖ®„Éó„É≠„Ç∏„Çß„ÇØ„Éà„ÅÆ‰∏¶ÂàóË©ï‰æ°"""
        print(f"\nüöÄ „Éá„Éº„Çø„Çª„ÉÉ„ÉàÂÖ®‰ª∂Ë©ï‰æ°ÈñãÂßã")
        print(f"üìä Ë©ï‰æ°ÂØæË±°: {len(self.projects)} „Éó„É≠„Ç∏„Çß„ÇØ„Éà")
        print(f"üß† ‰ΩøÁî®„É¢„Éá„É´: {self.llm_model}")
        print(f"‚ö° ‰∏¶ÂàóÂá¶ÁêÜ: {self.concurrent_limit} ÂêåÊôÇÂÆüË°å")
        if max_logs_per_project:
            print(f"üìã ÂêÑ„Éó„É≠„Ç∏„Çß„ÇØ„ÉàÊúÄÂ§ß„É≠„Ç∞Êï∞: {max_logs_per_project}")
        else:
            print(f"üìã Âá¶ÁêÜ„É¢„Éº„Éâ: ÂÖ®„É≠„Ç∞„Éï„Ç°„Ç§„É´")
        print(f"‚è∞ ÈñãÂßãÊôÇÂàª: {self.start_time}")
        print("=" * 80)
        
        # „Çª„Éû„Éï„Ç©„Çí‰ΩøÁî®„Åó„Åü‰∏¶ÂàóÂÆüË°åÂà∂Èôê
        semaphore = asyncio.Semaphore(self.concurrent_limit)
        
        async def evaluate_with_semaphore(project: str) -> Dict[str, Any]:
            async with semaphore:
                return await self.evaluate_single_project(project, max_logs_per_project)
        
        # ÂÖ®„Éó„É≠„Ç∏„Çß„ÇØ„Éà„ÅÆ‰∏¶ÂàóË©ï‰æ°
        project_tasks = [evaluate_with_semaphore(project) for project in self.projects]
        project_results = await asyncio.gather(*project_tasks, return_exceptions=True)
        
        # ÁµêÊûú„ÅÆÈõÜË®à
        successful_projects = []
        failed_projects = []
        total_logs_processed = 0
        total_successful_evaluations = 0
        total_failed_evaluations = 0
        total_tokens = 0
        total_cost = 0.0
        
        for i, result in enumerate(project_results):
            if isinstance(result, Exception):
                failed_projects.append({
                    "project": self.projects[i],
                    "error": str(result)
                })
                continue
            
            if result["success"]:
                successful_projects.append(result)
                project_result = result["result"]
                summary = project_result.get("summary", {})
                
                total_logs_processed += summary.get("total_logs_processed", 0)
                total_successful_evaluations += summary.get("successful_evaluations", 0)
                total_failed_evaluations += summary.get("failed_evaluations", 0)
                
                # „Éà„Éº„ÇØ„É≥„Å®„Ç≥„Çπ„ÉàÊÉÖÂ†±
                if result.get("logger"):
                    logger_responses = result["logger"].llm_responses
                    for response_data in logger_responses:
                        if response_data.get("usage"):
                            tokens = response_data["usage"].get("total_tokens", 0)
                            total_tokens += tokens
                            # GPT-4 Turbo„ÅÆ„Ç≥„Çπ„ÉàË®àÁÆó (input: $0.01/1K, output: $0.03/1K)
                            input_tokens = response_data["usage"].get("prompt_tokens", 0)
                            output_tokens = response_data["usage"].get("completion_tokens", 0)
                            cost = (input_tokens * 0.01 + output_tokens * 0.03) / 1000
                            total_cost += cost
            else:
                failed_projects.append(result)
        
        # Á∑èÂêàÁµêÊûú„ÅÆ‰ΩúÊàê
        end_time = datetime.now()
        total_duration = (end_time - self.start_time).total_seconds()
        
        overall_results = {
            "evaluation_metadata": {
                "timestamp": self.timestamp,
                "start_time": self.start_time.isoformat(),
                "end_time": end_time.isoformat(),
                "total_duration_seconds": total_duration,
                "llm_model": self.llm_model,
                "concurrent_limit": self.concurrent_limit,
                "max_logs_per_project": max_logs_per_project
            },
            "overall_summary": {
                "total_projects": len(self.projects),
                "successful_projects": len(successful_projects),
                "failed_projects": len(failed_projects),
                "total_logs_processed": total_logs_processed,
                "total_successful_evaluations": total_successful_evaluations,
                "total_failed_evaluations": total_failed_evaluations,
                "overall_success_rate": total_successful_evaluations / max(total_logs_processed, 1),
                "total_tokens": total_tokens,
                "total_cost_usd": total_cost,
                "average_logs_per_project": total_logs_processed / max(len(successful_projects), 1),
                "processing_rate_logs_per_minute": total_logs_processed / max(total_duration / 60, 1)
            },
            "project_results": successful_projects,
            "failed_projects": failed_projects
        }
        
        # Á∑èÂêàÁµêÊûú„Çí‰øùÂ≠ò
        overall_output = self.output_dir / f"full_dataset_analysis_{self.timestamp}.json"
        with open(overall_output, 'w', encoding='utf-8') as f:
            json.dump(overall_results, f, indent=2, ensure_ascii=False)
        
        # „Çµ„Éû„É™„ÉºË°®Á§∫
        self.print_final_summary(overall_results)
        
        return overall_results
    
    def print_final_summary(self, results: Dict[str, Any]):
        """ÊúÄÁµÇ„Çµ„Éû„É™„Éº„ÅÆË°®Á§∫"""
        print("\n" + "=" * 80)
        print("üéä „Éá„Éº„Çø„Çª„ÉÉ„ÉàÂÖ®‰ª∂Ë©ï‰æ°ÂÆå‰∫Ü")
        print("=" * 80)
        
        metadata = results["evaluation_metadata"]
        summary = results["overall_summary"]
        
        print(f"‚è∞ ÈñãÂßãÊôÇÂàª: {metadata['start_time']}")
        print(f"‚è∞ ÁµÇ‰∫ÜÊôÇÂàª: {metadata['end_time']}")
        print(f"‚è±Ô∏è  Á∑èÂÆüË°åÊôÇÈñì: {metadata['total_duration_seconds']:.2f}Áßí ({metadata['total_duration_seconds']/60:.1f}ÂàÜ)")
        print(f"üß† ‰ΩøÁî®„É¢„Éá„É´: {metadata['llm_model']}")
        
        print(f"\nüìä Ë©ï‰æ°Áµ±Ë®à:")
        print(f"  - ÂØæË±°„Éó„É≠„Ç∏„Çß„ÇØ„ÉàÊï∞: {summary['total_projects']}")
        print(f"  - ÊàêÂäü„Éó„É≠„Ç∏„Çß„ÇØ„ÉàÊï∞: {summary['successful_projects']}")
        print(f"  - Â§±Êïó„Éó„É≠„Ç∏„Çß„ÇØ„ÉàÊï∞: {summary['failed_projects']}")
        print(f"  - Á∑èÂá¶ÁêÜ„É≠„Ç∞Êï∞: {summary['total_logs_processed']:,}")
        print(f"  - ÊàêÂäüË©ï‰æ°Êï∞: {summary['total_successful_evaluations']:,}")
        print(f"  - Â§±ÊïóË©ï‰æ°Êï∞: {summary['total_failed_evaluations']:,}")
        print(f"  - ÂÖ®‰ΩìÊàêÂäüÁéá: {summary['overall_success_rate']:.3f}")
        
        print(f"\nüí∞ „Ç≥„Çπ„ÉàÊÉÖÂ†±:")
        print(f"  - Á∑è„Éà„Éº„ÇØ„É≥Êï∞: {summary['total_tokens']:,}")
        print(f"  - Á∑è„Ç≥„Çπ„Éà: ${summary['total_cost_usd']:.2f} USD")
        
        print(f"\n‚ö° „Éë„Éï„Ç©„Éº„Éû„É≥„Çπ:")
        print(f"  - Âπ≥Âùá„É≠„Ç∞/„Éó„É≠„Ç∏„Çß„ÇØ„Éà: {summary['average_logs_per_project']:.1f}")
        print(f"  - Âá¶ÁêÜÈÄüÂ∫¶: {summary['processing_rate_logs_per_minute']:.1f} „É≠„Ç∞/ÂàÜ")
        
        print(f"\nüìÅ Âá∫Âäõ„Éï„Ç°„Ç§„É´: {self.output_dir}/full_dataset_analysis_{metadata['timestamp']}.json")
        print("=" * 80)


async def main():
    """„É°„Ç§„É≥ÂÆüË°åÈñ¢Êï∞"""
    import argparse
    
    parser = argparse.ArgumentParser(description="„Éá„Éº„Çø„Çª„ÉÉ„ÉàÂÖ®‰ª∂LLMË©ï‰æ°ÂÆüË°å")
    parser.add_argument("--model", "-m", default="gpt-4o", help="‰ΩøÁî®„Åô„ÇãLLM„É¢„Éá„É´")
    parser.add_argument("--concurrent", "-c", type=int, default=5, help="‰∏¶ÂàóÂÆüË°åÊï∞")
    parser.add_argument("--max-logs", "-n", type=int, help="ÂêÑ„Éó„É≠„Ç∏„Çß„ÇØ„Éà„ÅÆÊúÄÂ§ß„É≠„Ç∞Êï∞ÔºàÊåáÂÆö„Å™„Åó„ÅßÂÖ®‰ª∂Ôºâ")
    parser.add_argument("--test", action="store_true", help="„ÉÜ„Çπ„Éà„É¢„Éº„ÉâÔºàÂêÑ„Éó„É≠„Ç∏„Çß„ÇØ„Éà2„É≠„Ç∞„ÅÆ„ÅøÔºâ")
    
    args = parser.parse_args()
    
    if args.test:
        args.max_logs = 2
        print("üß™ „ÉÜ„Çπ„Éà„É¢„Éº„Éâ: ÂêÑ„Éó„É≠„Ç∏„Çß„ÇØ„Éà2„É≠„Ç∞„ÅÆ„ÅøÂá¶ÁêÜ")
    
    print(f"üéØ „Éá„Éº„Çø„Çª„ÉÉ„ÉàÂÖ®‰ª∂Ë©ï‰æ°ÈñãÂßã")
    print(f"üß† ‰ΩøÁî®„É¢„Éá„É´: {args.model}")
    print(f"‚ö° ‰∏¶ÂàóÂÆüË°åÊï∞: {args.concurrent}")
    if args.max_logs:
        print(f"üìä ÂêÑ„Éó„É≠„Ç∏„Çß„ÇØ„ÉàÊúÄÂ§ß„É≠„Ç∞Êï∞: {args.max_logs}")
    else:
        print(f"üìä Âá¶ÁêÜ„É¢„Éº„Éâ: ÂÖ®‰ª∂Âá¶ÁêÜ")
    
    try:
        evaluator = FullDatasetEvaluator(
            llm_model=args.model,
            concurrent_limit=args.concurrent
        )
        
        results = await evaluator.evaluate_all_projects(args.max_logs)
        
        print(f"\n‚úÖ „Éá„Éº„Çø„Çª„ÉÉ„ÉàÂÖ®‰ª∂Ë©ï‰æ°„ÅåÊ≠£Â∏∏„Å´ÂÆå‰∫Ü„Åó„Åæ„Åó„ÅüÔºÅ")
        
    except KeyboardInterrupt:
        print("\n‚èπÔ∏è  „É¶„Éº„Ç∂„Éº„Å´„Çà„Å£„Å¶‰∏≠Êñ≠„Åï„Çå„Åæ„Åó„Åü")
    except Exception as e:
        print(f"\n‚ùå ÂÆüË°å„Ç®„É©„Éº: {e}")
        sys.exit(1)


if __name__ == "__main__":
    asyncio.run(main())
