#!/usr/bin/env python3
"""
ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆå…¨ä»¶LLMè©•ä¾¡å®Ÿè¡Œãƒ„ãƒ¼ãƒ« - GPT-4oä½¿ç”¨
æœ€æ–°GPT-4oãƒ¢ãƒ‡ãƒ«ã‚’ä½¿ç”¨ã—ãŸå…¨421ãƒ­ã‚°ã®å¤§è¦æ¨¡è©•ä¾¡
"""

import asyncio
import json
import sys
import os
import time
from datetime import datetime
from pathlib import Path
from typing import Dict, Any, List
from concurrent.futures import ThreadPoolExecutor

# .envãƒ•ã‚¡ã‚¤ãƒ«ã‚’æ˜ç¤ºçš„ã«ãƒ­ãƒ¼ãƒ‰
try:
    from dotenv import load_dotenv
    load_dotenv("/app/.env")
    print(f"âœ… .envãƒ•ã‚¡ã‚¤ãƒ«ã‚’ãƒ­ãƒ¼ãƒ‰: OPENAI_API_KEY={'è¨­å®šæ¸ˆã¿' if os.getenv('OPENAI_API_KEY') else 'æœªè¨­å®š'}")
except ImportError:
    print("âš ï¸  python-dotenvãŒæœªã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ« - OSã®ç’°å¢ƒå¤‰æ•°ã‚’ä½¿ç”¨")

sys.path.append('/app')
sys.path.append('/app/src')

from real_llm_evaluator import EnhancedSystemComplianceEvaluator, DetailedLLMLogger


class FullDatasetEvaluator:
    """ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆå…¨ä»¶è©•ä¾¡ã‚¯ãƒ©ã‚¹"""
    
    def __init__(self, llm_model: str = "gpt-4o", concurrent_limit: int = 5):
        self.llm_model = llm_model
        self.concurrent_limit = concurrent_limit
        self.start_time = datetime.now()
        self.timestamp = self.start_time.strftime("%Y%m%d_%H%M%S")
        
        # å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã®ä½œæˆ
        self.output_dir = Path("/app/full_dataset_results")
        self.output_dir.mkdir(exist_ok=True)
        
        # ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒªã‚¹ãƒˆ
        self.projects = [
            "boulder", "daos", "emojivoto", "go-micro-services", 
            "hmda-platform", "loop", "orchestra", "pravega", 
            "rasa-sdk", "servantes", "weaviate"
        ]
        
        print(f"ğŸš€ ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆå…¨ä»¶è©•ä¾¡ã‚·ã‚¹ãƒ†ãƒ åˆæœŸåŒ–")
        print(f"ğŸ§  ä½¿ç”¨ãƒ¢ãƒ‡ãƒ«: {self.llm_model}")
        print(f"âš¡ ä¸¦åˆ—å‡¦ç†æ•°: {self.concurrent_limit}")
        print(f"ğŸ“Š å¯¾è±¡ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆ: {len(self.projects)}ä»¶")
        print(f"ğŸ“ å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª: {self.output_dir}")
    
    def get_all_log_files(self) -> Dict[str, List[Path]]:
        """å…¨ãƒ­ã‚°ãƒ•ã‚¡ã‚¤ãƒ«ã‚’å–å¾—"""
        all_logs = {}
        total_logs = 0
        
        for project in self.projects:
            project_dir = Path(f"/app/apr-logs/{project}")
            if project_dir.exists():
                log_files = list(project_dir.glob("**/*.log"))
                all_logs[project] = log_files
                total_logs += len(log_files)
                print(f"ğŸ“ {project}: {len(log_files)} ãƒ­ã‚°")
            else:
                print(f"âš ï¸  {project}: ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªãŒå­˜åœ¨ã—ã¾ã›ã‚“")
                all_logs[project] = []
        
        print(f"ğŸ“Š ç·ãƒ­ã‚°ãƒ•ã‚¡ã‚¤ãƒ«æ•°: {total_logs}")
        return all_logs
    
    async def evaluate_single_project(self, project: str, max_logs: int = None) -> Dict[str, Any]:
        """å˜ä¸€ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã®è©•ä¾¡"""
        print(f"\nğŸ¯ ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆè©•ä¾¡é–‹å§‹: {project}")
        
        # ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆå°‚ç”¨ãƒ­ã‚°å‡ºåŠ›
        project_logger = DetailedLLMLogger(
            output_dir=f"/app/full_dataset_results/{project}_logs"
        )
        project_logger.log_evaluation_start(project, max_logs or "å…¨ä»¶", "openai", self.llm_model)
        
        try:
            # è©•ä¾¡å™¨ã®åˆæœŸåŒ–
            evaluator = EnhancedSystemComplianceEvaluator(
                workspace_path="/app",
                llm_provider="openai",
                llm_model=self.llm_model,
                prompt_template_style="default",
                detailed_logger=project_logger
            )
            
            # è©•ä¾¡å®Ÿè¡Œ
            result = await evaluator.evaluate_single_repository(project, max_logs=max_logs)
            
            # çµæœã®ãƒ­ã‚°å‡ºåŠ›
            project_logger.log_evaluation_summary(result)
            
            # çµæœã‚’ä¿å­˜
            output_file = self.output_dir / f"{project}_analysis_{self.timestamp}.json"
            result_data = {
                f"Full_Dataset_{self.llm_model.upper()}_Analysis": result
            }
            
            with open(output_file, 'w', encoding='utf-8') as f:
                json.dump(result_data, f, indent=2, ensure_ascii=False)
            
            project_logger.logger.info(f"ğŸ’¾ ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆè©•ä¾¡çµæœã‚’ä¿å­˜: {output_file}")
            
            return {
                "project": project,
                "result": result,
                "success": True,
                "logger": project_logger
            }
            
        except Exception as e:
            project_logger.logger.error(f"âŒ ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆè©•ä¾¡ã‚¨ãƒ©ãƒ¼: {project} - {e}")
            return {
                "project": project,
                "result": None,
                "success": False,
                "error": str(e),
                "logger": project_logger
            }
        finally:
            project_logger.finalize()
    
    async def evaluate_all_projects(self, max_logs_per_project: int = None) -> Dict[str, Any]:
        """å…¨ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã®ä¸¦åˆ—è©•ä¾¡"""
        print(f"\nğŸš€ ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆå…¨ä»¶è©•ä¾¡é–‹å§‹")
        print(f"ğŸ“Š è©•ä¾¡å¯¾è±¡: {len(self.projects)} ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆ")
        print(f"ğŸ§  ä½¿ç”¨ãƒ¢ãƒ‡ãƒ«: {self.llm_model}")
        print(f"âš¡ ä¸¦åˆ—å‡¦ç†: {self.concurrent_limit} åŒæ™‚å®Ÿè¡Œ")
        if max_logs_per_project:
            print(f"ğŸ“‹ å„ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆæœ€å¤§ãƒ­ã‚°æ•°: {max_logs_per_project}")
        else:
            print(f"ğŸ“‹ å‡¦ç†ãƒ¢ãƒ¼ãƒ‰: å…¨ãƒ­ã‚°ãƒ•ã‚¡ã‚¤ãƒ«")
        print(f"â° é–‹å§‹æ™‚åˆ»: {self.start_time}")
        print("=" * 80)
        
        # ã‚»ãƒãƒ•ã‚©ã‚’ä½¿ç”¨ã—ãŸä¸¦åˆ—å®Ÿè¡Œåˆ¶é™
        semaphore = asyncio.Semaphore(self.concurrent_limit)
        
        async def evaluate_with_semaphore(project: str) -> Dict[str, Any]:
            async with semaphore:
                return await self.evaluate_single_project(project, max_logs_per_project)
        
        # å…¨ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã®ä¸¦åˆ—è©•ä¾¡
        project_tasks = [evaluate_with_semaphore(project) for project in self.projects]
        project_results = await asyncio.gather(*project_tasks, return_exceptions=True)
        
        # çµæœã®é›†è¨ˆ
        successful_projects = []
        failed_projects = []
        total_logs_processed = 0
        total_successful_evaluations = 0
        total_failed_evaluations = 0
        total_tokens = 0
        total_cost = 0.0
        
        for i, result in enumerate(project_results):
            if isinstance(result, Exception):
                failed_projects.append({
                    "project": self.projects[i],
                    "error": str(result)
                })
                continue
            
            if result["success"]:
                successful_projects.append(result)
                project_result = result["result"]
                summary = project_result.get("summary", {})
                
                total_logs_processed += summary.get("total_logs_processed", 0)
                total_successful_evaluations += summary.get("successful_evaluations", 0)
                total_failed_evaluations += summary.get("failed_evaluations", 0)
                
                # ãƒˆãƒ¼ã‚¯ãƒ³ã¨ã‚³ã‚¹ãƒˆæƒ…å ±
                if result.get("logger"):
                    logger_responses = result["logger"].llm_responses
                    for response_data in logger_responses:
                        if response_data.get("usage"):
                            tokens = response_data["usage"].get("total_tokens", 0)
                            total_tokens += tokens
                            # GPT-4 Turboã®ã‚³ã‚¹ãƒˆè¨ˆç®— (input: $0.01/1K, output: $0.03/1K)
                            input_tokens = response_data["usage"].get("prompt_tokens", 0)
                            output_tokens = response_data["usage"].get("completion_tokens", 0)
                            cost = (input_tokens * 0.01 + output_tokens * 0.03) / 1000
                            total_cost += cost
            else:
                failed_projects.append(result)
        
        # ç·åˆçµæœã®ä½œæˆ
        end_time = datetime.now()
        total_duration = (end_time - self.start_time).total_seconds()
        
        overall_results = {
            "evaluation_metadata": {
                "timestamp": self.timestamp,
                "start_time": self.start_time.isoformat(),
                "end_time": end_time.isoformat(),
                "total_duration_seconds": total_duration,
                "llm_model": self.llm_model,
                "concurrent_limit": self.concurrent_limit,
                "max_logs_per_project": max_logs_per_project
            },
            "overall_summary": {
                "total_projects": len(self.projects),
                "successful_projects": len(successful_projects),
                "failed_projects": len(failed_projects),
                "total_logs_processed": total_logs_processed,
                "total_successful_evaluations": total_successful_evaluations,
                "total_failed_evaluations": total_failed_evaluations,
                "overall_success_rate": total_successful_evaluations / max(total_logs_processed, 1),
                "total_tokens": total_tokens,
                "total_cost_usd": total_cost,
                "average_logs_per_project": total_logs_processed / max(len(successful_projects), 1),
                "processing_rate_logs_per_minute": total_logs_processed / max(total_duration / 60, 1)
            },
            "project_results": successful_projects,
            "failed_projects": failed_projects
        }
        
        # ç·åˆçµæœã‚’ä¿å­˜
        overall_output = self.output_dir / f"full_dataset_analysis_{self.timestamp}.json"
        with open(overall_output, 'w', encoding='utf-8') as f:
            json.dump(overall_results, f, indent=2, ensure_ascii=False)
        
        # ã‚µãƒãƒªãƒ¼è¡¨ç¤º
        self.print_final_summary(overall_results)
        
        return overall_results
    
    def print_final_summary(self, results: Dict[str, Any]):
        """æœ€çµ‚ã‚µãƒãƒªãƒ¼ã®è¡¨ç¤º"""
        print("\n" + "=" * 80)
        print("ğŸŠ ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆå…¨ä»¶è©•ä¾¡å®Œäº†")
        print("=" * 80)
        
        metadata = results["evaluation_metadata"]
        summary = results["overall_summary"]
        
        print(f"â° é–‹å§‹æ™‚åˆ»: {metadata['start_time']}")
        print(f"â° çµ‚äº†æ™‚åˆ»: {metadata['end_time']}")
        print(f"â±ï¸  ç·å®Ÿè¡Œæ™‚é–“: {metadata['total_duration_seconds']:.2f}ç§’ ({metadata['total_duration_seconds']/60:.1f}åˆ†)")
        print(f"ğŸ§  ä½¿ç”¨ãƒ¢ãƒ‡ãƒ«: {metadata['llm_model']}")
        
        print(f"\nğŸ“Š è©•ä¾¡çµ±è¨ˆ:")
        print(f"  - å¯¾è±¡ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆæ•°: {summary['total_projects']}")
        print(f"  - æˆåŠŸãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆæ•°: {summary['successful_projects']}")
        print(f"  - å¤±æ•—ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆæ•°: {summary['failed_projects']}")
        print(f"  - ç·å‡¦ç†ãƒ­ã‚°æ•°: {summary['total_logs_processed']:,}")
        print(f"  - æˆåŠŸè©•ä¾¡æ•°: {summary['total_successful_evaluations']:,}")
        print(f"  - å¤±æ•—è©•ä¾¡æ•°: {summary['total_failed_evaluations']:,}")
        print(f"  - å…¨ä½“æˆåŠŸç‡: {summary['overall_success_rate']:.3f}")
        
        print(f"\nğŸ’° ã‚³ã‚¹ãƒˆæƒ…å ±:")
        print(f"  - ç·ãƒˆãƒ¼ã‚¯ãƒ³æ•°: {summary['total_tokens']:,}")
        print(f"  - ç·ã‚³ã‚¹ãƒˆ: ${summary['total_cost_usd']:.2f} USD")
        
        print(f"\nâš¡ ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹:")
        print(f"  - å¹³å‡ãƒ­ã‚°/ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆ: {summary['average_logs_per_project']:.1f}")
        print(f"  - å‡¦ç†é€Ÿåº¦: {summary['processing_rate_logs_per_minute']:.1f} ãƒ­ã‚°/åˆ†")
        
        print(f"\nğŸ“ å‡ºåŠ›ãƒ•ã‚¡ã‚¤ãƒ«: {self.output_dir}/full_dataset_analysis_{metadata['timestamp']}.json")
        print("=" * 80)


async def main():
    """ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œé–¢æ•°"""
    import argparse
    
    parser = argparse.ArgumentParser(description="ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆå…¨ä»¶LLMè©•ä¾¡å®Ÿè¡Œ")
    parser.add_argument("--model", "-m", default="gpt-4o", help="ä½¿ç”¨ã™ã‚‹LLMãƒ¢ãƒ‡ãƒ«")
    parser.add_argument("--concurrent", "-c", type=int, default=5, help="ä¸¦åˆ—å®Ÿè¡Œæ•°")
    parser.add_argument("--max-logs", "-n", type=int, help="å„ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã®æœ€å¤§ãƒ­ã‚°æ•°ï¼ˆæŒ‡å®šãªã—ã§å…¨ä»¶ï¼‰")
    parser.add_argument("--test", action="store_true", help="ãƒ†ã‚¹ãƒˆãƒ¢ãƒ¼ãƒ‰ï¼ˆå„ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆ2ãƒ­ã‚°ã®ã¿ï¼‰")
    
    args = parser.parse_args()
    
    if args.test:
        args.max_logs = 2
        print("ğŸ§ª ãƒ†ã‚¹ãƒˆãƒ¢ãƒ¼ãƒ‰: å„ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆ2ãƒ­ã‚°ã®ã¿å‡¦ç†")
    
    print(f"ğŸ¯ ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆå…¨ä»¶è©•ä¾¡é–‹å§‹")
    print(f"ğŸ§  ä½¿ç”¨ãƒ¢ãƒ‡ãƒ«: {args.model}")
    print(f"âš¡ ä¸¦åˆ—å®Ÿè¡Œæ•°: {args.concurrent}")
    if args.max_logs:
        print(f"ğŸ“Š å„ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆæœ€å¤§ãƒ­ã‚°æ•°: {args.max_logs}")
    else:
        print(f"ğŸ“Š å‡¦ç†ãƒ¢ãƒ¼ãƒ‰: å…¨ä»¶å‡¦ç†")
    
    try:
        evaluator = FullDatasetEvaluator(
            llm_model=args.model,
            concurrent_limit=args.concurrent
        )
        
        results = await evaluator.evaluate_all_projects(args.max_logs)
        
        print(f"\nâœ… ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆå…¨ä»¶è©•ä¾¡ãŒæ­£å¸¸ã«å®Œäº†ã—ã¾ã—ãŸï¼")
        
    except KeyboardInterrupt:
        print("\nâ¹ï¸  ãƒ¦ãƒ¼ã‚¶ãƒ¼ã«ã‚ˆã£ã¦ä¸­æ–­ã•ã‚Œã¾ã—ãŸ")
    except Exception as e:
        print(f"\nâŒ å®Ÿè¡Œã‚¨ãƒ©ãƒ¼: {e}")
        sys.exit(1)


if __name__ == "__main__":
    asyncio.run(main())
