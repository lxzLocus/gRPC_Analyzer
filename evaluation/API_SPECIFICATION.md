# APIä»•æ§˜æ›¸ - APRè©•ä¾¡ã‚·ã‚¹ãƒ†ãƒ 

## ğŸ“‹ æ¦‚è¦

æœ¬ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã¯ã€APRï¼ˆAutomatic Program Repairï¼‰è©•ä¾¡ã‚·ã‚¹ãƒ†ãƒ ã®è©³ç´°ãªAPIä»•æ§˜ã¨ã‚¤ãƒ³ã‚¿ãƒ¼ãƒ•ã‚§ãƒ¼ã‚¹å®šç¾©ã‚’æä¾›ã—ã¾ã™ã€‚

---

## ğŸ¯ ä¸»è¦APIã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆ

### main.py ã‚³ãƒãƒ³ãƒ‰ãƒ©ã‚¤ãƒ³ã‚¤ãƒ³ã‚¿ãƒ¼ãƒ•ã‚§ãƒ¼ã‚¹

```bash
python main.py [OPTIONS] [COMMANDS]
```

#### ã‚ªãƒ—ã‚·ãƒ§ãƒ³

| ã‚ªãƒ—ã‚·ãƒ§ãƒ³ | å‹ | ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ | èª¬æ˜ |
|-----------|---|-----------|------|
| `--mode` | str | `"full"` | è©•ä¾¡ãƒ¢ãƒ¼ãƒ‰ (`setup`, `compliance`, `quality`, `full`, `dry-run`) |
| `--project` | str | `None` | å¯¾è±¡ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆå (æŒ‡å®šã—ãªã„å ´åˆã¯å…¨ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆ) |
| `--dataset` | str | `"all"` | ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆæŒ‡å®š (`filtered_confirmed`, `filtered_commit`, `test`, `all`) |
| `--output-format` | str | `"json"` | å‡ºåŠ›å½¢å¼ (`json`, `csv`, `html`) |
| `--output-dir` | str | `"/app/results"` | çµæœå‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª |
| `--limit` | int | `None` | å‡¦ç†ä»¶æ•°åˆ¶é™ï¼ˆãƒ†ã‚¹ãƒˆç”¨ï¼‰ |
| `--parallel-workers` | int | `4` | ä¸¦åˆ—å®Ÿè¡Œãƒ¯ãƒ¼ã‚«ãƒ¼æ•° |
| `--debug` | bool | `False` | ãƒ‡ãƒãƒƒã‚°ãƒ¢ãƒ¼ãƒ‰æœ‰åŠ¹åŒ– |
| `--verbose` | bool | `False` | è©³ç´°ãƒ­ã‚°å‡ºåŠ› |
| `--log-level` | str | `"INFO"` | ãƒ­ã‚°ãƒ¬ãƒ™ãƒ« (`DEBUG`, `INFO`, `WARNING`, `ERROR`) |
| `--config-file` | str | `None` | ã‚«ã‚¹ã‚¿ãƒ è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹ |
| `--llm-provider` | str | `"openai"` | LLMãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ (`openai`, `anthropic`) |
| `--model` | str | `"gpt-4"` | ä½¿ç”¨ã™ã‚‹LLMãƒ¢ãƒ‡ãƒ« |
| `--timeout` | int | `300` | APIå‘¼ã³å‡ºã—ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆï¼ˆç§’ï¼‰ |

#### ä½¿ç”¨ä¾‹

```bash
# åŸºæœ¬çš„ãªå®Œå…¨è©•ä¾¡
python main.py --mode full

# ç‰¹å®šãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã®ã‚·ã‚¹ãƒ†ãƒ é©åˆæ€§è©•ä¾¡ã®ã¿
python main.py --mode compliance --project servantes

# ãƒ‡ãƒãƒƒã‚°ãƒ¢ãƒ¼ãƒ‰ã§å°è¦æ¨¡ãƒ†ã‚¹ãƒˆ
python main.py --mode quality --limit 10 --debug --verbose

# ã‚«ã‚¹ã‚¿ãƒ è¨­å®šã§ã®è©•ä¾¡
python main.py --config-file /app/config/custom_config.json --llm-provider anthropic
```

---

## ğŸ”§ ãƒ—ãƒ­ã‚°ãƒ©ãƒãƒ†ã‚£ãƒƒã‚¯API

### BaseEvaluator ã‚¤ãƒ³ã‚¿ãƒ¼ãƒ•ã‚§ãƒ¼ã‚¹

```python
from src.evaluators.base_evaluator import BaseEvaluator

class BaseEvaluator:
    def __init__(self, config: Dict[str, Any]):
        """è©•ä¾¡å™¨ã®åˆæœŸåŒ–
        
        Args:
            config: è¨­å®šè¾æ›¸
        """
    
    def evaluate(self, data: Any) -> EvaluationResult:
        """è©•ä¾¡ã®å®Ÿè¡Œ
        
        Args:
            data: è©•ä¾¡å¯¾è±¡ãƒ‡ãƒ¼ã‚¿
            
        Returns:
            EvaluationResult: è©•ä¾¡çµæœ
            
        Raises:
            EvaluationError: è©•ä¾¡å¤±æ•—æ™‚
        """
    
    def generate_report(self, results: List[EvaluationResult]) -> Dict[str, Any]:
        """ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ
        
        Args:
            results: è©•ä¾¡çµæœãƒªã‚¹ãƒˆ
            
        Returns:
            Dict: ãƒ¬ãƒãƒ¼ãƒˆãƒ‡ãƒ¼ã‚¿
        """
```

### ComplianceEvaluator API

```python
from src.evaluators.compliance_evaluator import ComplianceEvaluator

class ComplianceEvaluator(BaseEvaluator):
    def evaluate_control_flow(self, apr_logs: List[Dict]) -> float:
        """åˆ¶å¾¡ãƒ•ãƒ­ãƒ¼è§£æç²¾åº¦ã®è©•ä¾¡
        
        Args:
            apr_logs: APRãƒ­ã‚°ãƒ‡ãƒ¼ã‚¿
            
        Returns:
            float: ç²¾åº¦ã‚¹ã‚³ã‚¢ (0.0-1.0)
        """
    
    def evaluate_parser_accuracy(self, apr_logs: List[Dict]) -> float:
        """ãƒ‘ãƒ¼ã‚µãƒ¼ç²¾åº¦ã®è©•ä¾¡
        
        Args:
            apr_logs: APRãƒ­ã‚°ãƒ‡ãƒ¼ã‚¿
            
        Returns:
            float: ç²¾åº¦ã‚¹ã‚³ã‚¢ (0.0-1.0)
        """
    
    def evaluate_file_processing(self, apr_logs: List[Dict]) -> float:
        """ãƒ•ã‚¡ã‚¤ãƒ«å‡¦ç†èƒ½åŠ›ã®è©•ä¾¡
        
        Args:
            apr_logs: APRãƒ­ã‚°ãƒ‡ãƒ¼ã‚¿
            
        Returns:
            float: å‡¦ç†æˆåŠŸç‡ (0.0-1.0)
        """
    
    def evaluate_error_handling(self, apr_logs: List[Dict]) -> float:
        """ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°èƒ½åŠ›ã®è©•ä¾¡
        
        Args:
            apr_logs: APRãƒ­ã‚°ãƒ‡ãƒ¼ã‚¿
            
        Returns:
            float: ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°ã‚¹ã‚³ã‚¢ (0.0-1.0)
        """
```

### QualityEvaluator API

```python
from src.evaluators.quality_evaluator import QualityEvaluator

class QualityEvaluator(BaseEvaluator):
    async def evaluate_patch_quality(
        self, 
        generated_patch: str, 
        ground_truth: str
    ) -> Dict[str, float]:
        """ãƒ‘ãƒƒãƒå“è³ªã®ç·åˆè©•ä¾¡
        
        Args:
            generated_patch: ç”Ÿæˆã•ã‚ŒãŸãƒ‘ãƒƒãƒ
            ground_truth: ã‚°ãƒ©ã‚¦ãƒ³ãƒ‰ãƒˆã‚¥ãƒ«ãƒ¼ã‚¹
            
        Returns:
            Dict[str, float]: å“è³ªãƒ¡ãƒˆãƒªã‚¯ã‚¹
            - plausibility_score: å¦¥å½“æ€§ã‚¹ã‚³ã‚¢
            - correctness_r1: Correctness@1
            - correctness_r5: Correctness@5
            - correctness_r10: Correctness@10
            - reasoning_quality: æ¨è«–å“è³ªã‚¹ã‚³ã‚¢
        """
    
    def calculate_correctness_at_k(
        self, 
        predictions: List[str], 
        ground_truth: str, 
        k: int
    ) -> float:
        """Correctness@K ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã®è¨ˆç®—
        
        Args:
            predictions: äºˆæ¸¬ãƒ‘ãƒƒãƒãƒªã‚¹ãƒˆ
            ground_truth: æ­£è§£ãƒ‘ãƒƒãƒ
            k: ä¸Šä½Kå€‹
            
        Returns:
            float: Correctness@K ã‚¹ã‚³ã‚¢ (0.0-1.0)
        """
    
    async def evaluate_reasoning_quality(
        self, 
        reasoning_log: str
    ) -> float:
        """æ¨è«–å“è³ªã®è©•ä¾¡
        
        Args:
            reasoning_log: æ¨è«–ãƒ­ã‚°
            
        Returns:
            float: æ¨è«–å“è³ªã‚¹ã‚³ã‚¢ (0.0-1.0)
        """
```

---

## ğŸ“Š ãƒ‡ãƒ¼ã‚¿æ§‹é€ 

### EvaluationResult

```python
@dataclass
class EvaluationResult:
    """è©•ä¾¡çµæœã®æ¨™æº–ãƒ‡ãƒ¼ã‚¿æ§‹é€ """
    
    timestamp: datetime
    evaluator_type: str  # 'compliance', 'quality', 'integrated'
    project_name: str
    dataset_type: str
    metrics: Dict[str, float]
    details: Dict[str, Any]
    status: str  # 'success', 'partial', 'failed'
    execution_time: float  # ç§’
    error_message: Optional[str] = None
    
    def to_dict(self) -> Dict[str, Any]:
        """è¾æ›¸å½¢å¼ã«å¤‰æ›"""
        return asdict(self)
    
    def to_json(self) -> str:
        """JSONæ–‡å­—åˆ—ã«å¤‰æ›"""
        return json.dumps(self.to_dict(), default=str, indent=2)
```

### ComplianceMetrics

```python
@dataclass
class ComplianceMetrics:
    """ã‚·ã‚¹ãƒ†ãƒ é©åˆæ€§è©•ä¾¡ã®ãƒ¡ãƒˆãƒªã‚¯ã‚¹"""
    
    control_flow_accuracy: float  # åˆ¶å¾¡ãƒ•ãƒ­ãƒ¼è§£æç²¾åº¦
    parser_success_rate: float    # ãƒ‘ãƒ¼ã‚µãƒ¼æˆåŠŸç‡
    file_processing_rate: float   # ãƒ•ã‚¡ã‚¤ãƒ«å‡¦ç†æˆåŠŸç‡
    error_handling_score: float   # ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°ã‚¹ã‚³ã‚¢
    overall_compliance: float     # ç·åˆé©åˆæ€§ã‚¹ã‚³ã‚¢
    
    def calculate_overall_score(self) -> float:
        """ç·åˆã‚¹ã‚³ã‚¢ã®è¨ˆç®—"""
        weights = {
            'control_flow_accuracy': 0.3,
            'parser_success_rate': 0.3,
            'file_processing_rate': 0.2,
            'error_handling_score': 0.2
        }
        
        return sum(
            getattr(self, metric) * weight
            for metric, weight in weights.items()
        )
```

### QualityMetrics

```python
@dataclass
class QualityMetrics:
    """ãƒ‘ãƒƒãƒå“è³ªè©•ä¾¡ã®ãƒ¡ãƒˆãƒªã‚¯ã‚¹"""
    
    plausibility_score: float     # å¦¥å½“æ€§ã‚¹ã‚³ã‚¢
    correctness_r1: float         # Correctness@1
    correctness_r5: float         # Correctness@5
    correctness_r10: float        # Correctness@10
    reasoning_quality: float      # æ¨è«–å“è³ªã‚¹ã‚³ã‚¢
    semantic_similarity: float    # æ„å‘³çš„é¡ä¼¼åº¦
    syntactic_correctness: float  # æ§‹æ–‡æ­£ç¢ºæ€§
    
    def calculate_overall_quality(self) -> float:
        """ç·åˆå“è³ªã‚¹ã‚³ã‚¢ã®è¨ˆç®—"""
        weights = {
            'plausibility_score': 0.2,
            'correctness_r1': 0.25,
            'correctness_r5': 0.2,
            'correctness_r10': 0.15,
            'reasoning_quality': 0.2
        }
        
        return sum(
            getattr(self, metric) * weight
            for metric, weight in weights.items()
        )
```

---

## ğŸ¤– LLMãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼API

### BaseLLMProvider ã‚¤ãƒ³ã‚¿ãƒ¼ãƒ•ã‚§ãƒ¼ã‚¹

```python
from abc import ABC, abstractmethod

class BaseLLMProvider(ABC):
    def __init__(self, api_key: str, model: str):
        self.api_key = api_key
        self.model = model
        self.rate_limiter = None
    
    @abstractmethod
    async def evaluate(
        self, 
        prompt: str, 
        max_tokens: int = 2000,
        temperature: float = 0.1
    ) -> str:
        """LLMè©•ä¾¡ã®å®Ÿè¡Œ
        
        Args:
            prompt: è©•ä¾¡ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ
            max_tokens: æœ€å¤§ãƒˆãƒ¼ã‚¯ãƒ³æ•°
            temperature: æ¸©åº¦ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
            
        Returns:
            str: LLMå¿œç­”
            
        Raises:
            LLMAPIError: APIå‘¼ã³å‡ºã—å¤±æ•—
            RateLimitError: ãƒ¬ãƒ¼ãƒˆåˆ¶é™ã‚¨ãƒ©ãƒ¼
        """
    
    def parse_evaluation_response(self, response: str) -> Dict[str, float]:
        """LLMå¿œç­”ã®è§£æ
        
        Args:
            response: LLMå¿œç­”
            
        Returns:
            Dict[str, float]: è©•ä¾¡ã‚¹ã‚³ã‚¢
        """
```

### OpenAIProvider

```python
class OpenAIProvider(BaseLLMProvider):
    def __init__(self, api_key: str, model: str = "gpt-4"):
        super().__init__(api_key, model)
        self.client = OpenAI(api_key=api_key)
        self.rate_limiter = AsyncRateLimiter(max_calls=100, time_window=60)
    
    async def evaluate(
        self, 
        prompt: str, 
        max_tokens: int = 2000,
        temperature: float = 0.1
    ) -> str:
        """OpenAI APIçµŒç”±ã§ã®è©•ä¾¡å®Ÿè¡Œ"""
        async with self.rate_limiter:
            try:
                response = await self.client.chat.completions.acreate(
                    model=self.model,
                    messages=[
                        {"role": "system", "content": self._get_system_prompt()},
                        {"role": "user", "content": prompt}
                    ],
                    max_tokens=max_tokens,
                    temperature=temperature
                )
                
                return response.choices[0].message.content
                
            except openai.RateLimitError as e:
                raise RateLimitError(f"OpenAI rate limit exceeded: {e}")
            except Exception as e:
                raise LLMAPIError(f"OpenAI API error: {e}")
```

---

## ğŸ“ˆ ãƒ¬ãƒãƒ¼ãƒˆAPI

### JSONReporter

```python
class JSONReporter:
    def generate_report(
        self, 
        results: List[EvaluationResult],
        include_metadata: bool = True
    ) -> Dict[str, Any]:
        """JSONå½¢å¼ã®ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ
        
        Args:
            results: è©•ä¾¡çµæœãƒªã‚¹ãƒˆ
            include_metadata: ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿å«æœ‰ãƒ•ãƒ©ã‚°
            
        Returns:
            Dict: JSONãƒ¬ãƒãƒ¼ãƒˆãƒ‡ãƒ¼ã‚¿
        """
        
        report = {
            "evaluation_summary": {
                "total_evaluations": len(results),
                "successful_evaluations": len([r for r in results if r.status == 'success']),
                "failed_evaluations": len([r for r in results if r.status == 'failed']),
                "average_execution_time": self._calculate_avg_execution_time(results)
            },
            "metrics_summary": self._calculate_metrics_summary(results),
            "detailed_results": [result.to_dict() for result in results]
        }
        
        if include_metadata:
            report["metadata"] = self._generate_metadata()
        
        return report
    
    def save_report(
        self, 
        report: Dict[str, Any], 
        output_path: str
    ) -> None:
        """ãƒ¬ãƒãƒ¼ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ã®ä¿å­˜"""
        with open(output_path, 'w', encoding='utf-8') as f:
            json.dump(report, f, indent=2, ensure_ascii=False, default=str)
```

### ChartGenerator

```python
class ChartGenerator:
    def generate_metrics_chart(
        self, 
        metrics_data: Dict[str, List[float]],
        chart_type: str = "bar",
        output_path: str = None
    ) -> str:
        """ãƒ¡ãƒˆãƒªã‚¯ã‚¹ãƒãƒ£ãƒ¼ãƒˆã®ç”Ÿæˆ
        
        Args:
            metrics_data: ãƒ¡ãƒˆãƒªã‚¯ã‚¹ãƒ‡ãƒ¼ã‚¿
            chart_type: ãƒãƒ£ãƒ¼ãƒˆç¨®é¡ ('bar', 'line', 'scatter', 'heatmap')
            output_path: å‡ºåŠ›ãƒ‘ã‚¹
            
        Returns:
            str: ç”Ÿæˆã•ã‚ŒãŸãƒãƒ£ãƒ¼ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹
        """
    
    def generate_comparison_chart(
        self, 
        baseline_data: Dict[str, float],
        current_data: Dict[str, float],
        output_path: str = None
    ) -> str:
        """æ¯”è¼ƒãƒãƒ£ãƒ¼ãƒˆã®ç”Ÿæˆ"""
    
    def generate_trend_chart(
        self, 
        time_series_data: Dict[str, List[Tuple[datetime, float]]],
        output_path: str = None
    ) -> str:
        """ãƒˆãƒ¬ãƒ³ãƒ‰ãƒãƒ£ãƒ¼ãƒˆã®ç”Ÿæˆ"""
```

---

## âš™ï¸ è¨­å®šAPI

### ConfigManager

```python
class ConfigManager:
    def __init__(self, config_path: str = "/app/config/evaluation_config.json"):
        self.config_path = config_path
        self.config = self.load_config()
    
    def load_config(self) -> Dict[str, Any]:
        """è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã®èª­ã¿è¾¼ã¿"""
        try:
            with open(self.config_path, 'r', encoding='utf-8') as f:
                return json.load(f)
        except FileNotFoundError:
            return self.get_default_config()
    
    def get_default_config(self) -> Dict[str, Any]:
        """ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆè¨­å®šã®å–å¾—"""
        return {
            "evaluation": {
                "step1_enabled": True,
                "step2_enabled": True,
                "llm_provider": "openai",
                "model": "gpt-4",
                "timeout": 300,
                "max_retries": 3,
                "batch_size": 10,
                "parallel_workers": 4
            },
            "output": {
                "format": "json",
                "include_charts": True,
                "chart_format": "png",
                "include_raw_data": False
            },
            "logging": {
                "level": "INFO",
                "file_rotation": True,
                "max_file_size": "10MB",
                "backup_count": 5
            }
        }
    
    def update_config(self, updates: Dict[str, Any]) -> None:
        """è¨­å®šã®æ›´æ–°"""
        def deep_update(base_dict, update_dict):
            for key, value in update_dict.items():
                if key in base_dict and isinstance(base_dict[key], dict) and isinstance(value, dict):
                    deep_update(base_dict[key], value)
                else:
                    base_dict[key] = value
        
        deep_update(self.config, updates)
        self.save_config()
    
    def save_config(self) -> None:
        """è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã®ä¿å­˜"""
        with open(self.config_path, 'w', encoding='utf-8') as f:
            json.dump(self.config, f, indent=2, ensure_ascii=False)
```

---

## ğŸ” æ¤œè¨¼API

### DataValidator

```python
class DataValidator:
    @staticmethod
    def validate_apr_logs(logs: List[Dict]) -> Tuple[bool, List[str]]:
        """APRãƒ­ã‚°ãƒ‡ãƒ¼ã‚¿ã®æ¤œè¨¼
        
        Args:
            logs: APRãƒ­ã‚°ãƒ‡ãƒ¼ã‚¿
            
        Returns:
            Tuple[bool, List[str]]: (æœ‰åŠ¹æ€§, ã‚¨ãƒ©ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ãƒªã‚¹ãƒˆ)
        """
        errors = []
        
        if not logs:
            errors.append("Empty log data")
            return False, errors
        
        required_fields = ['timestamp', 'project', 'status']
        for i, log in enumerate(logs):
            missing_fields = [field for field in required_fields if field not in log]
            if missing_fields:
                errors.append(f"Log {i}: Missing fields {missing_fields}")
        
        return len(errors) == 0, errors
    
    @staticmethod
    def validate_patch_data(patch: str) -> Tuple[bool, List[str]]:
        """ãƒ‘ãƒƒãƒãƒ‡ãƒ¼ã‚¿ã®æ¤œè¨¼"""
        errors = []
        
        if not patch or not patch.strip():
            errors.append("Empty patch data")
        
        if not patch.startswith('diff ') and not patch.startswith('--- '):
            errors.append("Invalid patch format")
        
        return len(errors) == 0, errors
```

---

## ğŸš¨ ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°

### ã‚«ã‚¹ã‚¿ãƒ ä¾‹å¤–

```python
class EvaluationError(Exception):
    """è©•ä¾¡ã‚·ã‚¹ãƒ†ãƒ åŸºåº•ä¾‹å¤–"""
    pass

class ConfigurationError(EvaluationError):
    """è¨­å®šã‚¨ãƒ©ãƒ¼"""
    pass

class DataValidationError(EvaluationError):
    """ãƒ‡ãƒ¼ã‚¿æ¤œè¨¼ã‚¨ãƒ©ãƒ¼"""
    pass

class LLMAPIError(EvaluationError):
    """LLM API ã‚¨ãƒ©ãƒ¼"""
    pass

class RateLimitError(LLMAPIError):
    """ãƒ¬ãƒ¼ãƒˆåˆ¶é™ã‚¨ãƒ©ãƒ¼"""
    pass

class TimeoutError(EvaluationError):
    """ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆã‚¨ãƒ©ãƒ¼"""
    pass
```

### ã‚¨ãƒ©ãƒ¼ãƒ¬ã‚¹ãƒãƒ³ã‚¹å½¢å¼

```python
@dataclass
class ErrorResponse:
    """ã‚¨ãƒ©ãƒ¼ãƒ¬ã‚¹ãƒãƒ³ã‚¹ã®æ¨™æº–å½¢å¼"""
    
    error_type: str
    error_message: str
    error_code: str
    timestamp: datetime
    context: Dict[str, Any] = None
    suggestions: List[str] = None
    
    def to_dict(self) -> Dict[str, Any]:
        return asdict(self)
```

---

## ğŸ“š ä½¿ç”¨ä¾‹

### åŸºæœ¬çš„ãªè©•ä¾¡å®Ÿè¡Œ

```python
#!/usr/bin/env python3
"""åŸºæœ¬çš„ãªè©•ä¾¡å®Ÿè¡Œä¾‹"""

import asyncio
from src.evaluators.integrated_evaluator import IntegratedEvaluator
from src.utils.config_manager import ConfigManager

async def main():
    # è¨­å®šèª­ã¿è¾¼ã¿
    config_manager = ConfigManager()
    config = config_manager.load_config()
    
    # è©•ä¾¡å™¨åˆæœŸåŒ–
    evaluator = IntegratedEvaluator(config)
    
    # è©•ä¾¡å®Ÿè¡Œ
    try:
        results = await evaluator.evaluate_project('servantes')
        
        # çµæœå‡ºåŠ›
        for result in results:
            print(f"Evaluator: {result.evaluator_type}")
            print(f"Status: {result.status}")
            print(f"Metrics: {result.metrics}")
            print(f"Execution time: {result.execution_time:.2f}s")
            print("-" * 50)
    
    except Exception as e:
        print(f"Evaluation failed: {e}")

if __name__ == "__main__":
    asyncio.run(main())
```

### ã‚«ã‚¹ã‚¿ãƒ è©•ä¾¡å™¨ã®å®Ÿè£…

```python
"""ã‚«ã‚¹ã‚¿ãƒ è©•ä¾¡å™¨ã®å®Ÿè£…ä¾‹"""

from src.evaluators.base_evaluator import BaseEvaluator
from src.models.evaluation_result import EvaluationResult

class CustomMetricEvaluator(BaseEvaluator):
    def __init__(self, config):
        super().__init__(config)
        self.metric_name = "custom_complexity_score"
    
    def evaluate(self, data):
        """ã‚«ã‚¹ã‚¿ãƒ ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã®è©•ä¾¡"""
        try:
            # ã‚«ã‚¹ã‚¿ãƒ è©•ä¾¡ãƒ­ã‚¸ãƒƒã‚¯
            complexity_score = self._calculate_complexity(data)
            
            return EvaluationResult(
                timestamp=datetime.now(),
                evaluator_type='custom_complexity',
                project_name=data.get('project_name', 'unknown'),
                dataset_type=data.get('dataset_type', 'unknown'),
                metrics={self.metric_name: complexity_score},
                details={'algorithm': 'custom_complexity_v1'},
                status='success',
                execution_time=time.time() - start_time
            )
        
        except Exception as e:
            self.logger.error(f"Custom evaluation failed: {e}")
            return EvaluationResult(
                timestamp=datetime.now(),
                evaluator_type='custom_complexity',
                project_name=data.get('project_name', 'unknown'),
                dataset_type=data.get('dataset_type', 'unknown'),
                metrics={},
                details={},
                status='failed',
                execution_time=0,
                error_message=str(e)
            )
    
    def _calculate_complexity(self, data):
        """è¤‡é›‘åº¦è¨ˆç®—ãƒ­ã‚¸ãƒƒã‚¯"""
        # å®Ÿè£…ä¾‹: McCabeè¤‡é›‘åº¦çš„ãªè¨ˆç®—
        pass
```

---

**ä½œæˆæ—¥**: 2025-07-22  
**ãƒãƒ¼ã‚¸ãƒ§ãƒ³**: 1.0  
**å¯¾è±¡ã‚·ã‚¹ãƒ†ãƒ **: gRPC_Analyzer APRè©•ä¾¡ã‚·ã‚¹ãƒ†ãƒ 
