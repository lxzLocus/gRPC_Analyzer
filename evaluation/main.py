#!/usr/bin/env python3
"""
APRè©•ä¾¡ã‚·ã‚¹ãƒ†ãƒ  - ãƒ¡ã‚¤ãƒ³ã‚¨ãƒ³ãƒˆãƒªãƒ¼ãƒã‚¤ãƒ³ãƒˆ
"""

import argparse
import json
import os
import sys
from pathlib import Path
from datetime import datetime

def setup_paths():
    """ãƒ‘ã‚¹ã®è¨­å®šã¨åˆæœŸåŒ–"""
    base_path = Path("/app")
    
    paths = {
        'dataset': base_path / 'dataset',
        'apr_logs': base_path / 'apr-logs', 
        'apr_output': base_path / 'apr-output',
        'results': base_path / 'results',
        'evaluation_design': base_path / 'evaluation-design'
    }
    
    # å¿…è¦ãªãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã®å­˜åœ¨ç¢ºèª
    for name, path in paths.items():
        if not path.exists() and name == 'results':
            path.mkdir(parents=True, exist_ok=True)
        elif not path.exists() and name not in ['evaluation_design']:
            print(f"ERROR: Required directory not found: {path}")
            sys.exit(1)
    
    return paths

def run_step1_compliance_evaluation(paths, args):
    """ã‚¹ãƒ†ãƒƒãƒ—1: ã‚·ã‚¹ãƒ†ãƒ ä»•æ§˜æº–æ‹ æ€§è©•ä¾¡"""
    print("ğŸ” Step 1: System Compliance Evaluation")
    print(f"  - APR Logs directory: {paths['apr_logs']}")
    print(f"  - Output directory: {paths['results']}/step1")
    
    # ã‚¹ãƒ†ãƒƒãƒ—1è©•ä¾¡ã®å®Ÿè£…ã‚’ã“ã“ã«è¿½åŠ 
    # å®Ÿéš›ã®è©•ä¾¡ãƒ­ã‚¸ãƒƒã‚¯ã¯åˆ¥ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã«åˆ†é›¢
    try:
        # from evaluation.step1_compliance import SystemComplianceEvaluator
        # evaluator = SystemComplianceEvaluator()
        # results = evaluator.evaluate_all(paths['logs'])
        
        # ä»®ã®çµæœï¼ˆå®Ÿè£…æ™‚ã«ç½®ãæ›ãˆï¼‰
        results = {
            "evaluation_type": "system_compliance",
            "timestamp": datetime.now().isoformat(),
            "summary": {
                "total_cases": 0,
                "compliance_rate": 0.0,
                "parser_accuracy": 0.0
            },
            "detailed_results": []
        }
        
        # çµæœä¿å­˜
        output_dir = paths['results'] / 'step1'
        output_dir.mkdir(parents=True, exist_ok=True)
        
        output_file = output_dir / f"compliance_evaluation_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(results, f, indent=2, ensure_ascii=False)
        
        print(f"âœ… Step 1 completed. Results saved to: {output_file}")
        return output_file
        
    except Exception as e:
        print(f"âŒ Step 1 failed: {e}")
        return None

def run_step2_quality_evaluation(paths, args):
    """ã‚¹ãƒ†ãƒƒãƒ—2: ãƒ‘ãƒƒãƒå“è³ªè©•ä¾¡"""
    print("ğŸ¯ Step 2: Patch Quality Evaluation")
    print(f"  - Dataset directory: {paths['dataset']}")
    print(f"  - APR Results directory: {paths['apr_output']}")
    print(f"  - Output directory: {paths['results']}/step2")
    
    try:
        # from evaluation.step2_quality import PatchQualityEvaluator
        # evaluator = PatchQualityEvaluator()
        # results = evaluator.evaluate_all(paths['dataset'], paths['output'])
        
        # ä»®ã®çµæœï¼ˆå®Ÿè£…æ™‚ã«ç½®ãæ›ãˆï¼‰
        results = {
            "evaluation_type": "patch_quality",
            "timestamp": datetime.now().isoformat(),
            "summary": {
                "total_cases": 0,
                "plausible_rate": 0.0,
                "correct_rate": 0.0,
                "avg_reasoning_quality": 0.0
            },
            "detailed_results": []
        }
        
        # çµæœä¿å­˜
        output_dir = paths['results'] / 'step2'
        output_dir.mkdir(parents=True, exist_ok=True)
        
        output_file = output_dir / f"quality_evaluation_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(results, f, indent=2, ensure_ascii=False)
        
        print(f"âœ… Step 2 completed. Results saved to: {output_file}")
        return output_file
        
    except Exception as e:
        print(f"âŒ Step 2 failed: {e}")
        return None

def generate_comprehensive_report(paths, step1_file, step2_file, args):
    """çµ±åˆãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ"""
    print("ğŸ“Š Generating Comprehensive Report")
    
    try:
        # å„ã‚¹ãƒ†ãƒƒãƒ—ã®çµæœã‚’èª­ã¿è¾¼ã¿
        step1_results = {}
        step2_results = {}
        
        if step1_file and step1_file.exists():
            with open(step1_file, 'r', encoding='utf-8') as f:
                step1_results = json.load(f)
        
        if step2_file and step2_file.exists():
            with open(step2_file, 'r', encoding='utf-8') as f:
                step2_results = json.load(f)
        
        # çµ±åˆãƒ¬ãƒãƒ¼ãƒˆä½œæˆ
        comprehensive_report = {
            "report_type": "comprehensive_evaluation",
            "timestamp": datetime.now().isoformat(),
            "system_compliance": step1_results.get('summary', {}),
            "patch_quality": step2_results.get('summary', {}),
            "overall_assessment": {
                "apr_system_health": "To be calculated",
                "ai_agent_capability": "To be calculated",
                "recommendations": [
                    "Detailed recommendations will be generated based on evaluation results"
                ]
            },
            "detailed_analysis": {
                "step1_details": step1_results.get('detailed_results', []),
                "step2_details": step2_results.get('detailed_results', [])
            }
        }
        
        # ãƒ¬ãƒãƒ¼ãƒˆä¿å­˜
        output_file = paths['results'] / f"comprehensive_report_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(comprehensive_report, f, indent=2, ensure_ascii=False)
        
        print(f"âœ… Comprehensive report generated: {output_file}")
        return output_file
        
    except Exception as e:
        print(f"âŒ Report generation failed: {e}")
        return None

def main():
    """ãƒ¡ã‚¤ãƒ³é–¢æ•°"""
    parser = argparse.ArgumentParser(description="APR Evaluation System")
    parser.add_argument('--step', choices=['1', '2', 'all'], default='all',
                      help='Evaluation step to run (1: compliance, 2: quality, all: both)')
    parser.add_argument('--llm-model', default='gpt-4o-mini',
                      help='LLM model for evaluation')
    parser.add_argument('--verbose', '-v', action='store_true',
                      help='Verbose output')
    parser.add_argument('--dry-run', action='store_true',
                      help='Dry run mode (no actual evaluation)')
    
    args = parser.parse_args()
    
    print("ğŸš€ APR Evaluation System Starting...")
    print(f"  - Evaluation step: {args.step}")
    print(f"  - LLM model: {args.llm_model}")
    print(f"  - Verbose mode: {args.verbose}")
    print(f"  - Dry run: {args.dry_run}")
    print()
    
    # ãƒ‘ã‚¹è¨­å®š
    paths = setup_paths()
    
    if args.dry_run:
        print("ğŸ” Dry run mode - checking environment...")
        for name, path in paths.items():
            status = "âœ… EXISTS" if path.exists() else "âŒ MISSING"
            print(f"  - {name}: {path} [{status}]")
        print("âœ… Dry run completed")
        return
    
    # è©•ä¾¡å®Ÿè¡Œ
    step1_file = None
    step2_file = None
    
    if args.step in ['1', 'all']:
        step1_file = run_step1_compliance_evaluation(paths, args)
    
    if args.step in ['2', 'all']:
        step2_file = run_step2_quality_evaluation(paths, args)
    
    # çµ±åˆãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆï¼ˆä¸¡ã‚¹ãƒ†ãƒƒãƒ—å®Ÿè¡Œæ™‚ã®ã¿ï¼‰
    if args.step == 'all':
        generate_comprehensive_report(paths, step1_file, step2_file, args)
    
    print("ğŸ‰ APR Evaluation System Completed!")

if __name__ == "__main__":
    main()
