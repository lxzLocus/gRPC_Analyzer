#!/usr/bin/env python3
"""
APRè©•ä¾¡ã‚·ã‚¹ãƒ†ãƒ  - ãƒ¡ã‚¤ãƒ³ã‚¨ãƒ³ãƒˆãƒªãƒ¼ãƒã‚¤ãƒ³ãƒˆ

Usage:
    python main.py <command> [options]

Commands:
    evaluate      - APRã‚·ã‚¹ãƒ†ãƒ ã®è©•ä¾¡ã‚’å®Ÿè¡Œ
    analyze       - çµæœã®åˆ†æã‚’å®Ÿè¡Œ
    view          - ãƒ­ã‚°ã‚„ãƒ¬ã‚¹ãƒãƒ³ã‚¹ã®è¡¨ç¤º
    demo          - ãƒ‡ãƒ¢ãƒ³ã‚¹ãƒˆãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³å®Ÿè¡Œ

Examples:
    python main.py evaluate --repo boulder --max-logs 3
    python main.py analyze --input-dir /app/output/verification_results
    python main.py view --log-file /app/logs/evaluation.log
    python main.py demo --model gpt-4.1-mini
"""

import sys
import argparse
from pathlib import Path

# ãƒ‘ã‚¹ã‚’è¿½åŠ 
sys.path.append('/app')
sys.path.append('/app/src')


def main():
    parser = argparse.ArgumentParser(description="APRè©•ä¾¡ã‚·ã‚¹ãƒ†ãƒ ")
    subparsers = parser.add_subparsers(dest='command', help='åˆ©ç”¨å¯èƒ½ãªã‚³ãƒãƒ³ãƒ‰')
    
    # evaluate ã‚³ãƒãƒ³ãƒ‰
    eval_parser = subparsers.add_parser('evaluate', help='APRã‚·ã‚¹ãƒ†ãƒ ã®è©•ä¾¡ã‚’å®Ÿè¡Œ')
    eval_parser.add_argument('--repo', '-r', default='boulder', help='è©•ä¾¡å¯¾è±¡ãƒªãƒã‚¸ãƒˆãƒª')
    eval_parser.add_argument('--max-logs', '-n', type=int, default=3, help='æœ€å¤§å‡¦ç†ãƒ­ã‚°æ•°')
    eval_parser.add_argument('--provider', '-p', default='mock', help='LLMãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼')
    eval_parser.add_argument('--model', '-m', help='LLMãƒ¢ãƒ‡ãƒ«å')
    
    # analyze ã‚³ãƒãƒ³ãƒ‰
    analyze_parser = subparsers.add_parser('analyze', help='çµæœã®åˆ†æã‚’å®Ÿè¡Œ')
    analyze_parser.add_argument('--input-dir', '-i', default='/app/output/verification_results', help='å…¥åŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª')
    analyze_parser.add_argument('--output-file', '-o', help='å‡ºåŠ›ãƒ•ã‚¡ã‚¤ãƒ«')
    
    # view ã‚³ãƒãƒ³ãƒ‰
    view_parser = subparsers.add_parser('view', help='ãƒ­ã‚°ã‚„ãƒ¬ã‚¹ãƒãƒ³ã‚¹ã®è¡¨ç¤º')
    view_parser.add_argument('--log-file', '-l', help='ãƒ­ã‚°ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹')
    view_parser.add_argument('--response-file', '-f', help='ãƒ¬ã‚¹ãƒãƒ³ã‚¹ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹')
    
    # demo ã‚³ãƒãƒ³ãƒ‰
    demo_parser = subparsers.add_parser('demo', help='ãƒ‡ãƒ¢ãƒ³ã‚¹ãƒˆãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³å®Ÿè¡Œ')
    demo_parser.add_argument('--model', '-m', default='gpt-4.1-mini', help='ä½¿ç”¨ã™ã‚‹ãƒ¢ãƒ‡ãƒ«')
    
    args = parser.parse_args()
    
    if not args.command:
        parser.print_help()
        return
    
    if args.command == 'evaluate':
        from src.cli.real_llm_evaluator import run_real_llm_evaluation
        import asyncio
        
        print(f"ğŸ¯ è©•ä¾¡é–‹å§‹: {args.repo}")
        print(f"ğŸ“Š æœ€å¤§ãƒ­ã‚°æ•°: {args.max_logs}")
        print(f"ğŸ¤– ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼: {args.provider}")
        print(f"ğŸ§  ãƒ¢ãƒ‡ãƒ«: {args.model or 'ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ'}")
        
        try:
            asyncio.run(run_real_llm_evaluation(
                repository=args.repo,
                max_logs=args.max_logs,
                llm_provider=args.provider,
                llm_model=args.model
            ))
        except Exception as e:
            print(f"âŒ è©•ä¾¡å®Ÿè¡Œã‚¨ãƒ©ãƒ¼: {e}")
            sys.exit(1)
            
    elif args.command == 'analyze':
        print(f"ğŸ“Š åˆ†æé–‹å§‹")
        print(f"ğŸ“‚ å…¥åŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª: {args.input_dir}")
        # TODO: åˆ†æãƒ„ãƒ¼ãƒ«ã®çµ±åˆ
        print("âš ï¸  åˆ†ææ©Ÿèƒ½ã¯æº–å‚™ä¸­ã§ã™")
        
    elif args.command == 'view':
        print(f"ğŸ‘€ è¡¨ç¤ºé–‹å§‹")
        if args.log_file:
            print(f"ğŸ“‹ ãƒ­ã‚°ãƒ•ã‚¡ã‚¤ãƒ«: {args.log_file}")
        if args.response_file:
            print(f"ğŸ“„ ãƒ¬ã‚¹ãƒãƒ³ã‚¹ãƒ•ã‚¡ã‚¤ãƒ«: {args.response_file}")
        # TODO: ãƒ“ãƒ¥ãƒ¼ã‚¢ãƒ¼ã®çµ±åˆ
        print("âš ï¸  è¡¨ç¤ºæ©Ÿèƒ½ã¯æº–å‚™ä¸­ã§ã™")
        
    elif args.command == 'demo':
        print(f"ğŸ® ãƒ‡ãƒ¢é–‹å§‹")
        print(f"ğŸ§  ãƒ¢ãƒ‡ãƒ«: {args.model}")
        # TODO: ãƒ‡ãƒ¢ã®çµ±åˆ
        print("âš ï¸  ãƒ‡ãƒ¢æ©Ÿèƒ½ã¯æº–å‚™ä¸­ã§ã™")


if __name__ == '__main__':
    main()
