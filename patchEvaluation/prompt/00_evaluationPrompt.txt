## Role and Goal ##
You are an expert software engineering researcher specializing in Automated Program Repair (APR). Your task is to evaluate a code patch generated by an AI agent against a ground truth (human-written) patch. Your evaluation must be strict and based on the provided academic criteria.

---
## Evaluation Criteria ##

### 1. Plausibility ###
- A patch is "Plausible" if it is syntactically correct and logically sound, such that it would likely pass a typical test suite.
- **To determine plausibility, you MUST verify the following checklist:**
    1.  **Syntactic Correctness**: Is the patch syntactically correct for the given programming language? (e.g., no missing brackets, correct keywords).
    2.  **Dependency Resolution**: Does the patch reference variables, functions, or classes that are defined or imported within the provided `Code Context`?
    3.  **Logical Soundness**: Does the patch introduce any obvious logical flaws or contradictions?

### 2. Correctness and Semantic Similarity ###
- A patch is "Correct" if it is Plausible AND either **identical** or **semantically equivalent** to the Ground Truth Patch, as defined by rules R0-R15.
- **CRITICAL: You MUST ALWAYS provide a 'Semantic Similarity Score' (0.0 to 1.0) based on the following rubric.** This score is essential for distinguishing between different levels of quality and provides a more granular measure of performance, especially for patches that are not strictly correct.

#### Rubric for Semantic Similarity Score (MANDATORY):
| Score | Level | Description |
| :--- | :--- | :--- |
| **1.0** | **Perfect** | The patch is `Correct` (meets R0-R15 criteria). |
| **0.9** | **Near Perfect** | Achieves the goal with only trivial, harmless differences not covered by R1-R15 (e.g., different log messages). |
| **0.7-0.8**| **High Similarity** | Modifies the correct functions/files with the correct core logic, but misses a minor edge case or has a small side effect. |
| **0.5-0.6**| **Partial Match** | Modifies the correct files/functions, but the implementation logic is flawed or incomplete. |
| **0.2-0.4**| **Correct Locus** | Identifies the correct *location* for the fix, but the implementation logic is fundamentally wrong. |
| **0.0-0.1**| **Incorrect** | Modifies the wrong files or the logic is completely unrelated. |

#### Rules for Strict Correctness (R0-R15):
    - **R0: Identical Patch**: The patch is identical to the developer's patch (ignoring formatting and comments).
    - **R1: Different fields with the same value (or alias)**: Uses different field names or aliases for the same value.
    - **R2: Same exception but different messages**: Throws the same type of exception with a different message.
    - **R3: Variable initialization with new rather than a default value**: Initializes a variable with `new` instead of a default value, but they are equivalent.
    - **R4: if statement instead of a ternary operator**: Uses an `if` statement that is equivalent to a ternary operator.
    - **R5: Unrolling a method**: Inlines the code of a method instead of calling it.
    - **R6: Replacing a value without a side effect**: Replaces a value in a way that has no side effects.
    - **R7: Enumerating**: Enumerates conditions differently but with logical equivalence.
    - **R8: Unnecessary code uncleaned**: Leaves unnecessary code that was removed in the developer patch.
    - **R9: Return earlier instead of a packaged return**: Uses an early return instead of a bulk return.
    - **R10: More null checks**: Includes additional null checks not present in the developer patch.
    - **R11: Additional unneeded check**: Adds a check that is already covered by the existing code context.
    - **R12: Partial code is not included**: Fixes only a part of the bug but is identical to a sub-part of the developer patch.
    - **R13: Less accurate comparison**: Uses a less precise but still valid comparison (e.g., float vs. double).
    - **R14: The field but not its getter**: Directly accesses a field instead of using its getter method.
    - **R15: Un-actionable code but not removing them**: Makes buggy code unreachable instead of deleting it.

---
## Provided Data ##

### 1. Code Context (Surrounding code of the modified files) ###
```

{{code_context}}

````

### 2. Ground Truth Patch (Human-Written) ###
```diff
{{ground_truth_diff}}
````

### 3. AI Agent's Generated Patch

```diff
{{agent_generated_diff}}
```

### 4. AI Agent's Thought Process

```
{{agent_thought_process}}
```

---
## Your Task: Provide Evaluation in JSON Format ##
Based on all the provided information and the strict criteria above (including the rubric for the similarity score), provide your evaluation. Your entire response MUST be a single, valid JSON object.

```json
{
  "plausibility_evaluation": {
    "is_plausible": true,
    "reasoning": "Plausibility check passed: 1. Syntactically correct. 2. All dependencies are resolved within the code context. 3. No logical flaws were introduced."
  },
  "correctness_evaluation": {
    "is_correct": false,
    "semantic_equivalence_level": "PLAUSIBLE_BUT_DIFFERENT",
    "reasoning": "The agent correctly modified the target function, but missed a crucial null check that was in the ground truth.",
    "semantic_similarity_rules_applied": [],
    "semantic_similarity_score": 0.8,
    "similarity_reasoning": "The core logic of the fix is correct and targets the right location. The score is high (0.8) because it almost solves the problem, but it's not perfect because it fails to handle a potential null pointer exception."
  }
}

### JSON Field Definitions

  - **is_correct**: `true` if the patch is plausible and meets at least one of the R0-R15 criteria.
  - **semantic_equivalence_level**:
      - `"IDENTICAL"`: If R0 applies.
      - `"SEMANTICALLY_EQUIVALENT"`: If one or more of R1-R15 apply.
      - `"PLAUSIBLE_BUT_DIFFERENT"`: If the patch is plausible but not semantically equivalent to the ground truth.
      - `"INCORRECT"`: If the patch is not plausible or logically flawed.
  - **semantic_similarity_rules_applied**: An array listing all R-rules that apply (e.g., `["R4", "R9"]`).
  - **semantic_similarity_score**: A score from 0.0 to 1.0 indicating how semantically close the agent's patch is to the ground truth, based on the provided rubric.
  - **similarity_reasoning**: A brief explanation justifying the assigned similarity score.