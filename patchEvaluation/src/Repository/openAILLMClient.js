/**
 * OpenAI LLM ClientÔºàJavaScriptÁâàÔºâ
 * OpenAI API„Å®„ÅÆÈÄö‰ø°„ÇíÊãÖÂΩì
 */

import { config as dotenvConfig } from 'dotenv';
import { LLMClient, createLLMResponse } from './llmClient.js';

// Áí∞Â¢ÉÂ§âÊï∞„ÅÆË™≠„ÅøËæº„Åø
dotenvConfig({ path: '/app/.env' });

export class OpenAILLMClient extends LLMClient {
    constructor(config, apiKey) {
        super();
        this.config = config;
        this.client = null;
        this.isInitialized = false;
        
        // Áí∞Â¢ÉÂ§âÊï∞„Åã„Çâ API„Ç≠„Éº„ÇíÂèñÂæó
        const finalApiKey = apiKey || process.env.OPENAI_API_KEY || '';
        
        console.log(`üîë OpenAILLMClient: Using API key length: ${finalApiKey.length}`);
        console.log(`üîë Available env vars: OPENAI_API_KEY=${!!process.env.OPENAI_API_KEY}`);
        console.log(`ü§ñ OpenAILLMClient: Using model: ${this.config.get('llm.model', 'gpt-5')}`);
        
        // OpenAI„ÇØ„É©„Ç§„Ç¢„É≥„Éà„ÅÆÂàùÊúüÂåñ„ÇíÈùûÂêåÊúü„ÅßË°å„ÅÜ
        this.initPromise = this.initializeClient(finalApiKey);
    }

    async initializeClient(apiKey) {
        try {
            // ÂãïÁöÑimport„Çí‰ΩøÁî®„Åó„Å¶ES modulesÂØæÂøú
            const { default: OpenAI } = await import('openai');
            
            // „Çø„Ç§„É†„Ç¢„Ç¶„ÉàË®≠ÂÆö„ÇíÁµ±‰∏ÄÔºöopenai.timeout -> llm.timeout „ÅÆÈ†Ü„ÅßÂèñÂæó
            const timeoutMs = this.config.get('openai.timeout', this.config.get('llm.timeout', 120000));
            console.log(`üïí OpenAI client timeout set to: ${timeoutMs}ms`);
            
            this.client = new OpenAI({
                apiKey: apiKey,
                timeout: timeoutMs
            });
            
            this.isInitialized = true;
            console.log('‚úÖ OpenAI client initialized successfully');
        } catch (error) {
            console.error('‚ùå Failed to initialize OpenAI client:', error);
            throw error;
        }
    }

    async waitForInitialization() {
        return this.initPromise;
    }

    isReady() {
        return this.isInitialized;
    }

    getProviderName() {
        return 'OpenAI';
    }

    async generateContent(request) {
        if (!this.isInitialized) {
            await this.waitForInitialization();
        }

        try {
            const model = request.model || this.config.get('llm.model', this.config.get('openai.model', 'gpt-5'));
            const maxTokens = request.maxTokens || this.config.get('llm.maxTokens', 4000);
            
            console.log(`üöÄ OpenAI request: model=${model}, maxTokens=${maxTokens}`);

            // API„É™„ÇØ„Ç®„Çπ„Éà„Éë„É©„É°„Éº„Çø„ÇíÊ∫ñÂÇô
            const apiParams = {
                model: model,
                messages: request.messages
            };
            
            // gpt-5Á≥ª„É¢„Éá„É´„Åß„ÅØmax_completion_tokens„Çí„Çπ„Ç≠„ÉÉ„Éó
            const isGpt5Model = model.startsWith('gpt-5');
            if (!isGpt5Model && maxTokens) {
                apiParams.max_completion_tokens = maxTokens;
                console.log(`üî¢ OpenAI max_completion_tokens: ${maxTokens}`);
            } else if (isGpt5Model) {
                console.log(`‚ÑπÔ∏è  ${model}: max_completion_tokens„Éë„É©„É°„Éº„Çø„ÅØ„Çπ„Ç≠„ÉÉ„Éó„Åï„Çå„Åæ„Åô`);
            }
            
            // gpt-5Á≥ª„É¢„Éá„É´„Åß„ÅØtemperature„Çí„Çπ„Ç≠„ÉÉ„ÉóÔºà„Éá„Éï„Ç©„É´„ÉàÂÄ§1„ÅÆ„Åø„Çµ„Éù„Éº„ÉàÔºâ
            if (!isGpt5Model && request.temperature !== undefined) {
                apiParams.temperature = request.temperature;
                console.log(`üå°Ô∏è  OpenAI temperature: ${request.temperature}`);
            } else if (isGpt5Model) {
                console.log(`‚ÑπÔ∏è  ${model}: temperature„Éë„É©„É°„Éº„Çø„ÅØ„Çπ„Ç≠„ÉÉ„Éó„Åï„Çå„Åæ„ÅôÔºà„Éá„Éï„Ç©„É´„Éà1‰ΩøÁî®Ôºâ`);
            }
            
            // response_format„Éë„É©„É°„Éº„ÇøÔºàJSON modeÔºâ
            if (request.responseFormat) {
                apiParams.response_format = request.responseFormat;
                console.log(`üìã OpenAI response_format: ${JSON.stringify(request.responseFormat)}`);
            }

            // „Çø„Ç§„É†„Ç¢„Ç¶„Éà‰ªò„Åç„ÅÆAPIÂëº„Å≥Âá∫„Åó
            // Áµ±‰∏Ä„Åï„Çå„Åü„Çø„Ç§„É†„Ç¢„Ç¶„ÉàË®≠ÂÆö„Çí‰ΩøÁî®
            const apiTimeout = this.config.get('openai.timeout', this.config.get('llm.timeout', 120000));
            console.log(`üïí OpenAI API call timeout: ${apiTimeout}ms`);
            
            const apiCall = this.client.chat.completions.create(apiParams);

            const timeoutPromise = new Promise((_, reject) =>
                setTimeout(() => reject(new Error(`OpenAI API timeout after ${apiTimeout}ms`)), apiTimeout)
            );

            const response = await Promise.race([apiCall, timeoutPromise]);

            const content = response.choices[0]?.message?.content || '';
            const usage = response.usage;

            console.log(`‚úÖ OpenAI response: ${content.length} chars, usage: ${usage?.total_tokens || 0} tokens`);

            return createLLMResponse(content, {
                usage: {
                    promptTokens: usage?.prompt_tokens || 0,
                    completionTokens: usage?.completion_tokens || 0,
                    totalTokens: usage?.total_tokens || 0
                },
                model: response.model,
                finishReason: response.choices[0]?.finish_reason || 'unknown'
            });
        } catch (error) {
            console.error('‚ùå OpenAI API error:', error);
            
            // „Ç®„É©„ÉºÊÉÖÂ†±„ÇíÊ®ôÊ∫ñÂåñ„Åó„Å¶„Çπ„É≠„Éº
            const standardError = new Error(error.message);
            standardError.originalError = error;
            standardError.status = error.status;
            standardError.provider = 'openai';
            throw standardError;
        }
    }
}

export default OpenAILLMClient;
