/**
 * Gemini LLM Clientï¼ˆJavaScriptç‰ˆï¼‰
 * Google Gemini APIã¨ã®é€šä¿¡ã‚’æ‹…å½“
 */

import { LLMClient, createLLMResponse } from './llmClient.js';

export class GeminiLLMClient extends LLMClient {
    constructor(config, apiKey) {
        super();
        this.config = config;
        this.client = null;
        this.isInitialized = false;
        
        // ç’°å¢ƒå¤‰æ•°ã‹ã‚‰ APIã‚­ãƒ¼ã‚’å–å¾—
        const finalApiKey = apiKey || process.env.GEMINI_API_KEY || '';
        
        console.log(`ğŸ”‘ GeminiLLMClient: Using API key length: ${finalApiKey.length}`);
        console.log(`ğŸ”‘ Available env vars: GEMINI_API_KEY=${!!process.env.GEMINI_API_KEY}`);
        console.log(`ğŸ¤– GeminiLLMClient: Using model: ${this.config.get('gemini.model', 'gemini-2.5-pro')}`);
        
        // Geminiã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã®åˆæœŸåŒ–ã‚’éåŒæœŸã§è¡Œã†
        this.initPromise = this.initializeClient(finalApiKey);
    }

    async initializeClient(apiKey) {
        try {
            // å‹•çš„importã‚’ä½¿ç”¨ã—ã¦ES moduleså¯¾å¿œ
            const { GoogleGenAI } = await import('@google/genai');
            
            this.client = new GoogleGenAI({ 
                apiKey: apiKey 
            });
            
            this.isInitialized = true;
            console.log('âœ… Gemini client initialized successfully');
        } catch (error) {
            console.error('âŒ Failed to initialize Gemini client:', error);
            throw error;
        }
    }

    async waitForInitialization() {
        return this.initPromise;
    }

    isReady() {
        return this.isInitialized;
    }

    getProviderName() {
        return 'Gemini';
    }

    async generateContent(request) {
        if (!this.isInitialized) {
            await this.waitForInitialization();
        }

        try {
            const model = request.model || this.config.get('gemini.model', 'gemini-2.5-pro');
            const maxTokens = request.maxTokens || this.config.get('gemini.maxTokens', 4000);
            const temperature = request.temperature || this.config.get('gemini.temperature', 0.1);
            
            console.log(`ğŸš€ Gemini request: model=${model}, maxTokens=${maxTokens}, temperature=${temperature}`);

            // Gemini APIã®å‘¼ã³å‡ºã—
            const genModel = this.client.getGenerativeModel({ model: model });
            
            // ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’ Gemini å½¢å¼ã«å¤‰æ›
            const prompt = request.messages.map(msg => 
                `${msg.role === 'system' ? 'System' : msg.role === 'user' ? 'User' : 'Assistant'}: ${msg.content}`
            ).join('\n\n');

            const result = await genModel.generateContent({
                contents: [{ role: 'user', parts: [{ text: prompt }] }],
                generationConfig: {
                    maxOutputTokens: maxTokens,
                    temperature: temperature
                }
            });

            const response = await result.response;
            const content = response.text() || '';

            console.log(`âœ… Gemini response: ${content.length} chars`);

            return createLLMResponse(content, {
                usage: {
                    promptTokens: 0, // Gemini APIã§ã¯ãƒˆãƒ¼ã‚¯ãƒ³æƒ…å ±ãŒè©³ç´°ã«æä¾›ã•ã‚Œãªã„å ´åˆãŒã‚ã‚‹
                    completionTokens: 0,
                    totalTokens: 0
                },
                model: model,
                finishReason: 'stop'
            });
        } catch (error) {
            console.error('âŒ Gemini API error:', error);
            throw error;
        }
    }
}

export default GeminiLLMClient;
